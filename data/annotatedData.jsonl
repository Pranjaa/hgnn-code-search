{"query": "aes encryption", "function": "def aes_encrypt(base64_encryption_key, data):\n    \"\"\"Encrypt data with AES-CBC and sign it with HMAC-SHA256\n\n    Arguments:\n        base64_encryption_key (str): a base64-encoded string containing an AES encryption key\n            and HMAC signing key as generated by generate_encryption_key()\n        data (str): a byte string containing the data to be encrypted\n\n    Returns:\n        str: the encrypted data as a byte string with the HMAC signature appended to the end\n\n    \"\"\"\n    if isinstance(data, text_type):\n        data = data.encode(\"UTF-8\")\n    aes_key_bytes, hmac_key_bytes = _extract_keys(base64_encryption_key)\n    data = _pad(data)\n    iv_bytes = os.urandom(AES_BLOCK_SIZE)\n    cipher = AES.new(aes_key_bytes, mode=AES.MODE_CBC, IV=iv_bytes)\n    data = iv_bytes + cipher.encrypt(data)  # prepend init vector\n    hmac_signature = hmac.new(hmac_key_bytes, data, hashlib.sha256).digest()\n    return as_base64(data + hmac_signature)", "relevance": 3}
{"query": "aes encryption", "function": "def _cbc_encrypt(self, content, final_key):\n        \"\"\"This method encrypts the content.\"\"\"\n\n        aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv)\n        padding = (16 - len(content) % AES.block_size)\n\n        for _ in range(padding):\n            content += chr(padding).encode()\n\n        temp = bytes(content)\n        return aes.encrypt(temp)", "relevance": 2}
{"query": "aes encryption", "function": "def encryptData(self, encryptKey, privParameters, dataToEncrypt):\n        snmpEngineBoots, snmpEngineTime, salt = privParameters\n\n        # 8.3.1.1\n        desKey, salt, iv = self._getEncryptionKey(encryptKey, snmpEngineBoots)\n\n        # 8.3.1.2\n        privParameters = univ.OctetString(salt)\n\n        # 8.1.1.2\n        plaintext = dataToEncrypt\n        plaintext += univ.OctetString(\n            (0,) * (8 - len(dataToEncrypt) % 8)).asOctets()\n\n        try:\n            ciphertext = des.encrypt(plaintext, desKey, iv)\n\n        except PysnmpCryptoError:\n            raise error.StatusInformation(\n                errorIndication=errind.unsupportedPrivProtocol)\n\n        # 8.3.1.3 & 4\n        return univ.OctetString(ciphertext), privParameters", "relevance": 1}
{"query": "all permutations of a list", "function": "def deleterious_permutation(obs_del,\n                            context_counts,\n                            context_to_mut,\n                            seq_context,\n                            gene_seq,\n                            num_permutations=10000,\n                            stop_criteria=100,\n                            pseudo_count=0,\n                            max_batch=25000):\n    \"\"\"Performs null-permutations for deleterious mutation statistics\n    in a single gene.\n\n    Parameters\n    ----------\n    context_counts : pd.Series\n        number of mutations for each context\n    context_to_mut : dict\n        dictionary mapping nucleotide context to a list of observed\n        somatic base changes.\n    seq_context : SequenceContext\n        Sequence context for the entire gene sequence (regardless\n        of where mutations occur). The nucleotide contexts are\n        identified at positions along the gene.\n    gene_seq : GeneSequence\n        Sequence of gene of interest\n    num_permutations : int, default: 10000\n        number of permutations to create for null\n    pseudo_count : int, default: 0\n        Pseudo-count for number of deleterious mutations for each\n        permutation of the null distribution. Increasing pseudo_count\n        makes the statistical test more stringent.\n\n    Returns\n    -------\n    del_count_list : list\n        list of deleterious mutation counts under the null\n    \"\"\"\n    mycontexts = context_counts.index.tolist()\n    somatic_base = [base\n                    for one_context in mycontexts\n                    for base in context_to_mut[one_context]]\n\n    # calculate the # of batches for simulations\n    max_batch = min(num_permutations, max_batch)\n    num_batches = num_permutations // max_batch\n    remainder = num_permutations % max_batch\n    batch_sizes = [max_batch] * num_batches\n    if remainder:\n        batch_sizes += [remainder]\n\n    num_sim = 0\n    null_del_ct = 0\n    for j, batch_size in enumerate(batch_sizes):\n        # stop iterations if reached sufficient precision\n        if null_del_ct >= stop_criteria:\n            #j = j - 1\n            break\n\n        # get random positions determined by sequence context\n        tmp_contxt_pos = seq_context.random_pos(context_counts.iteritems(),\n                                                batch_size)\n        tmp_mut_pos = np.hstack(pos_array for base, pos_array in tmp_contxt_pos)\n\n        # determine result of random positions\n        for i, row in enumerate(tmp_mut_pos):\n            # get info about mutations\n            tmp_mut_info = mc.get_aa_mut_info(row,\n                                              somatic_base,\n                                              gene_seq)\n\n            # calc deleterious mutation info\n            tmp_del_count = cutils.calc_deleterious_info(tmp_mut_info['Reference AA'],\n                                                         tmp_mut_info['Somatic AA'],\n                                                         tmp_mut_info['Codon Pos'])\n\n            # update empricial null distribution\n            if tmp_del_count >= obs_del: null_del_ct += 1\n\n            # stop if reach sufficient precision on p-value\n            if null_del_ct >= stop_criteria:\n                break\n        # update number of simulations\n        num_sim += i + 1\n\n    #num_sim = j*max_batch + i+1\n    del_pval = float(null_del_ct) / (num_sim)\n\n    return del_pval", "relevance": 3}
{"query": "all permutations of a list", "function": "def print_permutations(self):\n        \"\"\"Print all valid permutations.\"\"\"\n        index = 0\n        permutations = []\n        for p in self._input_permutations:\n            permutations.append({'index': index, 'args': p})\n            index += 1\n        with open('permutations.json', 'w') as fh:\n            json.dump(permutations, fh, indent=2)\n        print('All permutations written to the \"permutations.json\" file.')", "relevance": 2}
{"query": "all permutations of a list", "function": "def p(i, sample_size, weights):\n        \"\"\"\n        Given a weighted set and sample size return the probabilty that the\n        weight `i` will be present in the sample.\n\n        Created to test the output of the `SomeOf` maker class. The math was\n        provided by Andy Blackshaw - thank you dad :)\n        \"\"\"\n\n        # Determine the initial pick values\n        weight_i = weights[i]\n        weights_sum = sum(weights)\n\n        # Build a list of weights that don't contain the weight `i`. This list will\n        # be used to build the possible picks before weight `i`.\n        other_weights = list(weights)\n        del other_weights[i]\n\n        # Calculate the probability\n        probability_of_i = 0\n        for picks in range(0, sample_size):\n\n            # Build the list of possible permutations for this pick in the sample\n            permutations = list(itertools.permutations(other_weights, picks))\n\n            # Calculate the probability for this permutation\n            permutation_probabilities = []\n            for permutation in permutations:\n\n                # Calculate the probability for each pick in the permutation\n                pick_probabilities = []\n                pick_weight_sum = weights_sum\n\n                for pick in permutation:\n                    pick_probabilities.append(pick / pick_weight_sum)\n\n                    # Each time we pick we update the sum of the weight the next\n                    # pick is from.\n                    pick_weight_sum -= pick\n\n                # Add the probability of picking i as the last pick\n                pick_probabilities += [weight_i / pick_weight_sum]\n\n                # Multiply all the probabilities for the permutation together\n                permutation_probability = reduce(\n                    lambda x, y: x * y, pick_probabilities\n                    )\n                permutation_probabilities.append(permutation_probability)\n\n            # Add together all the probabilities for all permutations together\n            probability_of_i += sum(permutation_probabilities)\n\n        return probability_of_i", "relevance": 1}
{"query": "buffered file reader read text", "function": "def reader(self):\n        if self._reader is None:\n            self._reader = WARCReader(self.fileobj)\n        return self._reader", "relevance": 3}
{"query": "buffered file reader read text", "function": "def _read_from_buffer(self, delimiter=None, nbytes=None, regex=None):\n        if nbytes is not None:\n            return self._read_buffer.read(nbytes)\n        elif delimiter is not None:\n            return self._read_buffer.read_until(delimiter)\n        elif regex is not None:\n            return self._read_buffer.read_until_regex(regex)", "relevance": 2}
{"query": "buffered file reader read text", "function": "def read_text(self, text: str) -> bool:\n        \"\"\"\n        Consume a strlen(text) text at current position in the stream\n        else return False.\n        Same as \"\" in BNF\n        ex : read_text(\"ls\");.\n        \"\"\"\n        if self.read_eof():\n            return False\n        self._stream.save_context()\n        if self.peek_text(text):\n            self._stream.incpos(len(text))\n            return self._stream.validate_context()\n        return self._stream.restore_context()", "relevance": 1}
{"query": "concatenate several file remove header lines", "function": "def header_without_lines(header, remove):\n    \"\"\"Return :py:class:`Header` without lines given in ``remove``\n\n    ``remove`` is an iterable of pairs ``key``/``ID`` with the VCF header key\n    and ``ID`` of entry to remove.  In the case that a line does not have\n    a ``mapping`` entry, you can give the full value to remove.\n\n    .. code-block:: python\n\n        # header is a vcfpy.Header, e.g., as read earlier from file\n        new_header = vcfpy.without_header_lines(\n            header, [('assembly', None), ('FILTER', 'PASS')])\n        # now, the header lines starting with \"##assembly=\" and the \"PASS\"\n        # filter line will be missing from new_header\n    \"\"\"\n    remove = set(remove)\n    # Copy over lines that are not removed\n    lines = []\n    for line in header.lines:\n        if hasattr(line, \"mapping\"):\n            if (line.key, line.mapping.get(\"ID\", None)) in remove:\n                continue  # filter out\n        else:\n            if (line.key, line.value) in remove:\n                continue  # filter out\n        lines.append(line)\n    return Header(lines, header.samples)", "relevance": 3}
{"query": "concatenate several file remove header lines", "function": "def clean_file(filename):\n    f = open(filename, 'r')\n    new_lines = []\n    for line in f.readlines():\n        new_lines.append(line.rstrip())\n    f.close()\n\n    f = open(filename, 'w')\n    for line in new_lines:\n        f.write(line + '\\n')\n\n\n    f.close()", "relevance": 2}
{"query": "concatenate several file remove header lines", "function": "def main(fpcfile):\n\n    fw = sys.stdout\n    f = FpcReader(fpcfile)\n\n    # first several lines are comments\n    header = '\\t'.join(('bac_name', 'ctg_name', 'map_left', 'map_right',\n        'bands', 'probes', 'remark'))\n    print(header, file=fw)\n\n    recs = list(f)\n    logging.debug(\"%d records parsed\" % len(recs))\n\n    recs.sort(key=lambda x: (x.ctg_name, x.map_left, x.map_right))\n    for rec in recs:\n        print(rec, file=fw)", "relevance": 1}
{"query": "confusion matrix", "function": "def from_existing(cls, confusion, *args, **kwargs):\n        \"\"\"Creates a confusion matrix from a DataFrame that already contains confusion counts (but not meta stats)\n        >>> df = pd.DataFrame(np.matrix([[0,1,2,0,1,2,1,2,2,1],[0,1,2,1,2,0,0,1,2,0]]).T, columns=['True', 'Pred'])\n        >>> c = Confusion(df)\n        >>> c2 = pd.DataFrame(c)\n        >>> hasattr(c2, '_binary_sensitivity')\n        False\n        >>> c3 = Confusion.from_existing(c2)\n        >>> hasattr(c3, '_binary_sensitivity')\n        True\n        >>> (c3 == c).all().all()\n        True\n        >>> c3\n        Pred  0  1  2\n        True\n        0     1  1  0\n        1     2  1  1\n        2     1  1  2\n        \"\"\"\n        # Extremely brute-force to recreate data from a confusion matrix!\n\n        df = []\n        for t, p in product(confusion.index.values, confusion.columns.values):\n            df += [[t, p]] * confusion[p][t]\n        if confusion.index.name is not None and confusion.columns.name is not None:\n            return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name]))\n        return Confusion(pd.DataFrame(df))", "relevance": 4}
{"query": "confusion matrix", "function": "def make_diagonal_povm(pi_basis, confusion_rate_matrix):\n    \"\"\"\n    Create a DiagonalPOVM from a ``pi_basis`` and the ``confusion_rate_matrix`` associated with a\n    readout.\n\n    See also the grove documentation.\n\n    :param OperatorBasis pi_basis: An operator basis of rank-1 projection operators.\n    :param numpy.ndarray confusion_rate_matrix: The matrix of detection probabilities conditional\n    on a prepared qubit state.\n    :return: The POVM corresponding to confusion_rate_matrix.\n    :rtype: DiagonalPOVM\n    \"\"\"\n\n    confusion_rate_matrix = np.asarray(confusion_rate_matrix)\n    if not np.allclose(confusion_rate_matrix.sum(axis=0), np.ones(confusion_rate_matrix.shape[1])):\n        raise CRMUnnormalizedError(\"Unnormalized confusion matrix:\\n{}\".format(\n            confusion_rate_matrix))\n    if not (confusion_rate_matrix >= 0).all() or not (confusion_rate_matrix <= 1).all():\n        raise CRMValueError(\"Confusion matrix must have values in [0, 1]:\"\n                            \"\\n{}\".format(confusion_rate_matrix))\n\n    ops = [sum((pi_j * pjk for (pi_j, pjk) in izip(pi_basis.ops, pjs)), 0)\n           for pjs in confusion_rate_matrix]\n    return DiagonalPOVM(pi_basis=pi_basis, confusion_rate_matrix=confusion_rate_matrix, ops=ops)", "relevance": 3}
{"query": "confusion matrix", "function": "def confusion_matrix(self, data):\n        \"\"\"\n        Returns a confusion matrix based of H2O's default prediction threshold for a dataset.\n\n        :param H2OFrame data: the frame with the prediction results for which the confusion matrix should be extracted.\n        \"\"\"\n        assert_is_type(data, H2OFrame)\n        j = h2o.api(\"POST /3/Predictions/models/%s/frames/%s\" % (self._id, data.frame_id))\n        return j[\"model_metrics\"][0][\"cm\"][\"table\"]", "relevance": 2}
{"query": "connect to sql", "function": "def connect(self):\n        self.close()\n        self._connect = pymysql.connect(**self._db_options)\n        self._connect.autocommit(True)", "relevance": 4}
{"query": "connect to sql", "function": "def _connect(self,\n                 engine: str = None,\n                 interface: str = None,\n                 host: str = None,\n                 port: int = None,\n                 database: str = None,\n                 driver: str = None,\n                 dsn: str = None,\n                 odbc_connection_string: str = None,\n                 user: str = None,\n                 password: str = None,\n                 autocommit: bool = True,\n                 charset: str = \"utf8\",\n                 use_unicode: bool = True) -> bool:\n        # Check engine\n        if engine == ENGINE_MYSQL:\n            self.flavour = MySQL()\n            self.schema = database\n        elif engine == ENGINE_SQLSERVER:\n            self.flavour = SQLServer()\n            if database:\n                self.schema = database\n            else:\n                self.schema = \"dbo\"  # default for SQL server\n        elif engine == ENGINE_ACCESS:\n            self.flavour = Access()\n            self.schema = \"dbo\"  # default for SQL server\n        else:\n            raise ValueError(\"Unknown engine\")\n\n        # Default interface\n        if interface is None:\n            if engine == ENGINE_MYSQL:\n                interface = INTERFACE_MYSQL\n            else:\n                interface = INTERFACE_ODBC\n\n        # Default port\n        if port is None:\n            if engine == ENGINE_MYSQL:\n                port = 3306\n            elif engine == ENGINE_SQLSERVER:\n                port = 1433\n\n        # Default driver\n        if driver is None:\n            if engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n                driver = \"{MySQL ODBC 5.1 Driver}\"\n\n        self._engine = engine\n        self._interface = interface\n        self._server = host\n        self._port = port\n        self._database = database\n        self._user = user\n        self._password = password\n        self._charset = charset\n        self._use_unicode = use_unicode\n        self.autocommit = autocommit\n\n        # Report intent\n        log.info(\n            \"Opening database: engine={e}, interface={i}, \"\n            \"use_unicode={u}, autocommit={a}\".format(\n                e=engine, i=interface, u=use_unicode, a=autocommit))\n\n        # Interface\n        if interface == INTERFACE_MYSQL:\n            if pymysql:\n                self.db_pythonlib = PYTHONLIB_PYMYSQL\n            elif MySQLdb:\n                self.db_pythonlib = PYTHONLIB_MYSQLDB\n            else:\n                raise ImportError(_MSG_MYSQL_DRIVERS_UNAVAILABLE)\n        elif interface == INTERFACE_ODBC:\n            if not pyodbc:\n                raise ImportError(_MSG_PYODBC_UNAVAILABLE)\n            self.db_pythonlib = PYTHONLIB_PYODBC\n        elif interface == INTERFACE_JDBC:\n            if not jaydebeapi:\n                raise ImportError(_MSG_JDBC_UNAVAILABLE)\n            if host is None:\n                raise ValueError(\"Missing host parameter\")\n            if port is None:\n                raise ValueError(\"Missing port parameter\")\n            # if database is None:\n            #     raise ValueError(\"Missing database parameter\")\n            if user is None:\n                raise ValueError(\"Missing user parameter\")\n            self.db_pythonlib = PYTHONLIB_JAYDEBEAPI\n        else:\n            raise ValueError(\"Unknown interface\")\n\n        # ---------------------------------------------------------------------\n        # Connect\n        # ---------------------------------------------------------------------\n        if engine == ENGINE_MYSQL and interface == INTERFACE_MYSQL:\n            # Connects to a MySQL database via MySQLdb/PyMySQL.\n            # http://dev.mysql.com/doc/refman/5.1/en/connector-odbc-configuration-connection-parameters.html  # noqa\n            # http://code.google.com/p/pyodbc/wiki/ConnectionStrings\n\n            # Between MySQLdb 1.2.3 and 1.2.5, the DateTime2literal function\n            # stops producing e.g.\n            #   '2014-01-03 18:15:51'\n            # and starts producing e.g.\n            #   '2014-01-03 18:15:51.842097+00:00'.\n            # Let's fix that...\n            datetimetype = datetime.datetime  # as per MySQLdb times.py\n            converters = mysql.converters.conversions.copy()\n            converters[datetimetype] = datetime2literal_rnc\n            # See also:\n            #   http://stackoverflow.com/questions/11053941\n            log.info(\n                \"{i} connect: host={h}, port={p}, user={u}, \"\n                \"database={d}\".format(\n                    i=interface, h=host, p=port, u=user, d=database))\n            self.db = mysql.connect(\n                host=host,\n                port=port,\n                user=user,\n                passwd=password,\n                db=database,\n                charset=charset,\n                use_unicode=use_unicode,\n                conv=converters\n            )\n            # noinspection PyCallingNonCallable\n            self.db.autocommit(autocommit)\n            # http://mysql-python.sourceforge.net/MySQLdb.html\n            # http://dev.mysql.com/doc/refman/5.0/en/mysql-autocommit.html\n            # https://github.com/PyMySQL/PyMySQL/blob/master/pymysql/connections.py  # noqa\n\n            # MySQL character sets and collations:\n            #   http://dev.mysql.com/doc/refman/5.1/en/charset.html\n            # Create a database using UTF8:\n            # ... CREATE DATABASE mydb DEFAULT CHARACTER SET utf8\n            #     DEFAULT COLLATE utf8_general_ci;\n            # What is my database using?\n            # ... SHOW VARIABLES LIKE 'character_set_%';\n            # Change a database character set:\n            # ... ALTER DATABASE mydatabasename charset=utf8;\n            # http://docs.moodle.org/23/en/\n            #        Converting_your_MySQL_database_to_UTF8\n            #\n            # Python talking to MySQL in Unicode:\n            # http://www.harelmalka.com/?p=81\n            # http://stackoverflow.com/questions/6001104\n\n        elif engine == ENGINE_MYSQL and interface == INTERFACE_ODBC:\n            log.info(\n                \"ODBC connect: DRIVER={dr};SERVER={s};PORT={p};\"\n                \"DATABASE={db};USER={u};PASSWORD=[censored]\".format(\n                    dr=driver, s=host, p=port,\n                    db=database, u=user))\n            dsn = (\n                \"DRIVER={0};SERVER={1};PORT={2};DATABASE={3};\"\n                \"USER={4};PASSWORD={5}\".format(driver, host, port, database,\n                                               user, password)\n            )\n            self.db = pyodbc.connect(dsn)\n            self.db.autocommit = autocommit\n            # http://stackoverflow.com/questions/1063770\n\n        elif engine == ENGINE_MYSQL and interface == INTERFACE_JDBC:\n            # https://help.ubuntu.com/community/JDBCAndMySQL\n            # https://github.com/baztian/jaydebeapi/issues/1\n            jclassname = \"com.mysql.jdbc.Driver\"\n            url = \"jdbc:mysql://{host}:{port}/{database}\".format(\n                host=host, port=port, database=database)\n            driver_args = [url, user, password]\n            jars = None\n            libs = None\n            log.info(\n                \"JDBC connect: jclassname={jclassname}, \"\n                \"url={url}, user={user}, password=[censored]\".format(\n                    jclassname=jclassname,\n                    url=url,\n                    user=user,\n                )\n            )\n            self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n\n        elif engine == ENGINE_SQLSERVER and interface == INTERFACE_ODBC:\n            # SQL Server:\n            # http://code.google.com/p/pyodbc/wiki/ConnectionStrings\n            if odbc_connection_string:\n                log.info(\"Using raw ODBC connection string [censored]\")\n                connectstring = odbc_connection_string\n            elif dsn:\n                log.info(\n                    \"ODBC connect: DSN={dsn};UID={u};PWD=[censored]\".format(\n                        dsn=dsn, u=user))\n                connectstring = \"DSN={};UID={};PWD={}\".format(dsn, user,\n                                                              password)\n            else:\n                log.info(\n                    \"ODBC connect: DRIVER={dr};SERVER={s};DATABASE={db};\"\n                    \"UID={u};PWD=[censored]\".format(\n                        dr=driver, s=host, db=database, u=user))\n                connectstring = (\n                    \"DRIVER={};SERVER={};DATABASE={};UID={};PWD={}\".format(\n                        driver, host, database, user, password)\n                )\n            self.db = pyodbc.connect(connectstring, unicode_results=True)\n            self.db.autocommit = autocommit\n            # http://stackoverflow.com/questions/1063770\n\n        elif engine == ENGINE_SQLSERVER and interface == INTERFACE_JDBC:\n            # jar tvf sqljdbc41.jar\n            # https://msdn.microsoft.com/en-us/sqlserver/aa937724.aspx\n            # https://msdn.microsoft.com/en-us/library/ms378428(v=sql.110).aspx\n            # https://msdn.microsoft.com/en-us/library/ms378988(v=sql.110).aspx\n            jclassname = 'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n            urlstem = 'jdbc:sqlserver://{host}:{port};'.format(\n                host=host,\n                port=port\n            )\n            nvp = {}\n            if database:\n                nvp['databaseName'] = database\n            nvp['user'] = user\n            nvp['password'] = password\n            nvp['responseBuffering'] = 'adaptive'  # default is 'full'\n            # ... THIS CHANGE (responseBuffering = adaptive) stops the JDBC\n            # driver crashing on cursor close [in a socket recv() call] when\n            # it's fetched a VARBINARY(MAX) field.\n            nvp['selectMethod'] = 'cursor'  # trying this; default is 'direct'\n            url = urlstem + ';'.join(\n                '{}={}'.format(x, y) for x, y in nvp.items())\n\n            nvp['password'] = '[censored]'\n            url_censored = urlstem + ';'.join(\n                '{}={}'.format(x, y) for x, y in nvp.items())\n            log.info(\n                'jdbc connect: jclassname={jclassname}, url = {url}'.format(\n                    jclassname=jclassname,\n                    url=url_censored\n                )\n            )\n\n            driver_args = [url]\n            jars = None\n            libs = None\n            self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit)\n\n        elif engine == ENGINE_ACCESS and interface == INTERFACE_ODBC:\n            dsn = \"DSN={}\".format(dsn)\n            log.info(\"ODBC connect: DSN={}\", dsn)\n            self.db = pyodbc.connect(dsn)\n            self.db.autocommit = autocommit\n            # http://stackoverflow.com/questions/1063770\n\n        else:\n            raise ValueError(\n                \"Unknown 'engine'/'interface' combination: {}/{}\".format(\n                    engine, interface\n                )\n            )\n\n        return True", "relevance": 3}
{"query": "connect to sql", "function": "def sqlalchemy_mysql_trace(request):\n    try:\n        engine = sqlalchemy.create_engine(\n            'mysql+mysqlconnector://{}:{}@{}'.format('root', MYSQL_PASSWORD,\n                                                     DB_HOST))\n        conn = engine.connect()\n\n        query = 'SELECT 2*3'\n\n        result_set = conn.execute(query)\n\n        result = []\n\n        for item in result_set:\n            result.append(item)\n\n        return HttpResponse(str(result))\n\n    except Exception:\n        msg = \"Query failed. Check your env vars for connection settings.\"\n        return HttpResponse(msg, status=500)", "relevance": 2}
{"query": "convert a date string into yyyymmdd", "function": "def string_to_date(input):\n    \"\"\"Convert string to date object.\n\n    :param input: the date string to parse\n    :type input: str\n    :returns: the parsed datetime object\n    :rtype: datetime.datetime\n    \"\"\"\n    # try date formats --mmdd, --mm-dd, yyyymmdd, yyyy-mm-dd and datetime\n    # formats yyyymmddThhmmss, yyyy-mm-ddThh:mm:ss, yyyymmddThhmmssZ,\n    # yyyy-mm-ddThh:mm:ssZ.\n    for format_string in (\"--%m%d\", \"--%m-%d\", \"%Y%m%d\", \"%Y-%m-%d\",\n                          \"%Y%m%dT%H%M%S\", \"%Y-%m-%dT%H:%M:%S\",\n                          \"%Y%m%dT%H%M%SZ\", \"%Y-%m-%dT%H:%M:%SZ\"):\n        try:\n            return datetime.strptime(input, format_string)\n        except ValueError:\n            pass\n    # try datetime formats yyyymmddThhmmsstz and yyyy-mm-ddThh:mm:sstz where tz\n    # may look like -06:00.\n    for format_string in (\"%Y%m%dT%H%M%S%z\", \"%Y-%m-%dT%H:%M:%S%z\"):\n        try:\n            return datetime.strptime(''.join(input.rsplit(\":\", 1)),\n                                     format_string)\n        except ValueError:\n            pass\n    raise ValueError", "relevance": 3}
{"query": "convert a date string into yyyymmdd", "function": "def DisjoinCalendars(self, cutoff):\n    \"\"\"Forces the old and new calendars to be disjoint about a cutoff date.\n\n    This truncates the service periods of the old schedule so that service\n    stops one day before the given cutoff date and truncates the new schedule\n    so that service only begins on the cutoff date.\n\n    Args:\n      cutoff: The cutoff date as a string in YYYYMMDD format. The timezone\n              is the same as used in the calendar.txt file.\n    \"\"\"\n\n    def TruncatePeriod(service_period, start, end):\n      \"\"\"Truncate the service period to into the range [start, end].\n\n      Args:\n        service_period: The service period to truncate.\n        start: The start date as a string in YYYYMMDD format.\n        end: The end date as a string in YYYYMMDD format.\n      \"\"\"\n      service_period.start_date = max(service_period.start_date, start)\n      service_period.end_date = min(service_period.end_date, end)\n      dates_to_delete = []\n      for k in service_period.date_exceptions:\n        if (k < start) or (k > end):\n          dates_to_delete.append(k)\n      for k in dates_to_delete:\n        del service_period.date_exceptions[k]\n\n    # find the date one day before cutoff\n    year = int(cutoff[:4])\n    month = int(cutoff[4:6])\n    day = int(cutoff[6:8])\n    cutoff_date = datetime.date(year, month, day)\n    one_day_delta = datetime.timedelta(days=1)\n    before = (cutoff_date - one_day_delta).strftime('%Y%m%d')\n\n    for a in self.feed_merger.a_schedule.GetServicePeriodList():\n      TruncatePeriod(a, 0, before)\n    for b in self.feed_merger.b_schedule.GetServicePeriodList():\n      TruncatePeriod(b, cutoff, '9'*8)", "relevance": 2}
{"query": "convert a date string into yyyymmdd", "function": "def to_date(self):\n        \"\"\"\n        construct datetime.date instance represented calendar date of BusinessDate instance\n\n        :return datetime.date:\n        \"\"\"\n        y, m, d = self.to_ymd()\n        return date(y, m, d)", "relevance": 1}
{"query": "convert a utc time to epoch", "function": "def get_time_array(self,\n                       datetime_simulation_start=None,\n                       simulation_time_step_seconds=None,\n                       return_datetime=False,\n                       time_index_array=None):\n        \"\"\"\n        This method extracts or generates an array of time.\n        The new version of RAPID output has the time array stored.\n        However, the old version requires the user to know when the\n        simulation began and the time step of the output.\n\n        Parameters\n        ----------\n        datetime_simulation_start: :obj:`datetime.datetime`, optional\n            The start datetime of the simulation. Only required if the time\n            variable is not included in the file.\n        simulation_time_step_seconds: int, optional\n            The time step of the file in seconds. Only required if the time\n            variable is not included in the file.\n        return_datetime: bool, optional\n            If true, it converts the data to a list of datetime objects.\n            Default is False.\n        time_index_array: list or :obj:`numpy.array`, optional\n            This is used to extract the datetime values by index from the main\n            list. This can be from the *get_time_index_range* function.\n\n        Returns\n        -------\n        list:\n            An array of integers representing seconds since Jan 1, 1970 UTC\n            or datetime objects if *return_datetime* is set to True.\n\n        These examples demonstrates how to retrieve or generate a time array\n        to go along with your RAPID streamflow series.\n\n\n        CF-Compliant Qout File Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            with RAPIDDataset(path_to_rapid_qout) as qout_nc:\n                #retrieve integer timestamp array\n                time_array = qout_nc.get_time_array()\n\n                #or, to get datetime array\n                time_datetime = qout_nc.get_time_array(return_datetime=True)\n\n\n        Legacy Qout File Example:\n\n        .. code:: python\n\n            from RAPIDpy import RAPIDDataset\n\n            path_to_rapid_qout = '/path/to/Qout.nc'\n            with RAPIDDataset(path_to_rapid_qout,\n                              datetime_simulation_start=datetime(1980, 1, 1),\n                              simulation_time_step_seconds=3 * 3600)\\\n                    as qout_nc:\n\n                #retrieve integer timestamp array\n                time_array = qout_nc.get_time_array()\n\n                #or, to get datetime array\n                time_datetime = qout_nc.get_time_array(return_datetime=True)\n\n        \"\"\"\n        # Original Qout file\n        if datetime_simulation_start is not None:\n            self.datetime_simulation_start = datetime_simulation_start\n        if simulation_time_step_seconds is not None:\n            self.simulation_time_step_seconds = simulation_time_step_seconds\n\n        epoch = datetime.datetime(1970, 1, 1, tzinfo=utc)\n        time_units = \"seconds since {0}\".format(epoch)\n\n        # CF-1.6 compliant file\n        if self.is_time_variable_valid():\n            time_array = self.qout_nc.variables['time'][:]\n            if self.qout_nc.variables['time'].units:\n                time_units = self.qout_nc.variables['time'].units\n\n        # Original Qout file\n        elif self._is_legacy_time_valid():\n            initial_time_seconds = ((self.datetime_simulation_start\n                                    .replace(tzinfo=utc) - epoch)\n                                    .total_seconds() +\n                                    self.simulation_time_step_seconds)\n            final_time_seconds = (initial_time_seconds +\n                                  self.size_time *\n                                  self.simulation_time_step_seconds)\n            time_array = np.arange(initial_time_seconds,\n                                   final_time_seconds,\n                                   self.simulation_time_step_seconds)\n        else:\n            raise ValueError(\"This file does not contain the time\"\n                             \" variable. To get time array, add\"\n                             \" datetime_simulation_start and\"\n                             \" simulation_time_step_seconds\")\n\n        if time_index_array is not None:\n            time_array = time_array[time_index_array]\n\n        if return_datetime:\n            time_array = num2date(time_array, time_units)\n\n            if self.out_tzinfo is not None:\n                for i in xrange(len(time_array)):\n                    # convert time to output timezone\n                    time_array[i] = utc.localize(time_array[i]) \\\n                                       .astimezone(self.out_tzinfo) \\\n                                       .replace(tzinfo=None)\n\n        return time_array", "relevance": 3}
{"query": "convert a utc time to epoch", "function": "def _datetime_to_utc_int(date):\n    \"\"\"Convert the integer UTC time value into a local datetime.\"\"\"\n    if date is None:\n      return None\n\n    # Convert localized datetime to a UTC integer\n    epoch = dsub_util.replace_timezone(datetime.utcfromtimestamp(0), pytz.utc)\n    return (date - epoch).total_seconds()", "relevance": 2}
{"query": "convert a utc time to epoch", "function": "def epoch_to_human_time(epoch_time):\n    \"\"\"Converts an epoch timestamp to human readable time.\n\n    This essentially converts an output of get_current_epoch_time to an output\n    of get_current_human_time\n\n    Args:\n        epoch_time: An integer representing an epoch timestamp in milliseconds.\n\n    Returns:\n        A time string representing the input time.\n        None if input param is invalid.\n    \"\"\"\n    if isinstance(epoch_time, int):\n        try:\n            d = datetime.datetime.fromtimestamp(epoch_time / 1000)\n            return d.strftime(\"%m-%d-%Y %H:%M:%S \")\n        except ValueError:\n            return None", "relevance": 1}
{"query": "convert decimal to hex", "function": "def int_to_hex(i):\n    \"\"\"Create a hex-representation of the given serial.\n\n    >>> int_to_hex(12345678)\n    'BC:61:4E'\n    \"\"\"\n    s = hex(i)[2:].upper()\n    if six.PY2 is True and isinstance(i, long):  # pragma: only py2  # NOQA\n        # Strip the \"L\" suffix, since hex(1L) -> 0x1L.\n        # NOTE: Do not convert to int earlier. int(<very-large-long>) is still long\n        s = s[:-1]\n    return add_colons(s)", "relevance": 3}
{"query": "convert decimal to hex", "function": "def dec2str(n):\n    \"\"\"\n    decimal number to string.\n    \"\"\"\n    s = hex(int(n))[2:].rstrip('L')\n    if len(s) % 2 != 0:\n        s = '0' + s\n    return hex2str(s)", "relevance": 2}
{"query": "convert decimal to hex", "function": "def _hexvalue_to_rgb(hexvalue):\n        \"\"\"\n        Converts the hexvalue used by tuya for colour representation into\n        an RGB value.\n        \n        Args:\n            hexvalue(string): The hex representation generated by BulbDevice._rgb_to_hexvalue()\n        \"\"\"\n        r = int(hexvalue[0:2], 16)\n        g = int(hexvalue[2:4], 16)\n        b = int(hexvalue[4:6], 16)\n\n        return (r, g, b)", "relevance": 1}
{"query": "convert html to pdf", "function": "def html_to_pdf(tmp_filenames, output_directory, lang_options):\n    input_html = output_directory + \"/\" + tmp_filenames[0]\n    wkthml_cmd = [\"wkhtmltopdf\"]\n    # Basic margins etc\n    wkthml_cmd.extend([\"--margin-left\", \"18\"])\n    wkthml_cmd.extend([\"--margin-right\", \"18\"])\n    wkthml_cmd.extend([\"--page-size\", \"Letter\"])\n    # Header and Footer\n    header_file = pkg_resources.resource_filename(\"wrc\", \"data/header.html\")\n    footer_file = pkg_resources.resource_filename(\"wrc\", \"data/footer.html\")\n    wkthml_cmd.extend([\"--header-html\", header_file])\n    wkthml_cmd.extend([\"--footer-html\", footer_file])\n    wkthml_cmd.extend([\"--header-spacing\", \"8\"])\n    wkthml_cmd.extend([\"--footer-spacing\", \"8\"])\n    wkthml_cmd.append(input_html)\n    wkthml_cmd.append(output_directory + \"/\" + lang_options['pdf'] + '.pdf')\n    try:\n        check_call(wkthml_cmd)\n        print \"Successfully generated pdf file!\"\n        print \"Cleaning temporary file (%s)...\" % input_html\n        os.remove(input_html)\n    except CalledProcessError as err:\n        print \"Error while generating pdf:\"\n        print err\n        sys.exit(1)\n    except OSError as err:\n        print \"Error when running command \\\"\" + \" \".join(wkthml_cmd) + \"\\\"\"\n        print err\n        sys.exit(1)", "relevance": 4}
{"query": "convert html to pdf", "function": "def add_to_writer(self,\n                      writer: PdfFileWriter,\n                      start_recto: bool = True) -> None:\n        \"\"\"\n        Add the PDF described by this class to a PDF writer.\n\n        Args:\n            writer: a :class:`PyPDF2.PdfFileWriter`\n            start_recto: start a new right-hand page?\n\n        \"\"\"\n        if self.is_html:\n            pdf = get_pdf_from_html(\n                html=self.html,\n                header_html=self.header_html,\n                footer_html=self.footer_html,\n                wkhtmltopdf_filename=self.wkhtmltopdf_filename,\n                wkhtmltopdf_options=self.wkhtmltopdf_options)\n            append_memory_pdf_to_writer(pdf, writer, start_recto=start_recto)\n        elif self.is_filename:\n            if start_recto and writer.getNumPages() % 2 != 0:\n                writer.addBlankPage()\n            writer.appendPagesFromReader(PdfFileReader(\n                open(self.filename, 'rb')))\n        else:\n            raise AssertionError(\"PdfPlan: shouldn't get here!\")", "relevance": 3}
{"query": "convert html to pdf", "function": "def to_pdf(\n        self,\n        outFileName,\n        imageFileName=None,\n        showBoundingboxes=False,\n        fontname=\"Helvetica\",\n        invisibleText=False,\n        interwordSpaces=False,\n    ):\n        \"\"\"\n        Creates a PDF file with an image superimposed on top of the text.\n        Text is positioned according to the bounding box of the lines in\n        the hOCR file.\n        The image need not be identical to the image used to create the hOCR\n        file.\n        It can have a lower resolution, different color mode, etc.\n        \"\"\"\n        # create the PDF file\n        # page size in points (1/72 in.)\n        pdf = Canvas(outFileName, pagesize=(self.width, self.height), pageCompression=1)\n\n        # draw bounding box for each paragraph\n        # light blue for bounding box of paragraph\n        pdf.setStrokeColorRGB(0, 1, 1)\n        # light blue for bounding box of paragraph\n        pdf.setFillColorRGB(0, 1, 1)\n        pdf.setLineWidth(0)  # no line for bounding box\n        for elem in self.hocr.findall(\".//%sp[@class='%s']\" % (self.xmlns, \"ocr_par\")):\n\n            elemtxt = self._get_element_text(elem).rstrip()\n            if len(elemtxt) == 0:\n                continue\n\n            pxl_coords = self.element_coordinates(elem)\n            pt = self.pt_from_pixel(pxl_coords)\n\n            # draw the bbox border\n            if showBoundingboxes:\n                pdf.rect(\n                    pt.x1, self.height - pt.y2, pt.x2 - pt.x1, pt.y2 - pt.y1, fill=1\n                )\n\n        found_lines = False\n        for line in self.hocr.findall(\n            \".//%sspan[@class='%s']\" % (self.xmlns, \"ocr_line\")\n        ):\n            found_lines = True\n            self._do_line(\n                pdf,\n                line,\n                \"ocrx_word\",\n                fontname,\n                invisibleText,\n                interwordSpaces,\n                showBoundingboxes,\n            )\n\n        if not found_lines:\n            # Tesseract did not report any lines (just words)\n            root = self.hocr.find(\".//%sdiv[@class='%s']\" % (self.xmlns, \"ocr_page\"))\n            self._do_line(\n                pdf,\n                root,\n                \"ocrx_word\",\n                fontname,\n                invisibleText,\n                interwordSpaces,\n                showBoundingboxes,\n            )\n        # put the image on the page, scaled to fill the page\n        if imageFileName is not None:\n            pdf.drawImage(imageFileName, 0, 0, width=self.width, height=self.height)\n\n        # finish up the page and save it\n        pdf.showPage()\n        pdf.save()", "relevance": 2}
{"query": "convert int to bool", "function": "def _convert(value, default=None, convert=None):\n    def convert_bool(value):\n        if value.lower() in ('true', 't', 'yes', 'y'):\n            return True\n        if value.lower() in ('false', 'f', 'no', 'n'):\n            return False\n        raise ValueError('{} cannot be converted to bool'.format(value))\n\n    if value == '':\n        value = None\n\n    if convert is None:\n        if default is not None:\n            convert = type(default)\n        else:\n            convert = str\n\n    if convert == bool:\n        convert = convert_bool\n\n    if value is None:\n        return default\n    else:\n        return convert(value)", "relevance": 3}
{"query": "convert int to bool", "function": "def str_to_bool(s):\n    s = s.lower()\n    if s in ('true', 'yes', '1'):\n        return True\n    if s in ('false', 'no', '0'):\n        return False\n    raise ConversionError(\"failed to convert {0} to bool\".format(s))", "relevance": 2}
{"query": "convert int to bool", "function": "def to_bool(self, value):\n\t \tif value == None:\n\t \t\treturn False\n\t \telif isinstance(value, bool):\n\t \t\treturn value\n\t \telse:\n\t \t\tif str(value).lower() in [\"true\", \"1\", \"yes\"]:\n\t \t\t\treturn True\n\t \t\telse:\n\t \t\t\treturn False", "relevance": 1}
{"query": "convert int to string", "function": "def string_to_int( s ):\n  \"\"\"Convert a string of bytes into an integer, as per X9.62.\"\"\"\n  result = 0\n  for c in s:\n    if not isinstance(c, int): c = ord( c )\n    result = 256 * result + c\n  return result", "relevance": 4}
{"query": "convert int to string", "function": "def convert_to_integer(self, str):\n        if str.startswith('0x') or str.startswith('0X'):\n            return int(str, 16)\n        elif str.startswith('2_'):\n            return int(str[2:], 2)\n        else:\n            return int(str)", "relevance": 2}
{"query": "convert int to string", "function": "def int_to_decimal_str(integer):\n    \"\"\"\n    Helper to convert integers (representing cents) into decimal currency\n    string. WARNING: DO NOT TRY TO DO THIS BY DIVISION, FLOATING POINT\n    ERRORS ARE NO FUN IN FINANCIAL SYSTEMS.\n    @param integer The amount in cents\n    @return string The amount in currency with full stop decimal separator\n    \"\"\"\n    int_string = str(integer)\n    if len(int_string) < 2:\n        return \"0.\" + int_string.zfill(2)\n    else:\n        return int_string[:-2] + \".\" + int_string[-2:]", "relevance": 1}
{"query": "convert json to csv", "function": "def _tsv2json(in_tsv, out_json, index_column, additional_metadata=None,\n              drop_columns=None, enforce_case=True):\n    \"\"\"\n    Convert metadata from TSV format to JSON format.\n\n    Parameters\n    ----------\n    in_tsv: str\n        Path to the metadata in TSV format.\n    out_json: str\n        Path where the metadata should be saved in JSON format after\n        conversion. If this is None, then a dictionary is returned instead.\n    index_column: str\n        Name of the column in the TSV to be used as an index (top-level key in\n        the JSON).\n    additional_metadata: dict\n        Any additional metadata that should be applied to all entries in the\n        JSON.\n    drop_columns: list\n        List of columns from the input TSV to be dropped from the JSON.\n    enforce_case: bool\n        Indicates whether BIDS case conventions should be followed. Currently,\n        this means that index fields (column names in the associated data TSV)\n        use snake case and other fields use camel case.\n\n    Returns\n    -------\n    str\n        Path to the metadata saved in JSON format.\n    \"\"\"\n    import pandas as pd\n    # Adapted from https://dev.to/rrampage/snake-case-to-camel-case-and- ...\n    # back-using-regular-expressions-and-python-m9j\n    re_to_camel = r'(.*?)_([a-zA-Z0-9])'\n    re_to_snake = r'(^.+?|.*?)((?<![_A-Z])[A-Z]|(?<![_0-9])[0-9]+)'\n\n    def snake(match):\n        return '{}_{}'.format(match.group(1).lower(), match.group(2).lower())\n\n    def camel(match):\n        return '{}{}'.format(match.group(1), match.group(2).upper())\n\n    # from fmriprep\n    def less_breakable(a_string):\n        \"\"\" hardens the string to different envs (i.e. case insensitive, no\n        whitespace, '#' \"\"\"\n        return ''.join(a_string.split()).strip('#')\n\n    drop_columns = drop_columns or []\n    additional_metadata = additional_metadata or {}\n    tsv_data = pd.read_csv(in_tsv, '\\t')\n    for k, v in additional_metadata.items():\n        tsv_data[k] = v\n    for col in drop_columns:\n        tsv_data.drop(labels=col, axis='columns', inplace=True)\n    tsv_data.set_index(index_column, drop=True, inplace=True)\n    if enforce_case:\n        tsv_data.index = [re.sub(re_to_snake, snake,\n                                 less_breakable(i), 0).lower()\n                          for i in tsv_data.index]\n        tsv_data.columns = [re.sub(re_to_camel, camel,\n                                   less_breakable(i).title(), 0)\n                            for i in tsv_data.columns]\n    json_data = tsv_data.to_json(orient='index')\n    json_data = json.JSONDecoder(\n        object_pairs_hook=OrderedDict).decode(json_data)\n    if out_json is None:\n        return json_data\n\n    with open(out_json, 'w') as f:\n        json.dump(json_data, f, indent=4)\n    return out_json", "relevance": 3}
{"query": "convert json to csv", "function": "def get_csv_from_json(d):\n    \"\"\"\n    Get CSV values when mixed into json data. Pull out the CSV data and put it into a dictionary.\n    :param dict d: JSON with CSV values\n    :return dict: CSV values. (i.e. { CSVFilename1: { Column1: [Values], Column2: [Values] }, CSVFilename2: ... }\n    \"\"\"\n    logger_jsons.info(\"enter get_csv_from_json\")\n    csv_data = OrderedDict()\n\n    if \"paleoData\" in d:\n        csv_data = _get_csv_from_section(d, \"paleoData\", csv_data)\n\n    if \"chronData\" in d:\n        csv_data = _get_csv_from_section(d, \"chronData\", csv_data)\n\n    logger_jsons.info(\"exit get_csv_from_json\")\n    return csv_data", "relevance": 2}
{"query": "convert json to csv", "function": "def serialize(graph: BELGraph, csv, sif, gsea, graphml, json, bel):\n    \"\"\"Serialize a graph to various formats.\"\"\"\n    if csv:\n        log.info('Outputting CSV to %s', csv)\n        to_csv(graph, csv)\n\n    if sif:\n        log.info('Outputting SIF to %s', sif)\n        to_sif(graph, sif)\n\n    if graphml:\n        log.info('Outputting GraphML to %s', graphml)\n        to_graphml(graph, graphml)\n\n    if gsea:\n        log.info('Outputting GRP to %s', gsea)\n        to_gsea(graph, gsea)\n\n    if json:\n        log.info('Outputting JSON to %s', json)\n        to_json_file(graph, json)\n\n    if bel:\n        log.info('Outputting BEL to %s', bel)\n        to_bel(graph, bel)", "relevance": 1}
{"query": "convert string to number", "function": "def str2num(num_str, convert_type=\"float\"):\n    num = float(grep_comma(num_str))\n    return num if convert_type == \"float\" else int(num)", "relevance": 3}
{"query": "convert string to number", "function": "def pack_unsigned_int(number, size, le):\n  if not isinstance(number, int):\n    raise StructError(\"argument for i,I,l,L,q,Q,h,H must be integer\")\n  if number < 0:\n    raise TypeError(\"can't convert negative long to unsigned\")\n  if number > (1 << (8 * size)) - 1:\n    raise OverflowError(\"Number:%i too large to convert\" % number)\n  return pack_int(number, size, le)", "relevance": 2}
{"query": "convert string to number", "function": "def pack_unsigned_int(number, size, le):\n  if not isinstance(number, int):\n    raise StructError(\"argument for i,I,l,L,q,Q,h,H must be integer\")\n  if number < 0:\n    raise TypeError(\"can't convert negative long to unsigned\")\n  if number > (1 << (8 * size)) - 1:\n    raise OverflowError(\"Number:%i too large to convert\" % number)\n  return pack_int(number, size, le)", "relevance": 1}
{"query": "converting uint8 array to image", "function": "def from_array(x, frame='unspecified'):\n        \"\"\" Converts an array of data to an Image based on the values in the array and the data format. \"\"\"\n\n        if not Image.can_convert(x):\n            raise ValueError('Cannot convert array to an Image!')\n\n        dtype = x.dtype\n        height = x.shape[0]\n        width = x.shape[1]\n        channels = 1\n        if len(x.shape) == 3:\n            channels = x.shape[2]\n        if dtype == np.uint8:\n            if channels == 1:\n                if np.any((x % BINARY_IM_MAX_VAL) > 0):\n                    return GrayscaleImage(x, frame)\n                return BinaryImage(x, frame)\n            elif channels == 3:\n                return ColorImage(x, frame)\n            else:\n                raise ValueError(\n                    'No available image conversion for uint8 array with 2 channels')\n        elif dtype == np.uint16:\n            if channels != 1:\n                raise ValueError(\n                    'No available image conversion for uint16 array with 2 or 3 channels')\n            return GrayscaleImage(x, frame)\n        elif dtype == np.float32 or dtype == np.float64:\n            if channels == 1:\n                return DepthImage(x, frame)\n            elif channels == 2:\n                return GdImage(x, frame)\n            elif channels == 3:\n                logging.warning('Converting float array to uint8')\n                return ColorImage(x.astype(np.uint8), frame)\n            return RgbdImage(x, frame)\n        else:\n            raise ValueError(\n                'Conversion for dtype %s not supported!' %\n                (str(dtype)))", "relevance": 3}
{"query": "converting uint8 array to image", "function": "def uint32_to_uint8(cls, img):\n        \"\"\"\n        Cast uint32 RGB image to 4 uint8 channels.\n        \"\"\"\n        return np.flipud(img.view(dtype=np.uint8).reshape(img.shape + (4,)))", "relevance": 2}
{"query": "converting uint8 array to image", "function": "def _validate_image_datatype(self, img_array):\n        \"\"\"\n        Only uint8 and uint16 images are currently supported.\n        \"\"\"\n        if img_array.dtype != np.uint8 and img_array.dtype != np.uint16:\n            msg = (\"Only uint8 and uint16 datatypes are currently supported \"\n                   \"when writing.\")\n            raise RuntimeError(msg)", "relevance": 1}
{"query": "copy to clipboard", "function": "def copy_to_clipboard(i): # pragma: no cover \n    \"\"\"\n    Input:  {\n              string - string to copy\n\n    Output: {\n              return       - return code =  0, if successful\n                                         >  0, if error\n              (error)      - error text if return > 0\n            }\n    \"\"\"\n\n    s=i['string']\n\n    failed=False\n    ee=''\n\n    # Try to load pyperclip (seems to work fine on Windows)\n    try:\n       import pyperclip\n    except Exception as e:\n       ee=format(e)\n       failed=True\n       pass\n\n    if not failed:\n       pyperclip.copy(s)\n    else:\n       failed=False\n\n       # Try to load Tkinter\n       try:\n          from Tkinter import Tk\n       except ImportError as e:\n          ee=format(e)\n          failed=True\n          pass\n\n       if failed:\n          failed=False\n          try:\n             from tkinter import Tk\n          except ImportError as e:\n             ee=format(e)\n             failed=True\n             pass\n\n       if failed:\n          return {'return':1, 'error':'none of pyperclip/Tkinter/tkinter packages is installed'}\n\n       # Copy to clipboard\n       try:\n          r = Tk()\n          r.withdraw()\n          r.clipboard_clear()\n          r.clipboard_append(s)\n          r.destroy()\n       except Exception as e:\n          return {'return':1, 'error':'problem copying string to clipboard ('+format(e)+')'}\n\n    return {'return':0}", "relevance": 4}
{"query": "copy to clipboard", "function": "def copy_request_to_clipboard(self):\n        txt = \" \".join(self.context_model.request)\n        clipboard = app.clipboard()\n        clipboard.setText(txt)\n        with app.status(\"Copied request to clipboard\"):\n            pass", "relevance": 3}
{"query": "copy to clipboard", "function": "def paste(self):\n        \"\"\"Import text/data/code from clipboard\"\"\"\n        clipboard = QApplication.clipboard()\n        cliptext = ''\n        if clipboard.mimeData().hasText():\n            cliptext = to_text_string(clipboard.text())\n        if cliptext.strip():\n            self.import_from_string(cliptext, title=_(\"Import from clipboard\"))\n        else:\n            QMessageBox.warning(self, _( \"Empty clipboard\"),\n                                _(\"Nothing to be imported from clipboard.\"))", "relevance": 2}
{"query": "copying a file to a path", "function": "def copy_dir(self, path):\n        \"\"\"\n        Recursively copy directory\n        \"\"\"\n        for directory in path:\n            if os.path.isdir(path):\n                full_path = os.path.join(self.archive_dir, directory.lstrip('/'))\n                logger.debug(\"Copying %s to %s\", directory, full_path)\n                shutil.copytree(directory, full_path)\n            else:\n                logger.debug(\"Not a directory: %s\", directory)\n        return path", "relevance": 3}
{"query": "copying a file to a path", "function": "def copy_dir(self, path):\n        \"\"\"\n        Recursively copy directory\n        \"\"\"\n        for directory in path:\n            if os.path.isdir(path):\n                full_path = os.path.join(self.archive_dir, directory.lstrip('/'))\n                logger.debug(\"Copying %s to %s\", directory, full_path)\n                shutil.copytree(directory, full_path)\n            else:\n                logger.debug(\"Not a directory: %s\", directory)\n        return path", "relevance": 2}
{"query": "copying a file to a path", "function": "def _copy_file(self, file_obj, destination, suffix, overwrite):\n        if overwrite:\n            # Not yet implemented as we have to find a portable (for different storage backends) way to overwrite files\n            raise NotImplementedError\n\n        # We are assuming here that we are operating on an already saved database objects with current database state available\n\n        filename = self._generate_new_filename(file_obj.file.name, suffix)\n\n        # Due to how inheritance works, we have to set both pk and id to None\n        file_obj.pk = None\n        file_obj.id = None\n        file_obj.save()\n        file_obj.folder = destination\n        file_obj._file_data_changed_hint = False  # no need to update size, sha1, etc.\n        file_obj.file = file_obj._copy_file(filename)\n        file_obj.original_filename = self._generate_new_filename(file_obj.original_filename, suffix)\n        file_obj.save()", "relevance": 1}
{"query": "create cookie", "function": "def setcookie(self, key, value, max_age=None, expires=None, path='/', domain=None, secure=None, httponly=False):\n        \"\"\"\n        Add a new cookie\n        \"\"\"\n        newcookie = Morsel()\n        newcookie.key = key\n        newcookie.value = value\n        newcookie.coded_value = value\n        if max_age is not None:\n            newcookie['max-age'] = max_age\n        if expires is not None:\n            newcookie['expires'] = expires\n        if path is not None:\n            newcookie['path'] = path\n        if domain is not None:\n            newcookie['domain'] = domain\n        if secure:\n            newcookie['secure'] = secure\n        if httponly:\n            newcookie['httponly'] = httponly\n        self.sent_cookies = [c for c in self.sent_cookies if c.key != key]\n        self.sent_cookies.append(newcookie)", "relevance": 4}
{"query": "create cookie", "function": "def __call__(self, req, res):\n        \"\"\"\n        Parses cookies of the header request (using the 'cookie' header key)\n        and adds a callback to the 'on_headerstrings' response event.\n        \"\"\"\n        # Do not clobber cookies\n        if hasattr(req, 'cookies'):\n            return\n\n        # Create an empty cookie state\n        req.cookies, res.cookies = SimpleCookie(), SimpleCookie()\n\n        log.info(\"{:d} built with {}\", id(self), json.dumps(self.opts))\n\n        # If the request had a cookie, load it!\n        req.cookies.load(req.headers.get('COOKIE', ''))\n\n        def _gen_cookie():\n            if res.cookies:\n                cookie_string = res.cookies.output(header='', sep=res.EOL)\n                return cookie_string\n\n        res.headers['Set-Cookie'] = _gen_cookie", "relevance": 3}
{"query": "create cookie", "function": "def _setcookie(self, session_id, expires='', **kw):\n        cookie_name = self._config.cookie_name\n        cookie_domain = self._config.cookie_domain\n        cookie_path = self._config.cookie_path\n        httponly = self._config.httponly\n        secure = self._config.secure\n        web.setcookie(cookie_name, session_id, expires=expires, domain=cookie_domain, httponly=httponly, secure=secure, path=cookie_path)", "relevance": 1}
{"query": "custom http error response", "function": "def _error_response(e):\n    res = make_response(e.urlencoded, e.status_code)\n    res.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n    return res", "relevance": 3}
{"query": "custom http error response", "function": "def _send_custom_response(self, reply):\n        code, custom_response = reply\n        if self._is_multiline_reply(custom_response):\n            self.multiline_reply(code, custom_response)\n        else:\n            self.reply(code, custom_response)", "relevance": 2}
{"query": "custom http error response", "function": "def _do_post(self, url, **kwargs):\n        \"\"\"\n        Convinient method for POST requests\n        Returns http request status value from a POST request\n        \"\"\"\n        #TODO:\n        # Add error handling. Check for HTTP status here would be much more conveinent than in each calling method\n        scaleioapi_post_headers = {'Content-type':'application/json','Version':'1.0'}\n        self.logger.debug(\"_do_post()\")\n\n        if kwargs:\n            for key, value in kwargs.iteritems():\n                if key == 'headers':\n                    scaleio_post_headers = value\n                    print \"Adding custom POST headers\"\n                if key == 'files':\n                    upl_files = value\n                    print \"Adding files to upload\"\n        try:\n            response = self._session.post(url, headers=scaleioapi_post_headers, verify_ssl=self._im_verify_ssl, files=upl_files)\n            self.logger.debug(\"_do_post() - Response: \" + \"{}\".format(response.text))\n            if response.status_code == requests.codes.ok:\n                return response\n            else:\n                self.logger.error(\"_do_post() - Response Code: \" + \"{}\".format(response.status_code))\n                raise RuntimeError(\"_do_post() - HTTP response error\" + response.status_code)\n        except:\n            raise RuntimeError(\"_do_post() - Communication error with ScaleIO gateway\")\n        return response", "relevance": 1}
{"query": "deducting the median from each column", "function": "def median(self, **kwargs):\n        \"\"\"Returns median of each column or row.\n\n        Returns:\n            A new QueryCompiler object containing the median of each column or row.\n        \"\"\"\n        if self._is_transposed:\n            kwargs[\"axis\"] = kwargs.get(\"axis\", 0) ^ 1\n            return self.transpose().median(**kwargs)\n        # Pandas default is 0 (though not mentioned in docs)\n        axis = kwargs.get(\"axis\", 0)\n        func = self._build_mapreduce_func(pandas.DataFrame.median, **kwargs)\n        return self._full_axis_reduce(axis, func)", "relevance": 3}
{"query": "deducting the median from each column", "function": "def median_abs_dev(values):\n    # Median Absolute Deviation\n    median = float(statistics.median(values))\n    return statistics.median([abs(median - sample) for sample in values])", "relevance": 2}
{"query": "deducting the median from each column", "function": "def adjust_to_level(self, level, x, op, median):\n        if x > median:\n            if level > 0.5:\n                result = median + (x - median) * ((level - 0.5) / 0.5)\n            else:\n                result = op + (median - op) * (level / 0.5)\n        else:\n            if level > 0.5:\n                result = x + (median - x) * ((level - 0.5) / 0.5)\n            else:\n                result = median + (op - median) * (level / 0.5)\n        return result", "relevance": 1}
{"query": "deserialize json", "function": "def deserialize(self, deserializable: PrimitiveJsonType) -> Optional[SerializableType]:\n        if not isinstance(self._decoder, ParsedJSONDecoder):\n            # Decode must take a string (even though we have a richer representation) :/\n            json_as_string = json.dumps(deserializable)\n            return self._decoder.decode(json_as_string)\n        else:\n            # Optimisation - no need to convert our relatively rich representation into a string (just to turn it back\n            # again!)\n            return self._decoder.decode_parsed(deserializable)", "relevance": 3}
{"query": "deserialize json", "function": "def _deserialize(kwargs):\n    def deserialize(item):\n        if isinstance(item[1], str):\n            try:\n                data = json.loads(item[1])  # load as json\n            except:\n                data = item[1].decode('utf-8')  # must be a string\n        else:\n            data = item[1]  # already deserialized (method default value)\n        return (item[0], data)\n    return dict(map(deserialize, kwargs.items()))", "relevance": 2}
{"query": "deserialize json", "function": "async def deserialize(data: dict):\n        \"\"\"\n        Builds a Proof object with defined attributes.\n        Attributes are provided by a previous call to the serialize function.\n        :param data:\n        Example:\n        name = \"proof name\"\n        requested_attrs = [{\"name\": \"age\", \"restrictions\": [{\"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\" } ] }, { \"name\":\"name\", \"restrictions\": [ { \"schema_id\": \"6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"Faber Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"6XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"8XFh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766\" }, { \"schema_id\": \"5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11\", \"schema_name\":\"BYU Student Info\", \"schema_version\":\"1.0\", \"schema_issuer_did\":\"5XFh8yBzrpJQmNyZzgoTqB\", \"issuer_did\":\"66Fh8yBzrpJQmNyZzgoTqB\", \"cred_def_id\": \"66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766\"}]}]\n        proof = await Proof.create(source_id, name, requested_attrs)\n        data = proof.serialize()\n        proof2 = await Proof.deserialize(data)\n        :return: Proof Object\n        \"\"\"\n        return await Proof._deserialize(\"vcx_proof_deserialize\",\n                                        json.dumps(data),\n                                        data.get('data').get('source_id'))", "relevance": 1}
{"query": "encode url", "function": "def _urlencode(self, h):\n        rv = []\n        for k,v in h.iteritems():\n            rv.append('%s=%s' %\n                (urllib.quote(k.encode(\"utf-8\")),\n                urllib.quote(v.encode(\"utf-8\"))))\n        return '&'.join(rv)", "relevance": 4}
{"query": "encode url", "function": "def encode(self):\n        \"\"\"\n        Encodes the current state of the object into a string.\n\n        :return: The encoded string\n        \"\"\"\n        opt_dict = {}\n        for k, v in self.options.items():\n            opt_dict[k] = v[0]\n\n        ss = '{0}://{1}'.format(self.scheme, ','.join(self.hosts))\n        if self.bucket:\n            ss += '/' + self.bucket\n\n        # URL encode options then decoded forward slash /\n        ss += '?' + urlencode(opt_dict).replace('%2F', '/')\n\n        return ss", "relevance": 3}
{"query": "encode url", "function": "def url_encoder(self):\n        encodedurl = []\n        encodedurl.append(aios.IntByte(\"Tx Power\",self.power))\n        asisurl=\"\"\n        myurl = urlparse(self.type_payload)\n        myhostname = myurl.hostname\n        mypath = myurl.path\n        if (myurl.scheme,myhostname.startswith(\"www.\")) in url_schemes:\n            encodedurl.append(aios.IntByte(\"URL Scheme\",\n                                           url_schemes.index((myurl.scheme,myhostname.startswith(\"www.\")))))\n            if myhostname.startswith(\"www.\"):\n                myhostname = myhostname[4:]\n        extval=None\n        if myhostname.split(\".\")[-1] in url_domain:\n            extval = url_domain.index(myhostname.split(\".\")[-1])\n            myhostname = \".\".join(myhostname.split(\".\")[:-1])\n        if extval is not None and not mypath.startswith(\"/\"):\n            extval+=7\n        else:\n            if myurl.port is None:\n                if extval is not None:\n                    mypath = mypath[1:]\n            else:\n                extval += 7\n        encodedurl.append(aios.String(\"URL string\"))\n        encodedurl[-1].val = myhostname\n        if extval is not None:\n            encodedurl.append(aios.IntByte(\"URL Extention\",extval))\n\n        if myurl.port:\n            asisurl += \":\"+str(myurl.port)+mypath\n        asisurl += mypath\n        if myurl.params:\n            asisurl += \";\"+myurl.params\n        if myurl.query:\n            asisurl += \"?\"+myurl.query\n        if myurl.fragment:\n            asisurl += \"#\"+myurl.fragment\n        encodedurl.append(aios.String(\"Rest of URL\"))\n        encodedurl[-1].val = asisurl\n        tlength=0\n        for x in encodedurl: #Check the payload length\n            tlength += len(x)\n        if tlength > 19: #Actually 18 but we have tx power\n            raise Exception(\"Encoded url too long (max 18 bytes)\")\n        self.service_data_length.val += tlength #Update the payload length\n        return encodedurl", "relevance": 2}
{"query": "encrypt aes ctr mode", "function": "def aes_encrypt(key, data, mode='ECB', iv=None):\n    aes = AES()\n    aes.mode = mode\n    aes.iv = iv\n    aes.key = key\n    return aes.encrypt(data)", "relevance": 3}
{"query": "encrypt aes ctr mode", "function": "def _encrypt(data):\n    \"\"\"Equivalent to OpenSSL using 256 bit AES in CBC mode\"\"\"\n    BS = AES.block_size\n\n    def pad(s):\n        n = BS - len(s) % BS\n        char = chr(n).encode('utf8')\n        return s + n * char\n\n    password = settings.GECKOBOARD_PASSWORD\n    salt = Random.new().read(BS - len('Salted__'))\n    key, iv = _derive_key_and_iv(password, salt, 32, BS)\n    cipher = AES.new(key, AES.MODE_CBC, iv)\n    encrypted = b'Salted__' + salt + cipher.encrypt(pad(data))\n    return base64.b64encode(encrypted)", "relevance": 2}
{"query": "encrypt aes ctr mode", "function": "def main():\n    runtest(\"AES - CTR Mode\", AES.CTREnc, AES.CTRDec)\n    runtest(\"AES - GCM Mode\", AES.GCMEnc, AES.GCMDec)", "relevance": 1}
{"query": "export to excel", "function": "def on_excel(self):\n        from pylon.io.excel import ExcelWriter\n        filename = asksaveasfilename(filetypes=[(\"Excel file\", \".xls\")])\n        if filename:\n            ExcelWriter(self.case).write(filename)", "relevance": 3}
{"query": "export to excel", "function": "def excel_to_sql(excel_file_path, engine,\n                 read_excel_kwargs=None,\n                 to_generic_type_kwargs=None,\n                 to_sql_kwargs=None):\n    \"\"\"Create a database from excel.\n\n    :param read_excel_kwargs: dict, arguments for ``pandas.read_excel`` method.\n      example: ``{\"employee\": {\"skiprows\": 10}, \"department\": {}}``\n    :param to_sql_kwargs: dict, arguments for ``pandas.DataFrame.to_sql`` \n      method.\n\n    limitation:\n\n    1. If a integer column has None value, data type in database will be float.\n      Because pandas thinks that it is ``np.nan``.\n    2. If a string column looks like integer, ``pandas.read_excel()`` method\n      doesn't have options to convert it to string.\n    \"\"\"\n    if read_excel_kwargs is None:\n        read_excel_kwargs = dict()\n\n    if to_sql_kwargs is None:\n        to_sql_kwargs = dict()\n\n    if to_generic_type_kwargs is None:\n        to_generic_type_kwargs = dict()\n\n    xl = pd.ExcelFile(excel_file_path)\n    for sheet_name in xl.sheet_names:\n        df = pd.read_excel(\n            excel_file_path, sheet_name,\n            **read_excel_kwargs.get(sheet_name, dict())\n        )\n\n        kwargs = to_generic_type_kwargs.get(sheet_name)\n        if kwargs:\n            data = to_dict_list_generic_type(df, **kwargs)\n            smart_insert(data, sheet_name, engine)\n        else:\n            df.to_sql(\n                sheet_name, engine, index=False,\n                **to_sql_kwargs.get(sheet_name, dict(if_exists=\"replace\"))\n            )", "relevance": 2}
{"query": "export to excel", "function": "def excel_to_sql(excel_file_path, engine,\n                 read_excel_kwargs=None,\n                 to_generic_type_kwargs=None,\n                 to_sql_kwargs=None):\n    \"\"\"Create a database from excel.\n\n    :param read_excel_kwargs: dict, arguments for ``pandas.read_excel`` method.\n      example: ``{\"employee\": {\"skiprows\": 10}, \"department\": {}}``\n    :param to_sql_kwargs: dict, arguments for ``pandas.DataFrame.to_sql`` \n      method.\n\n    limitation:\n\n    1. If a integer column has None value, data type in database will be float.\n      Because pandas thinks that it is ``np.nan``.\n    2. If a string column looks like integer, ``pandas.read_excel()`` method\n      doesn't have options to convert it to string.\n    \"\"\"\n    if read_excel_kwargs is None:\n        read_excel_kwargs = dict()\n\n    if to_sql_kwargs is None:\n        to_sql_kwargs = dict()\n\n    if to_generic_type_kwargs is None:\n        to_generic_type_kwargs = dict()\n\n    xl = pd.ExcelFile(excel_file_path)\n    for sheet_name in xl.sheet_names:\n        df = pd.read_excel(\n            excel_file_path, sheet_name,\n            **read_excel_kwargs.get(sheet_name, dict())\n        )\n\n        kwargs = to_generic_type_kwargs.get(sheet_name)\n        if kwargs:\n            data = to_dict_list_generic_type(df, **kwargs)\n            smart_insert(data, sheet_name, engine)\n        else:\n            df.to_sql(\n                sheet_name, engine, index=False,\n                **to_sql_kwargs.get(sheet_name, dict(if_exists=\"replace\"))\n            )", "relevance": 1}
{"query": "extract data from html content", "function": "def parse_item(self, response):\n        key = None\n        if re.search(r'parameterontologies', response.url) and \\\n            response.css('html body div#SiteWrapper div#Content h2 span.procedurekey'\n                         '.dark::text').extract() is not None and \\\n            len(response.css('html body div#SiteWrapper div#Content h2 span.procedurekey'\n                             '.dark::text').extract()) > 0:\n            key = response.css('html body div#SiteWrapper div#Content h2 span.procedurekey'\n                               '.dark::text').extract()[0]\n        elif (re.search(r'parameters', response.url) or re.search(r'protocol', response.url)) and \\\n            response.css('html body div#SiteWrapper div#Content h2 span.procedurekey'\n                         '.dark::text').extract() is not None and \\\n            len(response.css('html body div#SiteWrapper div#Content h2 span.procedurekey'\n                             '.dark::text').extract()) > 0:\n            key = response.css('html body div#SiteWrapper div#Content h2 span.procedurekey'\n                               '.dark::text').extract()[0]\n        elif re.search(r'procedures', response.url) and \\\n            response.css('html body div#SiteWrapper div#Content h2 span.pipelinekey'\n                         '::text').extract() is not None and \\\n            len(response.css('html body div#SiteWrapper div#Content h2 span.pipelinekey'\n                             '::text').extract()) > 0:\n            key = response.css('html body div#SiteWrapper div#Content h2 span.pipelinekey'\n                               '::text').extract()[0]\n        if key is not None:\n            yield {\n                key: response.url\n            }", "relevance": 3}
{"query": "extract data from html content", "function": "def extract_content(html, encoding=None, as_blocks=False):\n    if 'content' not in _LOADED_MODELS:\n        _LOADED_MODELS['content'] = load_pickled_model(\n            'kohlschuetter_readability_weninger_content_model.pkl.gz')\n    return _LOADED_MODELS['content'].extract(html, encoding=encoding, as_blocks=as_blocks)", "relevance": 2}
{"query": "extract data from html content", "function": "def parse_content(self, content):\n        self.data = []\n        self.environment = {}\n        self.invalid_lines = []\n        # Crontabs can use 'nicknames' for common event frequencies:\n        nicknames = {\n            '@yearly': '0 0 1 1 *',\n            '@annually': '0 0 1 1 *',\n            '@monthly': '0 0 1 * *',\n            '@weekly': '0 0 * * 0',\n            '@daily': '0 0 * * *',\n            '@hourly': '0 * * * *',\n        }\n        cron_re = re.compile(_make_cron_re(), flags=re.IGNORECASE)\n        env_re = re.compile(r'^\\s*(?P<key>\\w+)\\s*=\\s*(?P<value>\\S.*)$')\n        for line in get_active_lines(content):\n            if line.startswith('@'):\n                # Reboot is 'special':\n                if line.startswith('@reboot'):\n                    parts = line.split(None, 2)\n                    self.data.append({'time': '@reboot', 'command': parts[1]})\n                    continue\n                else:\n                    parts = line.split(None, 2)\n                    if parts[0] not in nicknames:\n                        raise ParseException(\n                            \"{n} not recognised as a time specification 'nickname'\".format(n=parts[0])\n                        )\n                    # Otherwise, put the time spec nickname translation into\n                    # the line\n                    line = line.replace(parts[0], nicknames[parts[0]])\n                    # And then we fall through to the rest of the parsing.\n            cron_match = cron_re.match(line)\n            env_match = env_re.match(line)\n            if cron_match:\n                self.data.append(cron_match.groupdict())\n            elif env_match:\n                # Environment variable - capture in dictionary\n                self.environment[env_match.group('key')] = env_match.group('value')\n            else:\n                self.invalid_lines.append(line)", "relevance": 1}
{"query": "extract latitude and longitude from given input", "function": "def decode_longitude(self, longitude):\n        match = RE_LONGITUDE.match(longitude)\n        if not match:\n            raise ParserError('Reading longitude failed')\n\n        longitude = int(match.group(1)) + float(match.group(2)) / 60.\n\n        if not (0 <= longitude <= 180):\n            raise ParserError('Longitude out of bounds')\n\n        if match.group(3).upper() == 'W':\n            longitude = -longitude\n\n        return longitude", "relevance": 3}
{"query": "extract latitude and longitude from given input", "function": "def lat_lon_grid_deltas(longitude, latitude, **kwargs):\n    r\"\"\"Calculate the delta between grid points that are in a latitude/longitude format.\n\n    Calculate the signed delta distance between grid points when the grid spacing is defined by\n    delta lat/lon rather than delta x/y\n\n    Parameters\n    ----------\n    longitude : array_like\n        array of longitudes defining the grid\n    latitude : array_like\n        array of latitudes defining the grid\n    kwargs\n        Other keyword arguments to pass to :class:`~pyproj.Geod`\n\n    Returns\n    -------\n    dx, dy:\n        at least two dimensional arrays of signed deltas between grid points in the x and y\n        direction\n\n    Notes\n    -----\n    Accepts 1D, 2D, or higher arrays for latitude and longitude\n    Assumes [..., Y, X] for >=2 dimensional arrays\n\n    \"\"\"\n    from pyproj import Geod\n\n    # Inputs must be the same number of dimensions\n    if latitude.ndim != longitude.ndim:\n        raise ValueError('Latitude and longitude must have the same number of dimensions.')\n\n    # If we were given 1D arrays, make a mesh grid\n    if latitude.ndim < 2:\n        longitude, latitude = np.meshgrid(longitude, latitude)\n\n    geod_args = {'ellps': 'sphere'}\n    if kwargs:\n        geod_args = kwargs\n\n    g = Geod(**geod_args)\n\n    forward_az, _, dy = g.inv(longitude[..., :-1, :], latitude[..., :-1, :],\n                              longitude[..., 1:, :], latitude[..., 1:, :])\n    dy[(forward_az < -90.) | (forward_az > 90.)] *= -1\n\n    forward_az, _, dx = g.inv(longitude[..., :, :-1], latitude[..., :, :-1],\n                              longitude[..., :, 1:], latitude[..., :, 1:])\n    dx[(forward_az < 0.) | (forward_az > 180.)] *= -1\n\n    return dx * units.meter, dy * units.meter", "relevance": 2}
{"query": "extract latitude and longitude from given input", "function": "def lat_lon_grid_deltas(longitude, latitude, **kwargs):\n    r\"\"\"Calculate the delta between grid points that are in a latitude/longitude format.\n\n    Calculate the signed delta distance between grid points when the grid spacing is defined by\n    delta lat/lon rather than delta x/y\n\n    Parameters\n    ----------\n    longitude : array_like\n        array of longitudes defining the grid\n    latitude : array_like\n        array of latitudes defining the grid\n    kwargs\n        Other keyword arguments to pass to :class:`~pyproj.Geod`\n\n    Returns\n    -------\n    dx, dy:\n        at least two dimensional arrays of signed deltas between grid points in the x and y\n        direction\n\n    Notes\n    -----\n    Accepts 1D, 2D, or higher arrays for latitude and longitude\n    Assumes [..., Y, X] for >=2 dimensional arrays\n\n    \"\"\"\n    from pyproj import Geod\n\n    # Inputs must be the same number of dimensions\n    if latitude.ndim != longitude.ndim:\n        raise ValueError('Latitude and longitude must have the same number of dimensions.')\n\n    # If we were given 1D arrays, make a mesh grid\n    if latitude.ndim < 2:\n        longitude, latitude = np.meshgrid(longitude, latitude)\n\n    geod_args = {'ellps': 'sphere'}\n    if kwargs:\n        geod_args = kwargs\n\n    g = Geod(**geod_args)\n\n    forward_az, _, dy = g.inv(longitude[..., :-1, :], latitude[..., :-1, :],\n                              longitude[..., 1:, :], latitude[..., 1:, :])\n    dy[(forward_az < -90.) | (forward_az > 90.)] *= -1\n\n    forward_az, _, dx = g.inv(longitude[..., :, :-1], latitude[..., :, :-1],\n                              longitude[..., :, 1:], latitude[..., :, 1:])\n    dx[(forward_az < 0.) | (forward_az > 180.)] *= -1\n\n    return dx * units.meter, dy * units.meter", "relevance": 1}
{"query": "extracting data from a text file", "function": "def extractdata(pattern, text=None, filepath=None):\n    \"\"\"\n    Read through an entire file or body of text one line at a time. Parse each line that matches the supplied\n    pattern string and ignore the rest.\n\n    If *text* is supplied, it will be parsed according to the *pattern* string.\n    If *text* is not supplied, the file at *filepath* will be opened and parsed.\n    \"\"\"\n    y = []\n    if text is None:\n        textsource = open(filepath, 'r')\n    else:\n        textsource = text.splitlines()\n\n    for line in textsource:\n        match = scanf(pattern, line)\n        if match:\n            if len(y) == 0:\n                y = [[s] for s in match]\n            else:\n                for i, ydata in enumerate(y):\n                    ydata.append(match[i])\n\n    if text is None:\n        textsource.close()\n\n    return y", "relevance": 3}
{"query": "extracting data from a text file", "function": "def loadText(fname):\n    with codecs.open(fname, 'r', 'utf-8') as f:\n        data = f.read()\n        return normalize(data).split()", "relevance": 2}
{"query": "extracting data from a text file", "function": "def extract_context_data(self):\n        \"\"\"\n        Returns the contents of a AWS Lambda context.\n\n        :returns: A dict of relevant context data.\n        :rtype: dict\n        \"\"\"\n        data = {}\n        for k, v in {\n            # camel case names in the report to align with AWS standards\n            \"functionName\": \"function_name\",\n            \"functionVersion\": \"function_version\",\n            \"memoryLimitInMB\": \"memory_limit_in_mb\",\n            \"invokedFunctionArn\": \"invoked_function_arn\",\n            \"awsRequestId\": \"aws_request_id\",\n            \"logGroupName\": \"log_group_name\",\n            \"logStreamName\": \"log_stream_name\",\n        }.items():\n            if hasattr(self.context, v):\n                data[k] = getattr(self.context, v)\n        if (\n            hasattr(self.context, \"invoked_function_arn\")\n            and \"AWS_SAM_LOCAL\" in os.environ\n        ):\n            data[\"invokedFunctionArn\"] = (\n                \"arn:aws:lambda:local:0:function:%s\"\n                % data.get(\"functionName\", \"unknown\")\n            )\n        if hasattr(self.context, \"get_remaining_time_in_millis\") and callable(\n            self.context.get_remaining_time_in_millis\n        ):\n            data[\n                \"getRemainingTimeInMillis\"\n            ] = self.context.get_remaining_time_in_millis()\n        data[\"traceId\"] = os.getenv(\"_X_AMZN_TRACE_ID\", \"\")\n        return data", "relevance": 1}
{"query": "filter array", "function": "def filter(this, args):\n        array = to_object(this, args.space)\n        callbackfn = get_arg(args, 0)\n        arr_len = js_arr_length(array)\n        if not is_callable(callbackfn):\n            raise MakeError('TypeError', 'callbackfn must be a function')\n        _this = get_arg(args, 1)\n        k = 0\n        res = []\n        while k < arr_len:\n            if array.has_property(unicode(k)):\n                kValue = array.get(unicode(k))\n                if to_boolean(\n                        callbackfn.call(_this, (kValue, float(k), array))):\n                    res.append(kValue)\n            k += 1\n        return args.space.ConstructArray(res)", "relevance": 3}
{"query": "filter array", "function": "def apply_filter(x, filter=None, axis=0):\n    \"\"\"Apply a filter to an array.\"\"\"\n    x = _as_array(x)\n    if x.shape[axis] == 0:\n        return x\n    b, a = filter\n    return signal.filtfilt(b, a, x, axis=axis)", "relevance": 2}
{"query": "filter array", "function": "def _build_filtered_query(self, f, operator):\n        \"\"\"\n        Create the root of the filter tree\n        \"\"\"\n        self._filtered = True\n        if isinstance(f, Filter):\n            filter_object = f\n        else:\n            filter_object = Filter(operator).filter(f)\n        self._filter_dsl = filter_object", "relevance": 1}
{"query": "find int in string", "function": "def find_number(regex, s):\n    \"\"\"Find a number using a given regular expression.\n    If the string cannot be found, returns None.\n    The regex should contain one matching group, \n    as only the result of the first group is returned.\n    The group should only contain numeric characters ([0-9]+).\n    \n    s - The string to search.\n    regex - A string containing the regular expression.\n    \n    Returns an integer or None.\n    \"\"\"\n    result = find_string(regex, s)\n    if result is None:\n        return None\n    return int(result)", "relevance": 3}
{"query": "find int in string", "function": "def find(self, txt):\n        result = []\n        for e in self.table:\n            print('find(self, txt) e = ', e)\n            if txt in str(e):\n                result.append(e)\n                #print(e)\n        return result", "relevance": 2}
{"query": "find int in string", "function": "def _break(s, find):\n    \"\"\"Break a string s into the part before the substring to find,\n    and the part including and after the substring.\"\"\"\n    i = s.find(find)\n    return s[:i], s[i:]", "relevance": 1}
{"query": "finding time elapsed using a timer", "function": "def timed(func):\n    @functools.wraps(func)\n    def timer(*args, **kwargs):\n        start_time = time()\n        result = func(*args, **kwargs)\n        elapsed_time = str(timedelta(seconds=int(time() - start_time)))\n        print('Elapsed time is {}'.format(elapsed_time))\n        return result\n    return timer", "relevance": 3}
{"query": "finding time elapsed using a timer", "function": "def __init__(self, timer, elapsed):\n        self.timer = timer\n        self.elapsed = elapsed", "relevance": 2}
{"query": "finding time elapsed using a timer", "function": "def update(self):\n        # on every timer tick, record a gyro sample and exit if required time has elapsed\n        elapsed = int(100 * ((time.time() - self.started_at) / self.GYRO_BIAS_TIME))\n        self.progressBar.setValue(elapsed)\n        self.samples.append(self.imu.gyro)\n\n        if time.time() - self.started_at > self.GYRO_BIAS_TIME:\n            self.accept()\n            QtWidgets.QMessageBox.information(self, 'Gyro calibration', 'Calibration finished')", "relevance": 1}
{"query": "format date", "function": "def std_tsymbol(tsymbol):\n    s, date = tsymbol\n    if date == 0:\n        return '_{}_'.format(s)\n    elif date <= 0:\n        return '_{}_m{}_'.format(s, str(-date))\n    elif date >= 0:\n        return '_{}__{}_'.format(s, str(date))", "relevance": 3}
{"query": "format date", "function": "def format_date(date, gmt_offset=0, relative=True, shorter=False, full_format=False):\n    \"\"\"Formats the given date (which should be GMT).\n\n    By default, we return a relative time (e.g., \"2 minutes ago\"). You\n    can return an absolute date string with ``relative=False``.\n\n    You can force a full format date (\"July 10, 1980\") with\n    ``full_format=True``.\n\n    This method is primarily intended for dates in the past.\n    For dates in the future, we fall back to full format.\n\n    From tornado\n    \"\"\"\n\n    if not date:\n        return '-'\n    if isinstance(date, float) or isinstance(date, int):\n        date = datetime.datetime.utcfromtimestamp(date)\n    now = datetime.datetime.utcnow()\n    if date > now:\n        if relative and (date - now).seconds < 60:\n            # Due to click skew, things are some things slightly\n            # in the future. Round timestamps in the immediate\n            # future down to now in relative mode.\n            date = now\n        else:\n            # Otherwise, future dates always use the full format.\n            full_format = True\n    local_date = date - datetime.timedelta(minutes=gmt_offset)\n    local_now = now - datetime.timedelta(minutes=gmt_offset)\n    local_yesterday = local_now - datetime.timedelta(hours=24)\n    difference = now - date\n    seconds = difference.seconds\n    days = difference.days\n\n    format = None\n    if not full_format:\n        ret_, fff_format = fix_full_format(days, seconds, relative, shorter, local_date, local_yesterday)\n        format = fff_format\n        if ret_:\n            return format\n        else:\n            format = format\n\n    if format is None:\n        format = \"%(month_name)s %(day)s, %(year)s\" if shorter else \\\n            \"%(month_name)s %(day)s, %(year)s at %(time)s\"\n\n    str_time = \"%d:%02d\" % (local_date.hour, local_date.minute)\n\n    return format % {\n        \"month_name\": local_date.strftime('%b'),\n        \"weekday\": local_date.strftime('%A'),\n        \"day\": str(local_date.day),\n        \"year\": str(local_date.year),\n        \"month\": local_date.month,\n        \"time\": str_time\n    }", "relevance": 2}
{"query": "format date", "function": "def datetime_from_iso8601(date):\n    \"\"\"Small helper that parses ISO-8601 date dates.\n\n        >>> datetime_from_iso8601(\"2013-04-10T12:52:39\")\n        datetime.datetime(2013, 4, 10, 12, 52, 39)\n        >>> datetime_from_iso8601(\"2013-01-07T12:55:19.257\")\n        datetime.datetime(2013, 1, 7, 12, 55, 19, 257000)\n    \"\"\"\n    format = ISO8610_FORMAT\n    if date.endswith(\"Z\"):\n        date = date[:-1]  # Date date is UTC\n    if re.match(\".*\\.\\d+\", date):\n        # Date includes microseconds\n        format = ISO8610_FORMAT_MICROSECONDS\n    return datetime.datetime.strptime(date, format)", "relevance": 1}
{"query": "fuzzy match ranking", "function": "def fuzzy_match(name, strings):\n    global fuzzy_match_fun\n    if fuzzy_match_fun is not None:\n        return fuzzy_match_fun(name, strings)\n\n    try:\n        from fuzzywuzzy import process, fuzz\n        fuzzy_match_fun = lambda name, strings: process.extractOne(name, strings, scorer=fuzz.partial_ratio)[0]\n    except ImportError: # pragma: no cover\n        import difflib\n        fuzzy_match_fun = lambda name, strings: difflib.get_close_matches(name, strings, n=1, cutoff=0)[0]\n    return fuzzy_match_fun(name, strings)", "relevance": 3}
{"query": "fuzzy match ranking", "function": "def rank(raw_match_info, *raw_weights):\n    # Handle match_info called w/default args 'pcx' - based on the example rank\n    # function http://sqlite.org/fts3.html#appendix_a\n    match_info = _parse_match_info(raw_match_info)\n    score = 0.0\n\n    p, c = match_info[:2]\n    weights = get_weights(c, raw_weights)\n\n    # matchinfo X value corresponds to, for each phrase in the search query, a\n    # list of 3 values for each column in the search table.\n    # So if we have a two-phrase search query and three columns of data, the\n    # following would be the layout:\n    # p0 : c0=[0, 1, 2],   c1=[3, 4, 5],    c2=[6, 7, 8]\n    # p1 : c0=[9, 10, 11], c1=[12, 13, 14], c2=[15, 16, 17]\n    for phrase_num in range(p):\n        phrase_info_idx = 2 + (phrase_num * c * 3)\n        for col_num in range(c):\n            weight = weights[col_num]\n            if not weight:\n                continue\n\n            col_idx = phrase_info_idx + (col_num * 3)\n\n            # The idea is that we count the number of times the phrase appears\n            # in this column of the current row, compared to how many times it\n            # appears in this column across all rows. The ratio of these values\n            # provides a rough way to score based on \"high value\" terms.\n            row_hits = match_info[col_idx]\n            all_rows_hits = match_info[col_idx + 1]\n            if row_hits > 0:\n                score += weight * (float(row_hits) / all_rows_hits)\n\n    return -score", "relevance": 2}
{"query": "fuzzy match ranking", "function": "def fuzzmatch(self, fuzzkey, multi=False):\n        \"\"\"\n        Identify a filter by fuzzy string matching.\n\n        Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio`\n\n        Parameters\n        ----------\n        fuzzkey : str\n            A string that partially matches one filter name more than the others.\n\n        Returns\n        -------\n        The name of the most closely matched filter. : str\n        \"\"\"\n\n        keys, ratios = np.array([(f, seqm(None, fuzzkey, f).ratio()) for f in self.components.keys()]).T\n        mratio = max(ratios)\n\n        if multi:\n            return keys[ratios == mratio]\n        else:\n            if sum(ratios == mratio) == 1:\n                return keys[ratios == mratio][0]\n            else:\n                raise ValueError(\"\\nThe filter key provided ('{:}') matches two or more filter names equally well:\\n\".format(fuzzkey) + ', '.join(keys[ratios == mratio]) + \"\\nPlease be more specific!\")", "relevance": 1}
{"query": "get all parents of xml node", "function": "def _import_parents_from_xml(self, xml):\n        parents = xml.iterfind('parent')\n        for p in parents:\n            for o in p:\n                # Store a tuple of orgid, identifier\n                self._parents.append( o.attrib['frameid'] ) #( o.attrib['orgid'],  ) )", "relevance": 3}
{"query": "get all parents of xml node", "function": "def dominator_tree(graph):\n    \"\"\"\n    RETURN DOMINATOR FOREST\n    THERE ARE TWO TREES, \"ROOTS\" and \"LOOPS\"\n    ROOTS HAVE NO PARENTS\n    LOOPS ARE NODES THAT ARE A MEMBER OF A CYCLE THAT HAS NO EXTRNAL PARENT\n\n    roots = dominator_tree(graph).get_children(ROOTS)\n    \"\"\"\n    todo = Queue()\n    done = set()\n    dominator = Tree(None)\n    nodes = list(graph.nodes)\n\n    while True:\n        # FIGURE OUT NET ITEM TO WORK ON\n        if todo:\n            node = todo.pop()\n        elif nodes:\n            node = nodes.pop()\n            if len(nodes) % 1000 == 0:\n                Log.note(\"{{num}} nodes remaining\", num=len(nodes))\n        else:\n            break\n        if node in done:\n            continue\n\n        parents = graph.get_parents(node) - {node}\n        if not parents:\n            # node WITHOUT parents IS A ROOT\n            done.add(node)\n            dominator.add_edge(Edge(ROOTS, node))\n            continue\n\n        not_done = parents - done\n        if not_done:\n            # THERE ARE MORE parents TO DO FIRST\n            more_todo = not_done - todo\n            if not more_todo:\n                # ALL PARENTS ARE PART OF A CYCLE, MAKE node A ROOT\n                done.add(node)\n                dominator.add_edge(Edge(LOOPS, node))\n            else:\n                # DO THE PARENTS BEFORE node\n                todo.push(node)\n                for p in more_todo:\n                    todo.push(p)\n            continue\n\n        # WE CAN GET THE DOMINATORS FOR ALL parents\n        if len(parents) == 1:\n            # SHORTCUT\n            dominator.add_edge(Edge(list(parents)[0], node))\n            done.add(node)\n            continue\n\n        paths_from_roots = [\n            list(reversed(dominator.get_path_to_root(p)))\n            for p in parents\n        ]\n\n        if any(p[0] is ROOTS for p in paths_from_roots):\n            # THIS OBJECT CAN BE REACHED FROM A ROOT, IGNORE PATHS FROM LOOPS\n            paths_from_roots = [p for p in paths_from_roots if p[0] is ROOTS]\n            if len(paths_from_roots) == 1:\n                # SHORTCUT\n                dom = paths_from_roots[0][-1]\n                dominator.add_edge(Edge(dom, node))\n                done.add(node)\n                continue\n\n        # FIND COMMON PATH FROM root\n        num_paths = len(paths_from_roots)\n        for i, x in enumerate(zip_longest(*paths_from_roots)):\n            if x.count(x[0]) != num_paths:\n                dom = paths_from_roots[0][i-1]\n                if dom is LOOPS:\n                    # CAN BE REACHED FROM MORE THAN ONE LOOP, PICK ONE TO BLAME\n                    dom = paths_from_roots[0][-1]\n                break\n        else:\n            # ALL PATHS IDENTICAL\n            dom = paths_from_roots[0][-1]\n\n        dominator.add_edge(Edge(dom, node))\n        done.add(node)\n\n    return dominator", "relevance": 2}
{"query": "get all parents of xml node", "function": "def get_ancestor_ephemeral_nodes(self, selected_nodes):\n        node_names = {}\n        for node_id in selected_nodes:\n            if node_id not in self.manifest.nodes:\n                continue\n            node = self.manifest.nodes[node_id]\n            # sources don't have ancestors and this results in a silly select()\n            if node.resource_type == NodeType.Source:\n                continue\n            node_names[node_id] = node.name\n\n        include_spec = [\n            '+{}'.format(node_names[node])\n            for node in selected_nodes if node in node_names\n        ]\n        if not include_spec:\n            return set()\n\n        all_ancestors = self.select_nodes(self.linker.graph, include_spec, [])\n\n        res = []\n        for ancestor in all_ancestors:\n            ancestor_node = self.manifest.nodes.get(ancestor, None)\n\n            if ancestor_node and self.is_ephemeral_model(ancestor_node):\n                res.append(ancestor)\n\n        return set(res)", "relevance": 1}
{"query": "get current ip address", "function": "def get_ip(self):\n        if self._ip is None:\n            self._ip = self.fetch_ip()\n        return self._ip", "relevance": 3}
{"query": "get current ip address", "function": "async def get_ip(self) -> Union[IPv4Address, IPv6Address]:\n        \"\"\"\n        get ip address of client\n        :return:\n        \"\"\"\n        xff = await self.get_x_forwarded_for()\n        if xff: return xff[0]\n        ip_addr = self._request.transport.get_extra_info('peername')[0]\n        return ip_address(ip_addr)", "relevance": 2}
{"query": "get current ip address", "function": "def ip(ip_address, return_format=None):\n    \"\"\"Returns a summary of the information our database holds for a\n    particular IP address (similar to /ipinfo.html).\n\n    In the returned data:\n\n    Count: (also reports or records) total number of packets blocked from\n    this IP.\n    Attacks: (also targets) number of unique destination IP addresses for\n    these packets.\n\n    :param ip_address: a valid IP address\n    \"\"\"\n    response = _get('ip/{address}'.format(address=ip_address), return_format)\n    if 'bad IP address' in str(response):\n        raise Error('Bad IP address, {address}'.format(address=ip_address))\n    else:\n        return response", "relevance": 1}
{"query": "get current observable value", "function": "def make_dict_observable(matrix_observable):\n    \"\"\"Convert an observable in matrix form to dictionary form.\n\n    Takes in a diagonal observable as a matrix and converts it to a dictionary\n    form. Can also handle a list sorted of the diagonal elements.\n\n    Args:\n        matrix_observable (list): The observable to be converted to dictionary\n        form. Can be a matrix or just an ordered list of observed values\n\n    Returns:\n        Dict: A dictionary with all observable states as keys, and corresponding\n        values being the observed value for that state\n    \"\"\"\n    dict_observable = {}\n    observable = np.array(matrix_observable)\n    observable_size = len(observable)\n    observable_bits = int(np.ceil(np.log2(observable_size)))\n    binary_formater = '0{}b'.format(observable_bits)\n    if observable.ndim == 2:\n        observable = observable.diagonal()\n    for state_no in range(observable_size):\n        state_str = format(state_no, binary_formater)\n        dict_observable[state_str] = observable[state_no]\n    return dict_observable", "relevance": 3}
{"query": "get current observable value", "function": "def add(self, observableElement):\n        \"\"\"\n        add an observable element\n\n        :param str observableElement: the name of the observable element\n        :raises RuntimeError: if element name already exist in the store\n        \"\"\"\n        if observableElement not in self._observables:\n            self._observables.append(observableElement)\n        else:\n            raise RuntimeError(\n                \"{0} is already an observable element\"\n                .format(observableElement))", "relevance": 2}
{"query": "get current observable value", "function": "def make_dict_observable(matrix_observable):\n    \"\"\"Convert an observable in matrix form to dictionary form.\n\n    Takes in a diagonal observable as a matrix and converts it to a dictionary\n    form. Can also handle a list sorted of the diagonal elements.\n\n    Args:\n        matrix_observable (list): The observable to be converted to dictionary\n        form. Can be a matrix or just an ordered list of observed values\n\n    Returns:\n        Dict: A dictionary with all observable states as keys, and corresponding\n        values being the observed value for that state\n    \"\"\"\n    dict_observable = {}\n    observable = np.array(matrix_observable)\n    observable_size = len(observable)\n    observable_bits = int(np.ceil(np.log2(observable_size)))\n    binary_formater = '0{}b'.format(observable_bits)\n    if observable.ndim == 2:\n        observable = observable.diagonal()\n    for state_no in range(observable_size):\n        state_str = format(state_no, binary_formater)\n        dict_observable[state_str] = observable[state_no]\n    return dict_observable", "relevance": 1}
{"query": "get current process id", "function": "def get_pid(PROCNAME):\n    for proc in psutil.process_iter():\n        if proc.name == PROCNAME:\n            return proc.pid", "relevance": 3}
{"query": "get current process id", "function": "def _create_variable(orig_v, step, variables):\n    \"\"\"Create a new output variable, potentially over-writing existing or creating new.\n    \"\"\"\n    # get current variable, and convert to be the output of our process step\n    try:\n        v = _get_variable(orig_v[\"id\"], variables)\n    except ValueError:\n        v = copy.deepcopy(orig_v)\n        if not isinstance(v[\"id\"], six.string_types):\n            v[\"id\"] = _get_string_vid(v[\"id\"])\n    for key, val in orig_v.items():\n        if key not in [\"id\", \"type\"]:\n            v[key] = val\n    if orig_v.get(\"type\") != \"null\":\n        v[\"type\"] = orig_v[\"type\"]\n    v[\"id\"] = \"%s/%s\" % (step.name, get_base_id(v[\"id\"]))\n    return v", "relevance": 2}
{"query": "get current process id", "function": "def get_process_id_from_prefix(self):\n        if self.cmdprefix:\n            pid = self.input_process(self.cmdprefix)\n        else:\n            if self.lastEvent is None:\n                raise CmdError(\"no current process set\")\n            pid = self.lastEvent.get_pid()\n        return pid", "relevance": 1}
{"query": "get executable path", "function": "def which(name):\n    for p in os.environ['PATH'].split(os.pathsep):\n        exe = os.path.join(p, name)\n        if is_executable(exe):\n            return os.path.abspath(exe)\n        for ext in [''] + os.environ.get('PATHEXT', '').split(os.pathsep):\n            exe = '{}{}'.format(exe, ext.lower())\n            if is_executable(exe):\n                return os.path.abspath(exe)", "relevance": 4}
{"query": "get executable path", "function": "def which(name):\n    for p in os.environ['PATH'].split(os.pathsep):\n        exe = os.path.join(p, name)\n        if is_executable(exe):\n            return os.path.abspath(exe)\n        for ext in [''] + os.environ.get('PATHEXT', '').split(os.pathsep):\n            exe = '{}{}'.format(exe, ext.lower())\n            if is_executable(exe):\n                return os.path.abspath(exe)", "relevance": 3}
{"query": "get executable path", "function": "def which(cmd, mode=os.F_OK | os.X_OK, path=None):\n    \"\"\"\n    Given cmd, check where it is on PATH.\n\n    Loosely based on the version in python 3.3.\n    \"\"\"\n\n    if os.path.dirname(cmd):\n        if os.path.isfile(cmd) and os.access(cmd, mode):\n            return cmd\n\n    if path is None:\n        path = os.environ.get('PATH', defpath)\n    if not path:\n        return None\n\n    paths = path.split(pathsep)\n\n    if sys.platform == 'win32':\n        # oh boy\n        if curdir not in paths:\n            paths = [curdir] + paths\n\n        # also need to check the fileexts...\n        pathext = os.environ.get('PATHEXT', '').split(pathsep)\n\n        if any(cmd.lower().endswith(ext.lower()) for ext in pathext):\n            files = [cmd]\n        else:\n            files = [cmd + ext for ext in pathext]\n    else:\n        # sanity\n        files = [cmd]\n\n    seen = set()\n    for p in paths:\n        normpath = normcase(p)\n        if normpath in seen:\n            continue\n        seen.add(normpath)\n        for f in files:\n            fn = os.path.join(p, f)\n            if os.path.isfile(fn) and os.access(fn, mode):\n                return fn\n\n    return None", "relevance": 2}
{"query": "get inner html", "function": "def __set_html(self, html=None):\n        \"\"\"\n        Sets the html content in the View using given body.\n\n        :param html: Html content.\n        :type html: unicode\n        \"\"\"\n\n        self.__html = self.__get_html(html)\n        self.__view.setHtml(self.__html)", "relevance": 3}
{"query": "get inner html", "function": "def html(self) -> str:\n        \"\"\"Get whole html representation of this node.\"\"\"\n        if self._inner_element:\n            return self.start_tag + self._inner_element.html + self.end_tag\n        return super().html", "relevance": 2}
{"query": "get inner html", "function": "def get_png_img_html(blob: Union[bytes, memoryview],\n                     extra_html_class: str = None) -> str:\n    \"\"\"\n    Converts a PNG blob to an HTML IMG tag with embedded data.\n    \"\"\"\n    return \"\"\"<img {}src=\"{}\" />\"\"\".format(\n        'class=\"{}\" '.format(extra_html_class) if extra_html_class else \"\",\n        get_png_data_url(blob)\n    )", "relevance": 1}
{"query": "get name of enumerated value", "function": "def _parse_enumerated_value(self, enumerated_value_node):\n        return SVDEnumeratedValue(\n            name=_get_text(enumerated_value_node, 'name'),\n            description=_get_text(enumerated_value_node, 'description'),\n            value=_get_int(enumerated_value_node, 'value'),\n            is_default=_get_int(enumerated_value_node, 'isDefault')\n        )", "relevance": 3}
{"query": "get name of enumerated value", "function": "def _parse_enumerated_value(self, enumerated_value_node):\n        return SVDEnumeratedValue(\n            name=_get_text(enumerated_value_node, 'name'),\n            description=_get_text(enumerated_value_node, 'description'),\n            value=_get_int(enumerated_value_node, 'value'),\n            is_default=_get_int(enumerated_value_node, 'isDefault')\n        )", "relevance": 2}
{"query": "get name of enumerated value", "function": "def sanitize_enum(self, enum):\n        for name, enumeratorlist in enum.children():\n            for name, enumerator in enumeratorlist.children():\n                enumerator.value = c_ast.Constant('dummy', '...')\n        return enum", "relevance": 1}
{"query": "get the description of a http status code", "function": "def _catch_errors(self, json_response):\n        status = json_response.get('status')\n        if status and status.get('code') != 200:\n            self.status_code = status.get('code')\n            self.error = status.get('message')\n\n        return self.error", "relevance": 3}
{"query": "get the description of a http status code", "function": "def _get_status_code(self, http_status):\n    \"\"\"Get the HTTP status code from an HTTP status string.\n\n    Args:\n      http_status: A string containing a HTTP status code and reason.\n\n    Returns:\n      An integer with the status code number from http_status.\n    \"\"\"\n    try:\n      return int(http_status.split(' ', 1)[0])\n    except TypeError:\n      _logger.warning('Unable to find status code in HTTP status %r.',\n                      http_status)\n    return 500", "relevance": 2}
{"query": "get the description of a http status code", "function": "def error_code_to_str(code):\n    \"\"\"\n    Converts a given error code (errno) to a useful and human readable string.\n\n    :param int code: a possibly invalid/unknown error code\n    :rtype: str\n    :returns: a string explaining and containing the given error code, or a string\n              explaining that the errorcode is unknown if that is the case\n    \"\"\"\n\n    try:\n        name = errno.errorcode[code]\n    except KeyError:\n        name = \"UNKNOWN\"\n\n    try:\n        description = os.strerror(code)\n    except ValueError:\n        description = \"no description available\"\n\n    return \"{} (errno {}): {}\".format(name, code, description)", "relevance": 1}
{"query": "group by count", "function": "def group(self):\n        \"\"\"(re-)group all logevents by the given group.\"\"\"\n        if hasattr(self, 'group_by'):\n            group_by = self.group_by\n        else:\n            group_by = self.default_group_by\n            if self.args['group'] is not None:\n                group_by = self.args['group']\n\n        self.groups = Grouping(self.logevents, group_by)\n        self.groups.move_items(None, 'others')\n        self.groups.sort_by_size(group_limit=self.args['group_limit'],\n                                 discard_others=self.args['no_others'])", "relevance": 3}
{"query": "group by count", "function": "def groups_count(self):\n        \"\"\"Number of all groups (get-only).\n\n        :getter: Returns number of all groups\n        :type: int\n        \"\"\"\n        if self._keyboard_description.contents.ctrls is not None:\n            return self._keyboard_description.contents.ctrls.contents.num_groups\n        else:\n            groups_source = self._groups_source\n\n            groups_count = 0\n            while (groups_count < XkbNumKbdGroups and\n                   groups_source[groups_count] != None_):\n                groups_count += 1\n\n            return groups_count", "relevance": 2}
{"query": "group by count", "function": "def group_count(self, failures=False):\n        if self.group:\n            return self.get_group_count(self.group, failures)", "relevance": 1}
{"query": "hash set for counting distinct elements", "function": "def init_unique_hash(self):\n        while self.next_regular():\n            hash = hashlib.sha1(self.seq.upper().encode('utf-8')).hexdigest()\n            if hash in self.unique_hash_dict:\n                self.unique_hash_dict[hash]['ids'].append(self.id)\n                self.unique_hash_dict[hash]['count'] += 1\n            else:\n                self.unique_hash_dict[hash] = {'id': self.id,\n                                               'ids': [self.id],\n                                               'seq': self.seq,\n                                               'count': 1}\n\n        self.unique_hash_list = [i[1] for i in sorted([(self.unique_hash_dict[hash]['count'], hash)\\\n                        for hash in self.unique_hash_dict], reverse=True)]\n\n\n        self.total_unique = len(self.unique_hash_dict)\n        self.reset()", "relevance": 3}
{"query": "hash set for counting distinct elements", "function": "def distinct(self, *args, **kwargs):\n        return CursorWrapper(\n            self.cursor.distinct(*args, **kwargs), self.instance)", "relevance": 2}
{"query": "hash set for counting distinct elements", "function": "def clone_with_new_elements(\n            self,\n            new_elements,\n            drop_keywords=set([]),\n            rename_dict={},\n            extra_kwargs={}):\n        \"\"\"\n        Create another Collection of the same class and with same state but\n        possibly different entries. Extra parameters to control which keyword\n        arguments get passed to the initializer are necessary since derived\n        classes have different constructors than the base class.\n        \"\"\"\n        kwargs = dict(\n            elements=new_elements,\n            distinct=self.distinct,\n            sort_key=self.sort_key,\n            sources=self.sources)\n        for name in drop_keywords:\n            kwargs.pop(name)\n        for old_name, new_name in rename_dict.items():\n            kwargs[new_name] = kwargs.pop(old_name)\n        kwargs.update(extra_kwargs)\n        return self.__class__(**kwargs)", "relevance": 1}
{"query": "heatmap from 3d coordinates", "function": "def heatmap2():\n    x = np.arange(5)\n    pw.heatmap(z=np.arange(25), x=np.tile(x, 5), y=x.repeat(5)).save('fig_heatmap2.html', **options)", "relevance": 3}
{"query": "heatmap from 3d coordinates", "function": "def heatmap(dm, partition=None, cmap=CM.Blues, fontsize=10):\n    \"\"\" heatmap(dm, partition=None, cmap=CM.Blues, fontsize=10)\n    \n    Produce a 2D plot of the distance matrix, with values encoded by\n    coloured cells.\n\n    Args:\n        partition: treeCl.Partition object - if supplied, will reorder\n                   rows and columns of the distance matrix to reflect\n                   the groups defined by the partition\n        cmap: matplotlib colourmap object  - the colour palette to use\n        fontsize: int or None - sets the size of the locus lab\n\n    Returns:\n        matplotlib plottable object\n    \"\"\"\n    assert isinstance(dm, DistanceMatrix)\n    datamax = float(np.abs(dm.values).max())\n    length = dm.shape[0]\n\n    if partition:\n        sorting = np.array(flatten_list(partition.get_membership()))\n        new_dm = dm.reorder(dm.df.columns[sorting])\n    else:\n        new_dm = dm\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    \n    ax.xaxis.tick_top()\n    ax.grid(False)\n\n    tick_positions = np.array(list(range(length))) + 0.5\n    if fontsize is not None:\n        ax.set_yticks(tick_positions)\n        ax.set_xticks(tick_positions)\n        ax.set_xticklabels(new_dm.df.columns, rotation=90, fontsize=fontsize, ha='center')\n        ax.set_yticklabels(new_dm.df.index, fontsize=fontsize, va='center')\n\n    cbar_ticks_at = [0, 0.5 * datamax, datamax]\n    \n    cax = ax.imshow(\n        new_dm.values,\n        interpolation='nearest',\n        extent=[0., length, length, 0.],\n        vmin=0,\n        vmax=datamax,\n        cmap=cmap,\n    )\n    cbar = fig.colorbar(cax, ticks=cbar_ticks_at, format='%1.2g')\n    cbar.set_label('Distance')\n    return fig", "relevance": 2}
{"query": "heatmap from 3d coordinates", "function": "def get_mpl_heatmap_axes(dfr, fig, heatmap_gs):\n    \"\"\"Return axis for Matplotlib heatmap.\"\"\"\n    # Create heatmap axis\n    heatmap_axes = fig.add_subplot(heatmap_gs[1, 1])\n    heatmap_axes.set_xticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0]))\n    heatmap_axes.set_yticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0]))\n    heatmap_axes.grid(False)\n    heatmap_axes.xaxis.tick_bottom()\n    heatmap_axes.yaxis.tick_right()\n    return heatmap_axes", "relevance": 1}
{"query": "how to check if a checkbox is checked", "function": "def check_checkbox(step, value):\n    with AssertContextManager(step):\n        check_box = find_field(world.browser, 'checkbox', value)\n        if not check_box.is_selected():\n            check_box.click()", "relevance": 3}
{"query": "how to check if a checkbox is checked", "function": "def CheckboxHandler(self):\n        if self.Key is not None:\n            self.ParentForm.LastButtonClicked = self.Key\n        else:\n            self.ParentForm.LastButtonClicked = ''\n        self.ParentForm.FormRemainedOpen = True\n        if self.ParentForm.CurrentlyRunningMainloop:\n            self.ParentForm.TKroot.quit()", "relevance": 2}
{"query": "how to check if a checkbox is checked", "function": "def check(self, item_id):\n        \"\"\"Check if an analysis is complete.\n\n        :type  item_id: str\n        :param item_id: File ID to check.\n\n        :rtype:  bool\n        :return: Boolean indicating if a report is done or not.\n        \"\"\"\n        try:\n            return self.jbx.info(item_id).get('status').lower() == 'finished'\n        except jbxapi.JoeException:\n            return False\n\n        return False", "relevance": 1}
{"query": "how to determine a string is a valid word", "function": "def isHttpWord(self, word):\n        if(len(word) < 5):\n            return False\n        else:\n            if(word[0] == ord('h') and word[1] == ord('t') and word[2] == ord('t') and word[3] == ord('p')):\n                return True\n            else:\n                return False", "relevance": 3}
{"query": "how to determine a string is a valid word", "function": "def isWord(self,word,ret_ref_trie=False):\n        # see if @word is present in the current Trie; return True or False\n        letters = utf8.get_letters(word)\n        wLen = len(letters)\n        ref_trie = self.trie\n        ref_word_limits = self.word_limits\n        for itr,letter in enumerate(letters):\n            idx = self.getidx( letter )\n            #print(idx, letter)\n            if itr == (wLen-1):\n                break\n            if not ref_trie[idx][1]:\n                return False #this branch of Trie did not exist\n            ref_trie = ref_trie[idx][1]\n            ref_word_limits = ref_word_limits[idx][1]\n\n        if ret_ref_trie:\n            return ref_word_limits[idx][0],ref_trie,ref_word_limits\n        return ref_word_limits[idx][0]", "relevance": 2}
{"query": "how to determine a string is a valid word", "function": "def valid_scrabble_word(word):\n    \"\"\"Checks if the input word could be played with a full bag of tiles.\n\n    Returns:\n        True or false\n    \"\"\"\n\n    letters_in_bag = {\n        \"a\": 9,\n        \"b\": 2,\n        \"c\": 2,\n        \"d\": 4,\n        \"e\": 12,\n        \"f\": 2,\n        \"g\": 3,\n        \"h\": 2,\n        \"i\": 9,\n        \"j\": 1,\n        \"k\": 1,\n        \"l\": 4,\n        \"m\": 2,\n        \"n\": 6,\n        \"o\": 8,\n        \"p\": 2,\n        \"q\": 1,\n        \"r\": 6,\n        \"s\": 4,\n        \"t\": 6,\n        \"u\": 4,\n        \"v\": 2,\n        \"w\": 2,\n        \"x\": 1,\n        \"y\": 2,\n        \"z\": 1,\n        \"_\": 2,\n    }\n\n    for letter in word:\n        if letter == \"?\":\n            continue\n        try:\n            letters_in_bag[letter] -= 1\n        except KeyError:\n            return False\n        if letters_in_bag[letter] < 0:\n            letters_in_bag[\"_\"] -= 1\n            if letters_in_bag[\"_\"] < 0:\n                return False\n    return True", "relevance": 1}
{"query": "how to empty array", "function": "def no_data(self):\n        try:\n            empty = self.dfdata.empty\n        except AttributeError:\n            empty = True\n        return empty", "relevance": 3}
{"query": "how to empty array", "function": "def empty(shape, dtype=None, **kwargs):\n    \"\"\"Create an array of given shape and type, without initializing entries.\n\n    Args:\n        shape (sequence of ints): 2D shape of the array.\n        dtype (data-type, optional): Desired data-type for the array.\n        kwargs (optional): Other arguments of the array (*coords, attrs, and name).\n\n    Returns:\n        array (decode.array): Decode array without initializing entries.\n    \"\"\"\n    data = np.empty(shape, dtype)\n    return dc.array(data, **kwargs)", "relevance": 2}
{"query": "how to empty array", "function": "def empty_like(self, **overrides):\n        if isinstance(self.array, self.numpy.ndarray):\n            return self.numpy.empty_like(array)\n        else:\n            return self.array.empty_like(**overrides)", "relevance": 1}
{"query": "how to extract zip file recursively", "function": "def extract_zip(zip_file_path):\n    \"\"\"\n    Returns:\n        dict: Dict[str, DataFrame]\n    \"\"\"\n    dfs = {}\n    with zipfile.ZipFile(zip_file_path, mode='r') as z_file:\n        names = z_file.namelist()\n        for name in names:\n            content = z_file.read(name)\n            _, tmp_file_path = tempfile.mkstemp()\n            try:\n                with open(tmp_file_path, 'wb') as tmp_file:\n                    tmp_file.write(content)\n\n                dfs[name] = joblib.load(tmp_file_path)\n            finally:\n                shutil.rmtree(tmp_file_path, ignore_errors=True)\n    return dfs", "relevance": 3}
{"query": "how to extract zip file recursively", "function": "def extract_zip(self):\n        assert self.FILE_COUNT>0\n        try:\n            with zipfile.ZipFile(self.archive_path, \"r\") as zip:\n                namelist = zip.namelist()\n                print(\"namelist():\", namelist)\n                if len(namelist) != self.FILE_COUNT:\n                    msg = (\n                        \"Wrong archive content?!?\"\n                        \" There exists %i files, but it should exist %i.\"\n                        \"Existing names are: %r\"\n                    ) % (len(namelist), self.FILE_COUNT, namelist)\n                    log.error(msg)\n                    raise RuntimeError(msg)\n\n                for filename in namelist:\n                    content = zip.read(filename)\n                    dst = self.file_rename(filename)\n\n                    out_filename=os.path.join(self.ROM_PATH, dst)\n                    with open(out_filename, \"wb\") as f:\n                        f.write(content)\n\n                    if dst == filename:\n                        print(\"%r extracted\" % out_filename)\n                    else:\n                        print(\"%r extracted to %r\" % (filename, out_filename))\n\n                    self.post_processing(out_filename)\n\n        except BadZipFile as err:\n            msg = \"Error extracting archive %r: %s\" % (self.archive_path, err)\n            log.error(msg)\n            raise BadZipFile(msg)", "relevance": 2}
{"query": "how to extract zip file recursively", "function": "def _chmod(self, info, path):\n    # This magic works to extract perm bits from the 32 bit external file attributes field for\n    # unix-created zip files, for the layout, see:\n    #   https://www.forensicswiki.org/wiki/ZIP#External_file_attributes\n    attr = info.external_attr >> 16\n    os.chmod(path, attr)", "relevance": 1}
{"query": "how to get current date", "function": "def is_effective(self):\n        \"\"\"Tests if the current date is within the start end end dates inclusive.\n\n        return: (boolean) - ``true`` if this is effective, ``false``\n                otherwise\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        now = DateTime.utcnow()\n        return self.get_start_date() <= now and self.get_end_date() >= now", "relevance": 3}
{"query": "how to get current date", "function": "def is_effective(self):\n        \"\"\"Tests if the current date is within the start end end dates inclusive.\n\n        return: (boolean) - ``true`` if this is effective, ``false``\n                otherwise\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n        now = DateTime.utcnow()\n        return self.get_start_date() <= now and self.get_end_date() >= now", "relevance": 2}
{"query": "how to get current date", "function": "def date_0utc(date):\n    \"\"\"Get the 0 UTC date for a date\n\n    Parameters\n    ----------\n    date : ee.Date\n\n    Returns\n    -------\n    ee.Date\n\n    \"\"\"\n    return ee.Date.fromYMD(date.get('year'), date.get('month'),\n                           date.get('day'))", "relevance": 1}
{"query": "how to get database table name", "function": "def get_table_location(self, database_name, table_name):\n        \"\"\"\n        Get the physical location of the table\n\n        :param database_name: Name of hive database (schema) @table belongs to\n        :type database_name: str\n        :param table_name: Name of hive table\n        :type table_name: str\n        :return: str\n        \"\"\"\n\n        table = self.get_table(database_name, table_name)\n\n        return table['StorageDescriptor']['Location']", "relevance": 3}
{"query": "how to get database table name", "function": "def table_metadata(self, database, table):\n        \"Fetch table-specific metadata.\"\n        return (self.metadata(\"databases\") or {}).get(database, {}).get(\n            \"tables\", {}\n        ).get(\n            table, {}\n        )", "relevance": 2}
{"query": "how to get database table name", "function": "def main(options):\n\n    filename_database = options[\"--database\"]\n    name_table        = options[\"--table\"]\n\n    print(\"\\npyprel database examples\\n\")\n\n    if os.path.exists(filename_database):\n        print(\"create database {database}\".format(\n            database = filename_database\n        ))\n        create_database(filename = \"database.db\")\n\n    print(\"access database {filename}\".format(\n        filename = filename_database\n    ))\n    database = dataset.connect(\n        \"sqlite:///{filename_database}\".format(\n            filename_database = filename_database\n        )\n    )\n    table = database[name_table]\n\n    print(\"add data to database\")\n    table.insert(dict(\n        name     = \"Legolas Greenleaf\",\n        age      = 2000,\n        country  = \"Mirkwood\",\n        uuid4    = str(uuid.uuid4())\n    ))\n    table.insert(dict(\n        name     = \"Cody Rapol\",\n        age      = 30,\n        country  = \"USA\",\n        activity = \"DDR\",\n        uuid4    = str(uuid.uuid4())\n    ))\n\n    print(\n\"\"\"\ndatabase tables:\\n{tables}\n\\ntable {table} columns:\\n{columns}\n\\ntable {table} row one:\\n{row}\n\"\"\".format(\n            tables  = database.tables,\n            table   = name_table,\n            columns = database[name_table].columns,\n            row     = [entry for entry in table.find(id = \"1\")]\n        )\n    )\n\n    print(\"table {table} printout:\\n\".format(\n        table = name_table\n    ))\n\n    print(\n        pyprel.Table(\n            contents = pyprel.table_dataset_database_table(\n                table = database[name_table]\n            )\n        )\n    )", "relevance": 1}
{"query": "how to get html of website", "function": "def get_website(bucket_name, **conn):\n    try:\n        result = get_bucket_website(Bucket=bucket_name, **conn)\n    except ClientError as e:\n        if \"NoSuchWebsiteConfiguration\" not in str(e):\n            raise e\n        return None\n\n    website = {}\n    if result.get(\"IndexDocument\"):\n        website[\"IndexDocument\"] = result[\"IndexDocument\"]\n    if result.get(\"RoutingRules\"):\n        website[\"RoutingRules\"] = result[\"RoutingRules\"]\n    if result.get(\"RedirectAllRequestsTo\"):\n        website[\"RedirectAllRequestsTo\"] = result[\"RedirectAllRequestsTo\"]\n    if result.get(\"ErrorDocument\"):\n        website[\"ErrorDocument\"] = result[\"ErrorDocument\"]\n\n    return website", "relevance": 3}
{"query": "how to get html of website", "function": "def get_site(self, webspace_name, website_name):\n        '''\n        List the web sites defined on this webspace.\n\n        webspace_name:\n            The name of the webspace.\n        website_name:\n            The name of the website.\n        '''\n        return self._perform_get(self._get_sites_details_path(webspace_name,\n                                                              website_name),\n                                 Site)", "relevance": 2}
{"query": "how to get html of website", "function": "def update_website(self, website):\n        self.connect()\n        website = self.server.update_website(\n            self.session_id,\n            website['name'],\n            website['ip'],\n            website['https'],\n            website['subdomains'],\n            website['certificate'],\n            *website['website_apps']\n        )\n        return website", "relevance": 1}
{"query": "how to make the checkbox checked", "function": "def checkbox_uncheck(self, force_check=False):\n        \"\"\"\n        Wrapper to uncheck a checkbox\n        \"\"\"\n        if self.get_attribute('checked'):\n            self.click(force_click=force_check)", "relevance": 3}
{"query": "how to make the checkbox checked", "function": "def assert_checked_checkbox(self, value):\n    \"\"\"Assert the checkbox with label (recommended), name or id is checked.\"\"\"\n    check_box = find_field(world.browser, 'checkbox', value)\n    assert check_box, \"Cannot find checkbox '{}'.\".format(value)\n    assert check_box.is_selected(), \"Check box should be selected.\"", "relevance": 2}
{"query": "how to make the checkbox checked", "function": "def CheckboxHandler(self):\n        if self.Key is not None:\n            self.ParentForm.LastButtonClicked = self.Key\n        else:\n            self.ParentForm.LastButtonClicked = ''\n        self.ParentForm.FormRemainedOpen = True\n        if self.ParentForm.CurrentlyRunningMainloop:\n            self.ParentForm.TKroot.quit()", "relevance": 1}
{"query": "how to randomly pick a number", "function": "def sphere_pick_polar(d, n=1, rng=None):\n    \"\"\"Return vectors uniformly picked on the unit sphere.\n    Vectors are in a polar representation.\n\n    Parameters\n    ----------\n    d: float\n        The number of dimensions of the space in which the sphere lives.\n    n: integer\n        Number of samples to pick.\n\n    Returns\n    -------\n    r: array, shape (n, d)\n        Sample vectors.\n    \"\"\"\n    if rng is None:\n        rng = np.random\n    a = np.empty([n, d])\n    if d == 1:\n        a[:, 0] = rng.randint(2, size=n) * 2 - 1\n    elif d == 2:\n        a[:, 0] = 1.0\n        a[:, 1] = rng.uniform(-np.pi, +np.pi, n)\n    elif d == 3:\n        u, v = rng.uniform(0.0, 1.0, (2, n))\n        a[:, 0] = 1.0\n        a[:, 1] = np.arccos(2.0 * v - 1.0)\n        a[:, 2] = 2.0 * np.pi * u\n    else:\n        raise Exception('Invalid vector for polar representation')\n    return a", "relevance": 3}
{"query": "how to randomly pick a number", "function": "def randnum_min(self, randnum: int) -> None:\n        if not isinstance(randnum, int):\n            raise TypeError('randnum_min can only be int')\n        if randnum < 0:\n            raise ValueError('randnum_min should be greater than 0')\n        self._randnum_min = randnum", "relevance": 2}
{"query": "how to randomly pick a number", "function": "def tempoAdjust2(self, tempoFactor):\n    \"\"\"\n    Adjust tempo by aggregating active basal cell votes for pre vs. post\n\n    :param tempoFactor: scaling signal to MC clock from last sequence item\n    :return: adjusted scaling signal\n    \"\"\"\n\n    late_votes = (len(self.adtm.getNextBasalPredictedCells()) - len(self.apicalIntersect)) * -1\n    early_votes = len(self.apicalIntersect)\n    votes = late_votes + early_votes\n    print('vote tally', votes)\n\n    if votes > 0:\n      tempoFactor = tempoFactor * 0.5\n      print 'speed up'\n\n    elif votes < 0:\n      tempoFactor = tempoFactor * 2\n      print 'slow down'\n\n    elif votes == 0:\n      print 'pick randomly'\n      if random.random() > 0.5:\n        tempoFactor = tempoFactor * 0.5\n        print 'random pick: speed up'\n      else:\n        tempoFactor = tempoFactor * 2\n        print 'random pick: slow down'\n\n    return tempoFactor", "relevance": 1}
{"query": "how to read .csv file in an efficient way?", "function": "def read_csv(csv_path, delimiter=\",\", header=False):\n    csv_data = []\n\n    with open(csv_path, 'r') as csvfile:\n        csvreader = csv.reader(csvfile, delimiter=delimiter)\n        if header:\n            next(csvreader, None)\n\n        csv_data = zip(*csvreader)\n    return csv_data", "relevance": 3}
{"query": "how to read .csv file in an efficient way?", "function": "def serialize(self):\n        from itertools import chain\n        result = Stream()\n        result << self.version.to_bytes(4, 'little')\n\n        if self.network.tx_timestamp:\n            result << self.timestamp.to_bytes(4, 'little')\n\n        result << Parser.to_varint(len(self.ins))\n        # the most efficient way to flatten a list in python\n        result << bytearray(chain.from_iterable(txin.serialize() for txin in self.ins))\n        result << Parser.to_varint(len(self.outs))\n        # the most efficient way to flatten a list in python\n        result << bytearray(chain.from_iterable(txout.serialize() for txout in self.outs))\n        result << self.locktime\n        return result.serialize()", "relevance": 2}
{"query": "how to read .csv file in an efficient way?", "function": "def _read_csv(self, csv_path):\n        logger.info(\"Reading Dataframe from %s\", csv_path)\n        df = pd.read_csv(csv_path)\n        if 'seqname' in df:\n            # by default, Pandas will infer the type as int,\n            # then switch to str when it hits non-numerical\n            # chromosomes. Make sure whole column has the same type\n            df['seqname'] = df['seqname'].map(str)\n        return df", "relevance": 1}
{"query": "how to read the contents of a .gz compressed file?", "function": "def read_output(filename):\n    if os.path.isfile(filename):\n        with open(filename, 'rb') as f:\n            return f.read().decode('utf-8')\n    elif os.path.isfile('{}.gz'.format(filename)):\n        with gzip.open('{}.gz'.format(filename), 'rb') as f:\n            return f.read().decode('utf-8')\n    elif HAS_LZMA and os.path.isfile('{}.xz'.format(filename)):\n        with open('{}.xz'.format(filename), 'rb') as f:\n            return lzma.LZMADecompressor().decompress(f.read()).decode('utf-8')\n    else:\n        return None", "relevance": 3}
{"query": "how to read the contents of a .gz compressed file?", "function": "def compress(s):\n    zbuf = cStringIO.StringIO()\n    zfile = gzip.GzipFile(mode='wb', compresslevel=6, fileobj=zbuf)\n    zfile.write(s)\n    zfile.close()\n    return zbuf.getvalue()", "relevance": 2}
{"query": "how to read the contents of a .gz compressed file?", "function": "def AddLogFileOptions(self, argument_group):\n    \"\"\"Adds the log file option to the argument group.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n    argument_group.add_argument(\n        '--logfile', '--log_file', '--log-file', action='store',\n        metavar='FILENAME', dest='log_file', type=str, default='', help=(\n            'Path of the file in which to store log messages, by default '\n            'this file will be named: \"{0:s}-YYYYMMDDThhmmss.log.gz\". Note '\n            'that the file will be gzip compressed if the extension is '\n            '\".gz\".').format(self.NAME))", "relevance": 1}
{"query": "how to reverse a string", "function": "def convert_MAC_to_int(addr):\n    reverse_bytes_str = addr.split(':')\n    reverse_bytes_str.reverse()\n    addr_str = \"\".join(reverse_bytes_str)\n    return int(addr_str, 16)", "relevance": 3}
{"query": "how to reverse a string", "function": "def to_reverse(self):\n        \"\"\"Convert the IP address to a PTR record.\n\n        Using the .in-addr.arpa zone for IPv4 and .ip6.arpa for IPv6 addresses.\n\n        >>> ip = IP('192.0.2.42')\n        >>> print(ip.to_reverse())\n        42.2.0.192.in-addr.arpa\n        >>> print(ip.to_ipv6().to_reverse())\n        0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.a.2.2.0.0.0.0.c.2.0.0.2.ip6.arpa\n        \"\"\"\n        if self.v == 4:\n            return '.'.join(list(self.dq.split('.')[::-1]) + ['in-addr', 'arpa'])\n        else:\n            return '.'.join(list(self.hex())[::-1] + ['ip6', 'arpa'])", "relevance": 2}
{"query": "how to reverse a string", "function": "def _build_url_silent(self, _name, **kwargs):\n        subreverse = self\n        used_args = set()\n        for part in _name.split('.'):\n            if not subreverse._ready and subreverse._need_arguments:\n                used_args |= subreverse.url_arguments\n                subreverse = subreverse(**kwargs)\n            subreverse = getattr(subreverse, part)\n        if not subreverse._ready and subreverse._is_endpoint:\n            used_args |= subreverse.url_arguments\n            subreverse = subreverse(**kwargs)\n        return used_args, subreverse", "relevance": 1}
{"query": "html encode string", "function": "def _encode_str(self, obj, escape_quotes=True):\n        \"\"\"Return an ASCII-only JSON representation of a Python string\"\"\"\n        def replace(match):\n            s = match.group(0)\n            try:\n                if escape_quotes:\n                    return ESCAPE_DCT[s]\n                else:\n                    return BASE_ESCAPE_DCT[s]\n            except KeyError:\n                n = ord(s)\n                if n < 0x10000:\n                    return '\\\\u{0:04x}'.format(n)\n                else:\n                    # surrogate pair\n                    n -= 0x10000\n                    s1 = 0xd800 | ((n >> 10) & 0x3ff)\n                    s2 = 0xdc00 | (n & 0x3ff)\n                    return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n        if escape_quotes:\n            return '\"' + ESCAPE_ASCII.sub(replace, obj) + '\"'\n        else:\n            return BASE_ESCAPE_ASCII.sub(replace, obj)", "relevance": 3}
{"query": "html encode string", "function": "def toPdf(self):\n        html = safe_unicode(self.template()).encode('utf-8')\n        pdf_data = createPdf(html)\n        return pdf_data", "relevance": 2}
{"query": "html encode string", "function": "def toPdf(self):\n        html = safe_unicode(self.template()).encode('utf-8')\n        pdf_data = createPdf(html)\n        return pdf_data", "relevance": 1}
{"query": "html entities replace", "function": "def escape(self, string):\n        \"\"\"\n        Returns *string* with all instances of '<', '>', and '&' converted into\n        HTML entities.\n        \"\"\"\n        html_entities = {\"&\": \"&amp;\", '<': '&lt;', '>': '&gt;'}\n        return HTML(\"\".join(html_entities.get(c, c) for c in string))", "relevance": 3}
{"query": "html entities replace", "function": "def decode_html_entities(html):\n    \"\"\"\n    Decodes a limited set of HTML entities.\n    \"\"\"\n    if not html:\n        return html\n\n    for entity, char in six.iteritems(html_entity_map):\n        html = html.replace(entity, char)\n\n    return html", "relevance": 2}
{"query": "html entities replace", "function": "def _partition_keys_for_xml(self, o):\n        \"\"\"Breaks o into four content type by key syntax:\n            attrib keys (start with '@'),\n            text (value associated with the '$' or None),\n            child element keys (all others)\n            meta element\n        \"\"\"\n        ak = {}\n        tk = None\n        ck = {}\n        mc = {}\n        # _LOG.debug('o = {o}'.format(o=o))\n        for k, v in o.items():\n            if k.startswith('@'):\n                if k == '@xmlns':\n                    if '$' in v:\n                        ak['xmlns'] = v['$']\n                    for nsk, nsv in v.items():\n                        if nsk != '$':\n                            ak['xmlns:' + nsk] = nsv\n                else:\n                    s = k[1:]\n                    if isinstance(v, bool):\n                        v = u'true' if v else u'false'\n                    ak[s] = UNICODE(v)\n            elif k == '$':\n                tk = v\n            elif k.startswith('^') and (not self._migrating_from_bf):\n                s = k[1:]\n                val = _convert_hbf_meta_val_for_xml(s, v)\n                _add_value_to_dict_bf(mc, s, val)\n            elif (k == u'meta') and self._migrating_from_bf:\n                s, val = _convert_bf_meta_val_for_xml(v)\n                _add_value_to_dict_bf(mc, s, val)\n            else:\n                ck[k] = v\n        return ak, tk, ck, mc", "relevance": 1}
{"query": "httpclient post json", "function": "async def _http_post(self, url, data):\n        data = json.dumps(data)\n        headers = {\"Authorization\": \"GoogleLogin auth={0}\".format(self.token),\n                   \"Content-type\": \"application/json\"}\n        res = await self.session.request(\n            'POST',\n            FULL_SJ_URL + url,\n            data=data,\n            headers=headers,\n            params={'tier': 'aa',\n                    'hl': 'en_US',\n                    'dv': 0,\n                    'alt': 'json'})\n        ret = await res.json()\n        return ret", "relevance": 4}
{"query": "httpclient post json", "function": "def postjson( request, url, data):\n    if isinstance(data, dict) or isinstance(data,list) or isinstance(data, tuple):\n        data = json.dumps(data)\n    if url and url[0] == '/': url = url[1:]\n    docservereq = Request(\"http://\" + settings.FOLIADOCSERVE_HOST + \":\" + str(settings.FOLIADOCSERVE_PORT) + \"/\" + url + '/' + sid) #or opener.open()\n    setsid(docservereq, getsid(request))\n    docservereq.add_header('Content-Type', 'application/json')\n    f = urlopen(docservereq, urlencode(data).encode('utf-8'))\n    if sys.version < '3':\n        contents = unicode(f.read(),'utf-8')\n    else:\n        contents = str(f.read(),'utf-8')\n    f.close()\n    if contents and contents[0] == '{':\n        #assume this is json\n        return json.loads(contents)\n    elif contents:\n        return contents\n    else:\n        return None", "relevance": 3}
{"query": "httpclient post json", "function": "def post(self, json=None):\n        \"\"\"Send a POST request and return the JSON decoded result.\n\n        Args:\n            json (dict, optional): Object to encode and send in request.\n\n        Returns:\n            mixed: JSON decoded response data.\n        \"\"\"\n        return self._call('post', url=self.endpoint, json=json)", "relevance": 1}
{"query": "initializing array", "function": "def zeros(stype, shape, ctx=None, dtype=None, **kwargs):\n    \"\"\"Return a new array of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    stype: string\n        The storage type of the empty array, such as 'row_sparse', 'csr', etc\n    shape : int or tuple of int\n        The shape of the empty array\n    ctx : Context, optional\n        An optional device context (default is the current default context)\n    dtype : str or numpy.dtype, optional\n        An optional value type (default is `float32`)\n\n    Returns\n    -------\n    RowSparseNDArray or CSRNDArray\n        A created array\n    Examples\n    --------\n    >>> mx.nd.sparse.zeros('csr', (1,2))\n    <CSRNDArray 1x2 @cpu(0)>\n    >>> mx.nd.sparse.zeros('row_sparse', (1,2), ctx=mx.cpu(), dtype='float16').asnumpy()\n    array([[ 0.,  0.]], dtype=float16)\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    if stype == 'default':\n        return _zeros_ndarray(shape, ctx=ctx, dtype=dtype, **kwargs)\n    if ctx is None:\n        ctx = current_context()\n    dtype = mx_real_t if dtype is None else dtype\n    if stype in ('row_sparse', 'csr'):\n        aux_types = _STORAGE_AUX_TYPES[stype]\n    else:\n        raise ValueError(\"unknown storage type\" + stype)\n    out = _ndarray_cls(_new_alloc_handle(stype, shape, ctx, True, dtype, aux_types))\n    return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, out=out, **kwargs)", "relevance": 3}
{"query": "initializing array", "function": "def initialize(self):\n        LineSinkStringBase.initialize(self)\n        self.aq.add_element(self)\n        # self.pc = np.array([ls.pc for ls in self.lslist]).flatten()\n        if len(self.hls) == 1:\n            self.pc = self.hls * self.aq.T[self.layers] * np.ones(self.nparam)\n        elif len(self.hls) == self.nls:  # head specified at centers\n            self.pc = (self.hls[:, np.newaxis] * self.aq.T[self.layers]).flatten()\n        elif len(self.hls) == 2:\n            L = np.array([ls.L for ls in self.lslist])\n            Ltot = np.sum(L)\n            xp = np.zeros(self.nls)\n            xp[0] = 0.5 * L[0]\n            for i in range(1, self.nls):\n                xp[i] = xp[i - 1] + 0.5 * (L[i - 1] + L[i])\n            self.hls = np.interp(xp, [0, Ltot], self.hls)\n            self.pc = (self.hls[:, np.newaxis] * self.aq.T[self.layers]).flatten()\n        else:\n            print('Error: hls entry not supported')\n        self.resfac = 0.0", "relevance": 2}
{"query": "initializing array", "function": "def _init(init, X, N, rank, dtype):\n    \"\"\"\n    Initialization for CP models\n    \"\"\"\n    Uinit = [None for _ in range(N)]\n    if isinstance(init, list):\n        Uinit = init\n    elif init == 'random':\n        for n in range(1, N):\n            Uinit[n] = array(rand(X.shape[n], rank), dtype=dtype)\n    elif init == 'nvecs':\n        for n in range(1, N):\n            Uinit[n] = array(nvecs(X, n, rank), dtype=dtype)\n    else:\n        raise 'Unknown option (init=%s)' % str(init)\n    return Uinit", "relevance": 1}
{"query": "json to xml conversion", "function": "def xml_to_json(root):\n    \"\"\"Convert an Open511 XML document or document fragment to JSON.\n\n    Takes an lxml Element object. Returns a dict ready to be JSON-serialized.\"\"\"\n    j = {}\n\n    if len(root) == 0:  # Tag with no children, return str/int\n        return _maybe_intify(root.text)\n\n    if len(root) == 1 and root[0].tag.startswith('{' + NS_GML):  # GML\n        return gml_to_geojson(root[0])\n\n    if root.tag == 'open511':\n        j['meta'] = {'version': root.get('version')}\n\n    for elem in root:\n        name = elem.tag\n        if name == 'link' and elem.get('rel'):\n            name = elem.get('rel') + '_url'\n            if name == 'self_url':\n                name = 'url'\n            if root.tag == 'open511':\n                j['meta'][name] = elem.get('href')\n                continue\n        elif name.startswith('{' + NS_PROTECTED):\n            name = '!' + name[name.index('}') + 1:] \n        elif name[0] == '{':\n            # Namespace!\n            name = '+' + name[name.index('}') + 1:]\n\n        if name in j:\n            continue  # duplicate\n        elif elem.tag == 'link' and not elem.text:\n            j[name] = elem.get('href')\n        elif len(elem):\n            if name == 'grouped_events':\n                # An array of URLs\n                j[name] = [xml_link_to_json(child, to_dict=False) for child in elem]\n            elif name in ('attachments', 'media_files'):\n                # An array of JSON objects\n                j[name] = [xml_link_to_json(child, to_dict=True) for child in elem]\n            elif all((name == pluralize(child.tag) for child in elem)):\n                # <something><somethings> serializes to a JSON array\n                j[name] = [xml_to_json(child) for child in elem]\n            else:\n                j[name] = xml_to_json(elem)\n        else:\n            if root.tag == 'open511' and name.endswith('s') and not elem.text:\n                # Special case: an empty e.g. <events /> container at the root level\n                # should be serialized to [], not null\n                j[name] = []\n            else:\n                j[name] = _maybe_intify(elem.text)\n\n    return j", "relevance": 3}
{"query": "json to xml conversion", "function": "def convert(self, content, conversion):\n        \"\"\"Convert content to Python data structures.\"\"\"\n        if not conversion:\n            data = content\n        elif self.format == 'json':\n            data = json.loads(content)\n        elif self.format == 'xml':\n            content = xml(content)\n            first = list(content.keys())[0]\n            data = content[first]\n        else:\n            data = content\n        return data", "relevance": 2}
{"query": "json to xml conversion", "function": "def xml_to_root(xml: Union[str, IO]) -> ElementTree.Element:\n    \"\"\"Parse XML into an ElemeTree object.\n\n    Parameters\n    ----------\n    xml : str or file-like object\n        A filename, file object or string version of xml can be passed.\n\n    Returns\n    -------\n    Elementree.Element\n\n    \"\"\"\n    if isinstance(xml, str):\n        if '<' in xml:\n            return ElementTree.fromstring(xml)\n        else:\n            with open(xml) as fh:\n                xml_to_root(fh)\n    tree = ElementTree.parse(xml)\n    return tree.getroot()", "relevance": 1}
{"query": "k means clustering", "function": "def cluster_kmeans(data, n_clusters, **kwargs):\n    \"\"\"\n    Identify clusters using K - Means algorithm.\n\n    Parameters\n    ----------\n    data : array_like\n        array of size [n_samples, n_features].\n    n_clusters : int\n        The number of clusters expected in the data.\n\n    Returns\n    -------\n    dict\n        boolean array for each identified cluster.\n    \"\"\"\n    km = cl.KMeans(n_clusters, **kwargs)\n    kmf = km.fit(data)\n\n    labels = kmf.labels_\n\n    return labels, [np.nan]", "relevance": 4}
{"query": "k means clustering", "function": "def optimal_clustering(df, patch, method='kmeans', statistic='gap', max_K=5):\n    if len(patch) == 1:\n        return [patch]\n\n    if statistic == 'db':\n        if method == 'kmeans':\n            if len(patch) <= 5:\n                K_max = 2\n            else:\n                K_max = min(len(patch) / 2, max_K)\n            clustering = {}\n            db_index = []\n            X = df.ix[patch, :]\n            for k in range(2, K_max + 1):\n                kmeans = cluster.KMeans(n_clusters=k).fit(X)\n                clustering[k] = pd.DataFrame(kmeans.predict(X), index=patch)\n                dist_mu = squareform(pdist(kmeans.cluster_centers_))\n                sigma = []\n                for i in range(k):\n                    points_in_cluster = clustering[k][clustering[k][0] == i].index\n                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n            db_index = np.array(db_index)\n            k_optimal = np.argmin(db_index) + 2\n            return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n\n        elif method == 'agglomerative':\n            if len(patch) <= 5:\n                K_max = 2\n            else:\n                K_max = min(len(patch) / 2, max_K)\n            clustering = {}\n            db_index = []\n            X = df.ix[patch, :]\n            for k in range(2, K_max + 1):\n                agglomerative = cluster.AgglomerativeClustering(n_clusters=k, linkage='average').fit(X)\n                clustering[k] = pd.DataFrame(agglomerative.fit_predict(X), index=patch)\n                tmp = [list(clustering[k][clustering[k][0] == i].index) for i in range(k)]\n                centers = np.array([np.mean(X.ix[c, :], axis=0) for c in tmp])\n                dist_mu = squareform(pdist(centers))\n                sigma = []\n                for i in range(k):\n                    points_in_cluster = clustering[k][clustering[k][0] == i].index\n                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n            db_index = np.array(db_index)\n            k_optimal = np.argmin(db_index) + 2\n            return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n\n    elif statistic == 'gap':\n        X = np.array(df.ix[patch, :])\n        if method == 'kmeans':\n            f = cluster.KMeans\n        gaps = gap(X, ks=range(1, min(max_K, len(patch))), method=f)\n        k_optimal = list(gaps).index(max(gaps))+1\n        clustering = pd.DataFrame(f(n_clusters=k_optimal).fit_predict(X), index=patch)\n        return [list(clustering[clustering[0] == i].index) for i in range(k_optimal)]\n\n    else:\n        raise 'error: only db and gat statistics are supported'", "relevance": 3}
{"query": "k means clustering", "function": "def elbow_method(data, k_min, k_max, distance='euclidean'):\n        \"\"\"\n        Calculates and plots the plot of variance explained - number of clusters\n        Implementation reference: https://github.com/sarguido/k-means-clustering.rst\n\n        :param data: The dataset\n        :param k_min: lowerbound of the cluster range\n        :param k_max: upperbound of the cluster range\n        :param distance: the distance metric, 'euclidean' by default\n        :return:\n        \"\"\"\n        # Determine your k range\n        k_range = range(k_min, k_max)\n\n        # Fit the kmeans model for each n_clusters = k\n        k_means_var = [Clustering.kmeans(k).fit(data) for k in k_range]\n\n        # Pull out the cluster centers for each model\n        centroids = [X.model.cluster_centers_ for X in k_means_var]\n\n        # Calculate the Euclidean distance from\n        # each point to each cluster center\n        k_euclid = [cdist(data, cent, distance) for cent in centroids]\n        dist = [np.min(ke, axis=1) for ke in k_euclid]\n\n        # Total within-cluster sum of squares\n        wcss = [sum(d ** 2) for d in dist]\n\n        # The total sum of squares\n        tss = sum(pdist(data) ** 2) / data.shape[0]\n\n        # The between-cluster sum of squares\n        bss = tss - wcss\n\n        # elbow curve\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        ax.plot(k_range, bss / tss * 100, 'b*-')\n        ax.set_ylim((0, 100))\n        plt.grid(True)\n        plt.xlabel('n_clusters')\n        plt.ylabel('Percentage of variance explained')\n        plt.title('Variance Explained vs. k')\n        plt.show()", "relevance": 2}
{"query": "linear regression", "function": "def linear_regression(self):\n        \"\"\" Linear Regression.\n\n        This function runs linear regression and stores the, \n        1. Model\n        2. Model name \n        3. Mean score of cross validation\n        4. Metrics\n\n        \"\"\"\n\n        model = LinearRegression()\n        scores = []\n\n        kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n        for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)):\n            model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])\n            scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test]))\n\n        mean_score = sum(scores) / len(scores)\n        \n        self.models.append(model)\n        self.model_names.append('Linear Regression')\n        self.max_scores.append(mean_score)\n\n        self.metrics['Linear Regression'] = {}\n        self.metrics['Linear Regression']['R2'] = mean_score\n        self.metrics['Linear Regression']['Adj R2'] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])", "relevance": 3}
{"query": "linear regression", "function": "def regression(self, slope=None):\n        \"\"\"regress tip values against branch values\n\n        Parameters\n        ----------\n        slope : None, optional\n            if given, the slope isn't optimized\n\n        Returns\n        -------\n        dict\n            regression parameters\n        \"\"\"\n        self._calculate_averages()\n\n        clock_model = base_regression(self.tree.root.Q, slope)\n        clock_model['r_val'] = self.explained_variance()\n\n        return clock_model", "relevance": 2}
{"query": "linear regression", "function": "def _linreg_model(fitmodel):\n    output = \"Linear regression model\\n\\n\"\n    for k, v in result_iterator(fitmodel):\n        if k == \"thetas\":\n            output += \"Thetas\\n\"\n            output += \", \".join(map(str, v)) + \"\\n\\n\"\n    return output", "relevance": 1}
{"query": "map to json", "function": "def maps(map_id=None, lang=\"en\"):\n    \"\"\"This resource returns details about maps in the game, including details\n    about floor and translation data on how to translate between world\n    coordinates and map coordinates.\n\n    :param map_id: Only list this map.\n    :param lang: Show localized texts in the specified language.\n\n    The response is a dictionary where the key is the map id and the value is\n    a dictionary containing the following properties:\n\n    map_name (string)\n        The map name.\n\n    min_level (number)\n        The minimal level of this map.\n\n    max_level (number)\n        The maximum level of this map.\n\n    default_floor (number)\n        The default floor of this map.\n\n    floors (list)\n        A list of available floors for this map.\n\n    region_id (number)\n        The id of the region this map belongs to.\n\n    region_name (string)\n        The name of the region this map belongs to.\n\n    continent_id (number)\n        The id of the continent this map belongs to.\n\n    continent_name (string)\n        The name of the continent this map belongs to.\n\n    map_rect (rect)\n        The dimensions of the map.\n\n    continent_rect (rect)\n        The dimensions of the map within the continent coordinate system.\n\n    If a map_id is given, only the values for that map are returned.\n\n    \"\"\"\n    if map_id:\n        cache_name = \"maps.%s.%s.json\" % (map_id, lang)\n        params = {\"map_id\": map_id, \"lang\": lang}\n    else:\n        cache_name = \"maps.%s.json\" % lang\n        params = {\"lang\": lang}\n\n    data = get_cached(\"maps.json\", cache_name, params=params).get(\"maps\")\n    return data.get(str(map_id)) if map_id else data", "relevance": 3}
{"query": "map to json", "function": "def from_json(self, json_value) : \n        self.index_all = json_value.get(\"index_all\", True)\n        self.max_depth = json_value.get(\"max_depth\", -1)\n        self.alias = None\n        self.json_keys = {}\n        if \"alias\" in json_value:\n            self.alias = json_value[\"alias\"]\n        if \"json_keys\" in json_value:\n            self.json_keys = json_value[\"json_keys\"]", "relevance": 2}
{"query": "map to json", "function": "def start_cleaning(self, mode=2, navigation_mode=1, category=None, boundary_id=None):\n        # mode & navigation_mode used if applicable to service version\n        # mode: 1 eco, 2 turbo\n        # navigation_mode: 1 normal, 2 extra care, 3 deep\n        # category: 2 non-persistent map, 4 persistent map\n        # boundary_id: the id of the zone to clean\n\n        # Default to using the persistent map if we support basic-3 or basic-4.\n        if category is None:\n            category = 4 if self.service_version in ['basic-3', 'basic-4'] and self.has_persistent_maps else 2\n\n        if self.service_version == 'basic-1':\n            json = {'reqId': \"1\",\n                    'cmd': \"startCleaning\",\n                    'params': {\n                        'category': category,\n                        'mode': mode,\n                        'modifier': 1}\n                    }\n        elif self.service_version == 'basic-3' or 'basic-4':\n            json = {'reqId': \"1\",\n                    'cmd': \"startCleaning\",\n                    'params': {\n                        'category': category,\n                        'mode': mode,\n                        'modifier': 1,\n                        \"navigationMode\": navigation_mode}\n                    }\n            if boundary_id:\n                json['params']['boundaryId'] = boundary_id\n        elif self.service_version == 'minimal-2':\n            json = {'reqId': \"1\",\n                    'cmd': \"startCleaning\",\n                    'params': {\n                        'category': category,\n                        \"navigationMode\": navigation_mode}\n                    }\n        else:   # self.service_version == 'basic-2'\n            json = {'reqId': \"1\",\n                    'cmd': \"startCleaning\",\n                    'params': {\n                        'category': category,\n                        'mode': mode,\n                        'modifier': 1,\n                        \"navigationMode\": navigation_mode}\n                    }\n\n        response = self._message(json)\n        response_dict = response.json()\n\n        # Fall back to category 2 if we tried and failed with category 4\n        if category == 4 and 'alert' in response_dict and response_dict['alert'] == 'nav_floorplan_load_fail':\n            json['params']['category'] = 2\n            return self._message(json)\n\n        return response", "relevance": 1}
{"query": "matrix multiply", "function": "def __mul__(self, m):\n        if hasattr(m, \"__float__\"):\n            return Matrix(self.a * m, self.b * m, self.c * m,\n                          self.d * m, self.e * m, self.f * m)\n        m1 = Matrix(1,1)\n        return m1.concat(self, m)", "relevance": 3}
{"query": "matrix multiply", "function": "def multiply(self, matrix):\n        positions = [matrix * x for x in self.positions]\n        normals = list(self.normals)\n        uvs = list(self.uvs)\n        return Mesh(positions, normals, uvs)", "relevance": 2}
{"query": "matrix multiply", "function": "def __add__(self, other):\n        if isinstance(other, Matrix):\n            return Matrix(self.matrix + other.matrix)\n        else:\n            return Matrix(self.matrix + other)", "relevance": 1}
{"query": "memoize to disk  - persistent memoization", "function": "def memoize(self, code):\n        pk = hash(self.as_grammar())\n        return\"\"\"\nstart_pos_{2}= self.pos\nif ({0}, start_pos_{2}) in self._p_memoized:\n    result, self.pos = self._p_memoized[({0}, self.pos)]\nelse:\n{1}\n    self._p_memoized[({0}, start_pos_{2})] = result, self.pos\n        \"\"\".format(\n            pk,\n            self._indent(code, 1),\n            self.id,\n            repr(self.rulename)\n        )", "relevance": 3}
{"query": "memoize to disk  - persistent memoization", "function": "def memoize(self, obj):\n        # We want hashing to be sensitive to value instead of reference.\n        # For example we want ['aa', 'aa'] and ['aa', 'aaZ'[:2]]\n        # to hash to the same value and that's why we disable memoization\n        # for strings\n        if isinstance(obj, _bytes_or_unicode):\n            return\n        Pickler.memoize(self, obj)", "relevance": 2}
{"query": "memoize to disk  - persistent memoization", "function": "def memoize(obj):\n  cache = obj.cache = {}\n\n  @functools.wraps(obj)\n  def memoizer(*args, **kwargs):\n    key = tuple(list(args) + sorted(kwargs.items()))\n    if key not in cache:\n      cache[key] = obj(*args, **kwargs)\n    return cache[key]\n  return memoizer", "relevance": 1}
{"query": "nelder mead optimize", "function": "def fit(self, initial_parameters):\n        import sys\n        import logging\n        import warnings\n\n        try:\n            from scipy.optimize import leastsq\n            import scipy\n        except ImportError:\n            print(\"You need to install python-scipy.\")\n            sys.exit(1)\n\n        warnings.filterwarnings('error')\n\n        def residuals(p, eos, v, e):\n            return eos(v, *p) - e\n\n        try:\n            result = leastsq(residuals,\n                             initial_parameters,\n                             args=(self._eos, self._volume, self._energy),\n                             full_output=1)\n        except RuntimeError:\n            logging.exception('Fitting to EOS failed.')\n            raise\n        except (RuntimeWarning, scipy.optimize.optimize.OptimizeWarning):\n            logging.exception('Difficulty in fitting to EOS.')\n            raise\n        else:\n            self.parameters = result[0]", "relevance": 3}
{"query": "nelder mead optimize", "function": "def SCG(f, gradf, x, optargs=(), maxiters=500, max_f_eval=np.inf, xtol=None, ftol=None, gtol=None):\n    \"\"\"\n    Optimisation through Scaled Conjugate Gradients (SCG)\n\n    f: the objective function\n    gradf : the gradient function (should return a 1D np.ndarray)\n    x : the initial condition\n\n    Returns\n    x the optimal value for x\n    flog : a list of all the objective values\n    function_eval number of fn evaluations\n    status: string describing convergence status\n    \"\"\"\n    if xtol is None:\n        xtol = 1e-6\n    if ftol is None:\n        ftol = 1e-6\n    if gtol is None:\n        gtol = 1e-5\n\n    sigma0 = 1.0e-7\n    fold = f(x, *optargs) # Initial function value.\n    function_eval = 1\n    fnow = fold\n    gradnew = gradf(x, *optargs) # Initial gradient.\n    function_eval += 1\n    #if any(np.isnan(gradnew)):\n    #    raise UnexpectedInfOrNan, \"Gradient contribution resulted in a NaN value\"\n    current_grad = np.dot(gradnew, gradnew)\n    gradold = gradnew.copy()\n    d = -gradnew # Initial search direction.\n    success = True # Force calculation of directional derivs.\n    nsuccess = 0 # nsuccess counts number of successes.\n    beta = 1.0 # Initial scale parameter.\n    betamin = 1.0e-15 # Lower bound on scale.\n    betamax = 1.0e15 # Upper bound on scale.\n    status = \"Not converged\"\n\n    flog = [fold]\n\n    iteration = 0\n\n    # Main optimization loop.\n    while iteration < maxiters:\n\n        # Calculate first and second directional derivatives.\n        if success:\n            mu = np.dot(d, gradnew)\n            if mu >= 0:  # pragma: no cover\n                d = -gradnew\n                mu = np.dot(d, gradnew)\n            kappa = np.dot(d, d)\n            sigma = sigma0 / np.sqrt(kappa)\n            xplus = x + sigma * d\n            gplus = gradf(xplus, *optargs)\n            function_eval += 1\n            theta = np.dot(d, (gplus - gradnew)) / sigma\n\n        # Increase effective curvature and evaluate step size alpha.\n        delta = theta + beta * kappa\n        if delta <= 0: # pragma: no cover\n            delta = beta * kappa\n            beta = beta - theta / kappa\n\n        alpha = -mu / delta\n\n        # Calculate the comparison ratio.\n        xnew = x + alpha * d\n        fnew = f(xnew, *optargs)\n        function_eval += 1\n\n        Delta = 2.*(fnew - fold) / (alpha * mu)\n        if Delta >= 0.:\n            success = True\n            nsuccess += 1\n            x = xnew\n            fnow = fnew\n        else:\n            success = False\n            fnow = fold\n\n        # Store relevant variables\n        flog.append(fnow) # Current function value\n\n        iteration += 1\n\n        if success:\n            # Test for termination\n\n            if (np.abs(fnew - fold) < ftol):\n                status = 'converged - relative reduction in objective'\n                break\n#                 return x, flog, function_eval, status\n            elif (np.max(np.abs(alpha * d)) < xtol):\n                status = 'converged - relative stepsize'\n                break\n            else:\n                # Update variables for new position\n                gradold = gradnew\n                gradnew = gradf(x, *optargs)\n                function_eval += 1\n                current_grad = np.dot(gradnew, gradnew)\n                fold = fnew\n                # If the gradient is zero then we are done.\n                if current_grad <= gtol:\n                    status = 'converged - relative reduction in gradient'\n                    break\n                    # return x, flog, function_eval, status\n\n        # Adjust beta according to comparison ratio.\n        if Delta < 0.25:\n            beta = min(4.0 * beta, betamax)\n        if Delta > 0.75:\n            beta = max(0.25 * beta, betamin)\n\n        # Update search direction using Polak-Ribiere formula, or re-start\n        # in direction of negative gradient after nparams steps.\n        if nsuccess == x.size:\n            d = -gradnew\n            beta = 1. # This is not in the original paper\n            nsuccess = 0\n        elif success:\n            Gamma = np.dot(gradold - gradnew, gradnew) / (mu)\n            d = Gamma * d - gradnew\n    else:\n        # If we get here, then we haven't terminated in the given number of\n        # iterations.\n        status = \"maxiter exceeded\"\n\n    return x, flog, function_eval, status", "relevance": 2}
{"query": "nelder mead optimize", "function": "def get_feature(internel_layer, layers, filters, batch_norm = False, **kwargs):\n    for i, num in enumerate(layers):\n        for j in range(num):\n            internel_layer = mx.sym.Convolution(data = internel_layer, kernel=(3, 3), pad=(1, 1), num_filter=filters[i], name=\"conv%s_%s\" %(i + 1, j + 1))\n            if batch_norm:\n                internel_layer = mx.symbol.BatchNorm(data=internel_layer, name=\"bn%s_%s\" %(i + 1, j + 1))\n            internel_layer = mx.sym.Activation(data=internel_layer, act_type=\"relu\", name=\"relu%s_%s\" %(i + 1, j + 1))\n        internel_layer = mx.sym.Pooling(data=internel_layer, pool_type=\"max\", kernel=(2, 2), stride=(2,2), name=\"pool%s\" %(i + 1))\n    return internel_layer", "relevance": 1}
{"query": "normal distribution", "function": "def normal_distribution(mean, variance,\n                        minimum=None, maximum=None, weight_count=23):\n    \"\"\"\n    Return a list of weights approximating a normal distribution.\n\n    Args:\n        mean (float): The mean of the distribution\n        variance (float): The variance of the distribution\n        minimum (float): The minimum outcome possible to\n            bound the output distribution to\n        maximum (float): The maximum outcome possible to\n            bound the output distribution to\n        weight_count (int): The number of weights that will\n            be used to approximate the distribution\n\n    Returns:\n        list: a list of ``(float, float)`` weight tuples\n        approximating a normal distribution.\n\n    Raises:\n        ValueError: ``if maximum < minimum``\n        TypeError: if both ``minimum`` and ``maximum`` are ``None``\n\n    Example:\n        >>> weights = normal_distribution(10, 3,\n        ...                               minimum=0, maximum=20,\n        ...                               weight_count=5)\n        >>> rounded_weights = [(round(value, 2), round(strength, 2))\n        ...                    for value, strength in weights]\n        >>> rounded_weights\n        [(1.34, 0.0), (4.8, 0.0), (8.27, 0.14), (11.73, 0.14), (15.2, 0.0)]\n    \"\"\"\n    # Pin 0 to +- 5 sigma as bounds, or minimum and maximum\n    # if they cross +/- sigma\n    standard_deviation = math.sqrt(variance)\n    min_x = (standard_deviation * -5) + mean\n    max_x = (standard_deviation * 5) + mean\n    step = (max_x - min_x) / weight_count\n    current_x = min_x\n    weights = []\n    while current_x < max_x:\n        weights.append(\n            (current_x, _normal_function(current_x, mean, variance))\n        )\n        current_x += step\n    if minimum is not None or maximum is not None:\n        return bound_weights(weights, minimum, maximum)\n    else:\n        return weights", "relevance": 3}
{"query": "normal distribution", "function": "def _set_normal(self, normal):\n        '''\n\n        Parameters\n        ----------\n        electrodes\n\n        Returns\n        -------\n\n        '''\n        for i, el in enumerate(self.electrodes):\n            el.normal = normal/np.linalg.norm(normal)", "relevance": 2}
{"query": "normal distribution", "function": "def _set_normal(self, normal):\n        '''\n\n        Parameters\n        ----------\n        electrodes\n\n        Returns\n        -------\n\n        '''\n        for i, el in enumerate(self.electrodes):\n            el.normal = normal/np.linalg.norm(normal)", "relevance": 1}
{"query": "output to html file", "function": "def to_html_file(self, path=''):\n        with open('{}/output.html'.format(path), 'w') as file:\n            tpl = self.to_html()\n            file.write(tpl)", "relevance": 3}
{"query": "output to html file", "function": "def write_utf8_html_file(file_name, html_content):\n    with open(file_name, \"w+\", encoding=\"utf-8\") as html_file:\n        html_file.write(html_content)", "relevance": 2}
{"query": "output to html file", "function": "def write_utf8_html_file(file_name, html_content):\n    with open(file_name, \"w+\", encoding=\"utf-8\") as html_file:\n        html_file.write(html_content)", "relevance": 1}
{"query": "parse binary file to custom class", "function": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a .customDestinations-ms file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n\n    Raises:\n      UnableToParseFile: when the file cannot be parsed.\n    \"\"\"\n    file_entry = parser_mediator.GetFileEntry()\n    display_name = parser_mediator.GetDisplayName()\n\n    file_header_map = self._GetDataTypeMap('custom_file_header')\n\n    try:\n      file_header, file_offset = self._ReadStructureFromFileObject(\n          file_object, 0, file_header_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Invalid Custom Destination: {0:s} - unable to parse file header '\n          'with error: {1!s}').format(display_name, exception))\n\n    if file_header.unknown1 != 2:\n      raise errors.UnableToParseFile((\n          'Unsupported Custom Destination file: {0:s} - invalid unknown1: '\n          '{1:d}.').format(display_name, file_header.unknown1))\n\n    if file_header.header_values_type > 2:\n      raise errors.UnableToParseFile((\n          'Unsupported Custom Destination file: {0:s} - invalid header value '\n          'type: {1:d}.').format(display_name, file_header.header_values_type))\n\n    if file_header.header_values_type == 0:\n      data_map_name = 'custom_file_header_value_type_0'\n    else:\n      data_map_name = 'custom_file_header_value_type_1_or_2'\n\n    file_header_value_map = self._GetDataTypeMap(data_map_name)\n\n    try:\n      _, value_data_size = self._ReadStructureFromFileObject(\n          file_object, file_offset, file_header_value_map)\n    except (ValueError, errors.ParseError) as exception:\n      raise errors.UnableToParseFile((\n          'Invalid Custom Destination: {0:s} - unable to parse file header '\n          'value with error: {1!s}').format(display_name, exception))\n\n    file_offset += value_data_size\n    file_size = file_object.get_size()\n    remaining_file_size = file_size - file_offset\n\n    entry_header_map = self._GetDataTypeMap('custom_entry_header')\n    file_footer_map = self._GetDataTypeMap('custom_file_footer')\n\n    # The Custom Destination file does not have a unique signature in\n    # the file header that is why we use the first LNK class identifier (GUID)\n    # as a signature.\n    first_guid_checked = False\n    while remaining_file_size > 4:\n      try:\n        entry_header, entry_data_size = self._ReadStructureFromFileObject(\n            file_object, file_offset, entry_header_map)\n\n      except (ValueError, errors.ParseError) as exception:\n        if not first_guid_checked:\n          raise errors.UnableToParseFile((\n              'Invalid Custom Destination file: {0:s} - unable to parse '\n              'entry header with error: {1!s}').format(\n                  display_name, exception))\n\n        parser_mediator.ProduceExtractionWarning(\n            'unable to parse entry header with error: {0!s}'.format(\n                exception))\n        break\n\n      if entry_header.guid != self._LNK_GUID:\n        if not first_guid_checked:\n          raise errors.UnableToParseFile((\n              'Unsupported Custom Destination file: {0:s} - invalid entry '\n              'header signature offset: 0x{1:08x}.').format(\n                  display_name, file_offset))\n\n        try:\n          # Check if we found the footer instead of an entry header.\n          file_footer, _ = self._ReadStructureFromFileObject(\n              file_object, file_offset, file_footer_map)\n\n          if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n            parser_mediator.ProduceExtractionWarning(\n                'invalid entry header signature at offset: 0x{0:08x}'.format(\n                    file_offset))\n\n        except (ValueError, errors.ParseError) as exception:\n          parser_mediator.ProduceExtractionWarning((\n              'unable to parse footer at offset: 0x{0:08x} with error: '\n              '{1!s}').format(file_offset, exception))\n          break\n\n        # TODO: add support for Jump List LNK file recovery.\n        break\n\n      first_guid_checked = True\n      file_offset += entry_data_size\n      remaining_file_size -= entry_data_size\n\n      lnk_file_size = self._ParseLNKFile(\n          parser_mediator, file_entry, file_offset, remaining_file_size)\n\n      file_offset += lnk_file_size\n      remaining_file_size -= lnk_file_size\n\n    try:\n      file_footer, _ = self._ReadStructureFromFileObject(\n          file_object, file_offset, file_footer_map)\n\n      if file_footer.signature != self._FILE_FOOTER_SIGNATURE:\n        parser_mediator.ProduceExtractionWarning(\n            'invalid footer signature at offset: 0x{0:08x}'.format(file_offset))\n\n    except (ValueError, errors.ParseError) as exception:\n      parser_mediator.ProduceExtractionWarning((\n          'unable to parse footer at offset: 0x{0:08x} with error: '\n          '{1!s}').format(file_offset, exception))", "relevance": 3}
{"query": "parse binary file to custom class", "function": "def parse_bin(path):\n    f = open(path, 'rb')\n\n    s = {}  # the current Scale data to apply to next requester\n    output = []\n\n    # handlers for various fourCC codes\n    methods = {\n        'GPS5': parse_gps,\n        'GPSU': parse_time,\n        'GPSF': parse_fix,\n        'GPSP': parse_precision,\n        'ACCL': parse_accl,\n        'GYRO': parse_gyro,\n    }\n\n    d = {'gps': []}  # up to date dictionary, iterate and fill then flush\n\n    while True:\n        label = f.read(4)\n        if not label:  # eof\n            break\n\n        desc = f.read(4)\n\n        # null length\n        if '00' == binascii.hexlify(desc[0]):\n            continue\n\n        val_size = struct.unpack('>b', desc[1])[0]\n        num_values = struct.unpack('>h', desc[2:4])[0]\n        length = val_size * num_values\n\n        # print \"{} {} of size {} and type {}\".format(num_values, label,\n        # val_size, desc[0])\n\n        if label == 'DVID':\n            if len(d['gps']):  # first one is empty\n                output.append(d)\n            d = {'gps': []}  # reset\n\n        for i in range(num_values):\n            data = f.read(val_size)\n\n            if label in methods:\n                methods[label](data, d, s)\n\n            if label == 'SCAL':\n                if 2 == val_size:\n                    s[i] = struct.unpack('>h', data)[0]\n                elif 4 == val_size:\n                    s[i] = struct.unpack('>i', data)[0]\n                else:\n                    raise Exception('unknown scal size')\n\n        # pack\n        mod = length % 4\n        if mod != 0:\n            seek = 4 - mod\n            f.read(seek)  # discarded\n\n    return output", "relevance": 2}
{"query": "parse binary file to custom class", "function": "def _run():\n    \"\"\"Entry point for package and cli uses\"\"\"\n\n    args = parse_args()\n\n    # parse custom parameters\n    custom_meta = None\n    if args.custom_meta:\n        print \"Adding custom parameters:\"\n        custom_meta = {}\n        try:\n            for item in args.custom_meta.split(','):\n                key, value = item.split(':')\n                custom_meta[key] = value\n                print 'key: %s, value: %s' % (key, value)\n        except Exception as e:\n            sys.stderr.write(\"ERROR: Can not parse custom meta tags! %s\\n\" % (str(e)))\n\n    # we need to store some persistent info, so check if a config file\n    # exists (default location is ~/.centinel/config.ini). If the file\n    # does not exist, then create a new one at run time\n    configuration = centinel.config.Configuration()\n    if args.config:\n        configuration.parse_config(args.config)\n    else:\n        # if the file does not exist, then the default config file\n        # will be used\n        new_configuration = None\n        if os.path.exists(DEFAULT_CONFIG_FILE):\n            configuration.parse_config(DEFAULT_CONFIG_FILE)\n        else:\n            print 'Configuration file does not exist. Creating a new one.'\n            new_configuration = centinel.config.Configuration()\n\n        if not ('version' in configuration.params and\n                configuration.params['version']['version'] == centinel.__version__):\n            if not args.update_config:\n                print ('WARNING: configuration file is from '\n                       'a different version (%s) of '\n                       'Centinel. Run with --update-config to update '\n                       'it.' % (configuration.params['version']['version']))\n            else:\n                new_configuration = centinel.config.Configuration()\n                backup_path = DEFAULT_CONFIG_FILE + \".old\"\n                new_configuration.update(configuration, backup_path)\n\n        if new_configuration is not None:\n            configuration = new_configuration\n            configuration.write_out_config(DEFAULT_CONFIG_FILE)\n            print 'New configuration written to %s' % (DEFAULT_CONFIG_FILE)\n            if args.update_config:\n                sys.exit(0)\n\n    if args.verbose:\n        if 'log' not in configuration.params:\n            configuration.params['log'] = dict()\n        configuration.params['log']['log_level'] = logging.DEBUG\n\n    # add custom meta values from CLI\n    if custom_meta is not None:\n        if 'custom_meta' in configuration.params:\n            configuration.params['custom_meta'].update(custom_meta)\n        else:\n            configuration.params['custom_meta'] = custom_meta\n\n    centinel.conf = configuration.params\n    client = centinel.client.Client(configuration.params)\n    client.setup_logging()\n    # disable cert verification if the flag is set\n    if args.no_verify:\n        configuration.params['server']['verify'] = False\n\n    user = centinel.backend.User(configuration.params)\n    # Note: because we have mutually exclusive arguments, we don't\n    # have to worry about multiple arguments being called\n    if args.sync:\n        centinel.backend.sync(configuration.params)\n    elif args.consent:\n        user.informed_consent()\n    elif args.daemonize:\n        # if we don't have a valid binary location, then exit\n        if not os.path.exists(args.binary):\n            print \"Error: no binary found to daemonize\"\n            exit(1)\n        centinel.daemonize.daemonize(args.auto_update, args.binary,\n            args.user)\n    else:\n        client.run()", "relevance": 1}
{"query": "parse command line argument", "function": "def parse_command_line_arguments():\n    parser = argparse.ArgumentParser( description='Rotates the cell lattice in VASP POSCAR files' )\n    parser.add_argument( 'poscar', help=\"filename of the VASP POSCAR to be processed\" )\n    parser.add_argument( '-a', '--axis', nargs=3, type=float, help=\"vector for rotation axis\", required=True )\n    parser.add_argument( '-d', '--degrees', type=int, help=\"rotation angle in degrees\", required=True )\n    args = parser.parse_args()\n    return( args )", "relevance": 4}
{"query": "parse command line argument", "function": "def main():\n    parser = argparse.ArgumentParser(\n        description='Command line Marketplace client')\n    parser.add_argument('method', type=str,\n                        help='command to be run on arguments',\n                        choices=COMMANDS.keys())\n    parser.add_argument('attrs', metavar='attr', type=str, nargs='*',\n                        help='command arguments')\n    parser.add_argument('-v', action='store_true', default=False,\n                        dest='verbose',\n                        help='Switch to verbose mode')\n\n    args = parser.parse_args()\n\n    client = marketplace.Client(\n        domain=config.MARKETPLACE_DOMAIN,\n        protocol=config.MARKETPLACE_PROTOCOL,\n        port=config.MARKETPLACE_PORT,\n        consumer_key=config.CONSUMER_KEY,\n        consumer_secret=config.CONSUMER_SECRET)\n\n    if args.verbose:\n        logger.setLevel(logging.DEBUG)\n    if args.attrs:\n        result = COMMANDS[args.method](client, *args.attrs)\n    else:\n        result = COMMANDS[args.method](client)\n\n    if result['success']:\n        sys.stdout.write('%s\\n' % result['message'])\n    else:\n        sys.stderr.write('%s\\n' % result['message'])\n        sys.exit(1)", "relevance": 3}
{"query": "parse command line argument", "function": "def get_complete(command):\n    \"\"\"\n    Get the Cmd complete function for the click command\n    :param command: The click Command object\n    :return: the complete_* method for Cmd\n    :rtype: function\n    \"\"\"\n\n    assert isinstance(command, click.Command)\n\n    def complete_(self, text, line, begidx, endidx):  # pylint: disable=unused-argument\n        # Parse the args\n        args = shlex.split(line[:begidx])\n        # Strip of the first item which is the name of the command\n        args = args[1:]\n\n        # Then pass them on to the get_choices method that click uses for completion\n        return list(get_choices(command, command.name, args, text))\n\n    complete_.__name__ = 'complete_%s' % command.name\n    return complete_", "relevance": 1}
{"query": "parse json file", "function": "def parse_file_as_json(self, myfile):\n        try:\n            content = json.loads(myfile[\"f\"])\n        except ValueError:\n            log.warn('Could not parse file as json: {}'.format(myfile[\"fn\"]))\n            return\n        runId = content[\"RunId\"]\n        if runId not in self.bcl2fastq_data:\n            self.bcl2fastq_data[runId] = dict()\n        run_data = self.bcl2fastq_data[runId]\n        for conversionResult in content.get(\"ConversionResults\", []):\n            l = conversionResult[\"LaneNumber\"]\n            lane = 'L{}'.format(conversionResult[\"LaneNumber\"])\n            if lane in run_data:\n                log.debug(\"Duplicate runId/lane combination found! Overwriting: {}\".format(self.prepend_runid(runId, lane)))\n            run_data[lane] = {\n                \"total\": 0,\n                \"total_yield\": 0,\n                \"perfectIndex\": 0,\n                \"samples\": dict(),\n                \"yieldQ30\": 0,\n                \"qscore_sum\": 0\n            }\n            # simplify the population of dictionaries\n            rlane = run_data[lane]\n\n            # Add undetermined barcodes\n            try:\n                unknown_barcode = content['UnknownBarcodes'][l - 1]['Barcodes']\n            except IndexError:\n                unknown_barcode = next(\n                    (item['Barcodes'] for item in content['UnknownBarcodes'] if item['Lane'] == 8),\n                    None\n                )\n            run_data[lane]['unknown_barcodes'] = unknown_barcode\n\n            for demuxResult in conversionResult.get(\"DemuxResults\", []):\n                if demuxResult[\"SampleName\"] == demuxResult[\"SampleName\"]:\n                    sample = demuxResult[\"SampleName\"]\n                else:\n                    sample = \"{}-{}\".format(demuxResult[\"SampleId\"], demuxResult[\"SampleName\"])\n                if sample in run_data[lane][\"samples\"]:\n                    log.debug(\"Duplicate runId/lane/sample combination found! Overwriting: {}, {}\".format(self.prepend_runid(runId, lane), sample))\n                run_data[lane][\"samples\"][sample] = {\n                    \"total\": 0,\n                    \"total_yield\": 0,\n                    \"perfectIndex\": 0,\n                    \"filename\": os.path.join(myfile['root'], myfile[\"fn\"]),\n                    \"yieldQ30\": 0,\n                    \"qscore_sum\": 0\n                }\n                # simplify the population of dictionaries\n                lsample = run_data[lane][\"samples\"][sample]\n                for r in range(1,5):\n                        lsample[\"R{}_yield\".format(r)] = 0\n                        lsample[\"R{}_Q30\".format(r)] = 0\n                        lsample[\"R{}_trimmed_bases\".format(r)] = 0\n                rlane[\"total\"] += demuxResult[\"NumberReads\"]\n                rlane[\"total_yield\"] += demuxResult[\"Yield\"]\n                lsample[\"total\"] += demuxResult[\"NumberReads\"]\n                lsample[\"total_yield\"] += demuxResult[\"Yield\"]\n                for indexMetric in demuxResult.get(\"IndexMetrics\", []):\n                    rlane[\"perfectIndex\"] += indexMetric[\"MismatchCounts\"][\"0\"]\n                    lsample[\"perfectIndex\"] += indexMetric[\"MismatchCounts\"][\"0\"]\n                for readMetric in demuxResult.get(\"ReadMetrics\", []):\n                    r = readMetric[\"ReadNumber\"]\n                    rlane[\"yieldQ30\"] += readMetric[\"YieldQ30\"]\n                    rlane[\"qscore_sum\"] += readMetric[\"QualityScoreSum\"]\n                    lsample[\"yieldQ30\"] += readMetric[\"YieldQ30\"]\n                    lsample[\"qscore_sum\"] += readMetric[\"QualityScoreSum\"]\n                    lsample[\"R{}_yield\".format(r)] += readMetric[\"Yield\"]\n                    lsample[\"R{}_Q30\".format(r)] += readMetric[\"YieldQ30\"]\n                    lsample[\"R{}_trimmed_bases\".format(r)] += readMetric[\"TrimmedBases\"]\n                # Remove unpopulated read keys\n                for r in range(1,5):\n                    if not lsample[\"R{}_yield\".format(r)] and not lsample[\"R{}_Q30\".format(r)] and not lsample[\"R{}_trimmed_bases\".format(r)]:\n                        lsample.pop(\"R{}_yield\".format(r))\n                        lsample.pop(\"R{}_Q30\".format(r))\n                        lsample.pop(\"R{}_trimmed_bases\".format(r))\n            undeterminedYieldQ30 = 0\n            undeterminedQscoreSum = 0\n            undeterminedTrimmedBases = 0\n            if \"Undetermined\" in conversionResult:\n                for readMetric in conversionResult[\"Undetermined\"][\"ReadMetrics\"]:\n                    undeterminedYieldQ30 += readMetric[\"YieldQ30\"]\n                    undeterminedQscoreSum += readMetric[\"QualityScoreSum\"]\n                    undeterminedTrimmedBases += readMetric[\"TrimmedBases\"]\n                run_data[lane][\"samples\"][\"undetermined\"] = {\n                    \"total\": conversionResult[\"Undetermined\"][\"NumberReads\"],\n                    \"total_yield\": conversionResult[\"Undetermined\"][\"Yield\"],\n                    \"perfectIndex\": 0,\n                    \"yieldQ30\": undeterminedYieldQ30,\n                    \"qscore_sum\": undeterminedQscoreSum,\n                    \"trimmed_bases\": undeterminedTrimmedBases\n                }\n\n        # Calculate Percents and averages\n        for lane_id, lane in run_data.items():\n            try:\n                lane[\"percent_Q30\"] = (float(lane[\"yieldQ30\"])\n                    / float(lane[\"total_yield\"])) * 100.0\n            except ZeroDivisionError:\n                lane[\"percent_Q30\"] = \"NA\"\n            try:\n                lane[\"percent_perfectIndex\"] = (float(lane[\"perfectIndex\"])\n                    / float(lane[\"total\"])) * 100.0\n            except ZeroDivisionError:\n                lane[\"percent_perfectIndex\"] = \"NA\"\n            try:\n                lane[\"mean_qscore\"] = float(lane[\"qscore_sum\"]) / float(lane[\"total_yield\"])\n            except ZeroDivisionError:\n                lane[\"mean_qscore\"] = \"NA\"\n            for sample_id, sample in lane[\"samples\"].items():\n                try:\n                    sample[\"percent_Q30\"] = (float(sample[\"yieldQ30\"])\n                        / float(sample[\"total_yield\"])) * 100.0\n                except ZeroDivisionError:\n                    sample[\"percent_Q30\"] = \"NA\"\n                try:\n                    sample[\"percent_perfectIndex\"] = (float(sample[\"perfectIndex\"])\n                        / float(sample[\"total\"])) * 100.0\n                except ZeroDivisionError:\n                    sample[\"percent_perfectIndex\"] = \"NA\"\n                try:\n                    sample[\"mean_qscore\"] = float(sample[\"qscore_sum\"]) / float(sample[\"total_yield\"])\n                except ZeroDivisionError:\n                    sample[\"mean_qscore\"] = \"NA\"", "relevance": 3}
{"query": "parse json file", "function": "def parse_rc_json():\n    \"\"\" Reads the json configuration file(.yasirc.json), parses it and returns the\n    dictionary\n    \"\"\"\n    fname = '.yasirc.json'\n    path = os.path.expanduser('~/' + fname)\n    if os.path.exists(fname):\n        path = os.path.abspath(fname)\n    elif not os.path.exists(path):\n        path = ''\n    content = ''\n    if path:\n        with open(path) as f:\n            content = f.read()\n    ret = {}\n    if content:\n        ret = json.loads(content)\n    return collections.defaultdict(dict, ret)", "relevance": 2}
{"query": "parse json file", "function": "def post(self, data=None, function=None, idempotent_key=None, **kwargs):\n        # Allow us to parse through arbitrary request arguments\n        if data is None:\n            data = {}\n        request_kwargs = {}\n        if 'file' in kwargs:\n            request_kwargs['files'] = {\n                'file': (open(kwargs.get('file'), 'rb'))\n            }\n            kwargs.pop('file', None)\n        # We need this flag to force non-json on file uploads\n        json = True\n        if 'json' in kwargs:\n            json = kwargs.get('json')\n            kwargs.pop('json', None)\n        data = {**data, **kwargs}\n        url = self._build_url(function)\n        response = self.client.post(\n            url,\n            data,\n            json=json,\n            idempotent_key=idempotent_key,\n            **request_kwargs\n        )\n        return self._handle_resource_data(response)", "relevance": 1}
{"query": "parse query string in url", "function": "def parse_query(query, ilogger):\n    \"\"\"\n    Gets either a list of videos from a query, parsing links and search queries\n    and playlists\n\n    Args:\n        query (str): The YouTube search query\n        ilogger (logging.logger): The logger to log API calls to\n\n    Returns:\n        queue (list): The items obtained from the YouTube search\n    \"\"\"\n\n    # Try parsing this as a link\n    p = urlparse(query)\n    if p and p.scheme and p.netloc:\n        if \"youtube\" in p.netloc and p.query and ytdiscoveryapi is not None:\n            query_parts = p.query.split('&')\n            yturl_parts = {}\n            for q in query_parts:\n                s = q.split('=')\n                if len(s) < 2:\n                    continue\n\n                q_name = s[0]\n                q_val = '='.join(s[1:])\n                # Add to the query\n                if q_name not in yturl_parts:\n                    yturl_parts[q_name] = q_val\n\n            if \"list\" in yturl_parts:\n                ilogger.info(\"Queued YouTube playlist from link\")\n                return get_queue_from_playlist(yturl_parts[\"list\"])\n            elif \"v\" in yturl_parts:\n                ilogger.info(\"Queued YouTube video from link\")\n                return [[\"https://www.youtube.com/watch?v={}\".format(yturl_parts[\"v\"]), query]]\n        elif \"soundcloud\" in p.netloc:\n            if scclient is None:\n                ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                return [[query, query]]\n            try:\n                result = scclient.get('/resolve', url=query)\n\n                track_list = []\n                if isinstance(result, ResourceList):\n                    for r in result.data:\n                        tracks = get_sc_tracks(r)\n                        if tracks is not None:\n                            for t in tracks:\n                                track_list.append(t)\n                elif isinstance(result, Resource):\n                    tracks = get_sc_tracks(result)\n                    if tracks is not None:\n                        for t in tracks:\n                            track_list.append(t)\n\n                if track_list is not None and len(track_list) > 0:\n                    ilogger.info(\"Queued SoundCloud songs from link\")\n                    return track_list\n                else:\n                    ilogger.error(\"Could not queue from SoundCloud API\")\n                    return [[query, query]]\n            except Exception as e:\n                logger.exception(e)\n                ilogger.error(\"Could not queue from SoundCloud API, using link\")\n                return [[query, query]]\n        else:\n            ilogger.debug(\"Using url: {}\".format(query))\n            return [[query, query]]\n\n    args = query.split(' ')\n    if len(args) == 0:\n        ilogger.error(\"No query given\")\n        return []\n\n    if args[0].lower() in [\"sp\", \"spotify\"] and spclient is not None:\n        if spclient is None:\n            ilogger.error(\"Host does not support Spotify\")\n            return []\n\n        try:\n            if len(args) > 2 and args[1] in ['album', 'artist', 'song', 'track', 'playlist']:\n                query_type = args[1].lower()\n                query_search = ' '.join(args[2:])\n            else:\n                query_type = 'track'\n                query_search = ' '.join(args[1:])\n            query_type = query_type.replace('song', 'track')\n            ilogger.info(\"Queueing Spotify {}: {}\".format(query_type, query_search))\n            spotify_tracks = search_sp_tracks(query_type, query_search)\n            if spotify_tracks is None or len(spotify_tracks) == 0:\n                ilogger.error(\"Could not queue Spotify {}: {}\".format(query_type, query_search))\n                return []\n            ilogger.info(\"Queued Spotify {}: {}\".format(query_type, query_search))\n            return spotify_tracks\n        except Exception as e:\n            logger.exception(e)\n            ilogger.error(\"Error queueing from Spotify\")\n            return []\n    elif args[0].lower() in [\"sc\", \"soundcloud\"]:\n        if scclient is None:\n            ilogger.error(\"Host does not support SoundCloud\")\n            return []\n\n        try:\n            requests = ['song', 'songs', 'track', 'tracks', 'user', 'playlist', 'tagged', 'genre']\n            if len(args) > 2 and args[1] in requests:\n                query_type = args[1].lower()\n                query_search = ' '.join(args[2:])\n            else:\n                query_type = 'track'\n                query_search = ' '.join(args[1:])\n            query_type = query_type.replace('song', 'track')\n            ilogger.info(\"Queueing SoundCloud {}: {}\".format(query_type, query_search))\n            soundcloud_tracks = search_sc_tracks(query_type, query_search)\n            ilogger.info(\"Queued SoundCloud {}: {}\".format(query_type, query_search))\n            return soundcloud_tracks\n        except Exception as e:\n            logger.exception(e)\n            ilogger.error(\"Could not queue from SoundCloud\")\n            return []\n    elif args[0].lower() in [\"yt\", \"youtube\"] and ytdiscoveryapi is not None:\n        if ytdiscoveryapi is None:\n            ilogger.error(\"Host does not support YouTube\")\n            return []\n\n        try:\n            query_search = ' '.join(args[1:])\n            ilogger.info(\"Queued Youtube search: {}\".format(query_search))\n            return get_ytvideos(query_search, ilogger)\n        except Exception as e:\n            logger.exception(e)\n            ilogger.error(\"Could not queue YouTube search\")\n            return []\n\n    if ytdiscoveryapi is not None:\n        ilogger.info(\"Queued YouTube search: {}\".format(query))\n        return get_ytvideos(query, ilogger)\n    else:\n        ilogger.error(\"Host does not support YouTube\".format(query))\n        return []", "relevance": 3}
{"query": "parse query string in url", "function": "def url_parse_query (query, encoding=None):\n    \"\"\"Parse and re-join the given CGI query.\"\"\"\n    if isinstance(query, unicode):\n        if encoding is None:\n            encoding = url_encoding\n        query = query.encode(encoding, 'ignore')\n    # if ? is in the query, split it off, seen at msdn.microsoft.com\n    append = \"\"\n    while '?' in query:\n        query, rest = query.rsplit('?', 1)\n        append = '?'+url_parse_query(rest)+append\n    l = []\n    for k, v, sep in parse_qsl(query, keep_blank_values=True):\n        k = url_quote_part(k, '/-:,;')\n        if v:\n            v = url_quote_part(v, '/-:,;')\n            l.append(\"%s=%s%s\" % (k, v, sep))\n        elif v is None:\n            l.append(\"%s%s\" % (k, sep))\n        else:\n            # some sites do not work when the equal sign is missing\n            l.append(\"%s=%s\" % (k, sep))\n    return ''.join(l) + append", "relevance": 2}
{"query": "parse query string in url", "function": "def url(self) -> str:\n        \"\"\"\n        path + query url\n        \"\"\"\n        url_str = self.parse_url.path or \"\"\n        if self.parse_url.querystring is not None:\n            url_str += \"?\" + self.parse_url.querystring\n        return url_str", "relevance": 1}
{"query": "positions of substrings in string", "function": "def findAllSubstrings(string, substring):\n    \"\"\" Returns a list of all substring starting positions in string or an empty\n    list if substring is not present in string.\n\n    :param string: a template string\n    :param substring: a string, which is looked for in the ``string`` parameter.\n\n    :returns: a list of substring starting positions in the template string\n    \"\"\"\n    #TODO: solve with regex? what about '.':\n    #return [m.start() for m in re.finditer('(?='+substring+')', string)]\n    start = 0\n    positions = []\n    while True:\n        start = string.find(substring, start)\n        if start == -1:\n            break\n        positions.append(start)\n        #+1 instead of +len(substring) to also find overlapping matches\n        start += 1\n    return positions", "relevance": 3}
{"query": "positions of substrings in string", "function": "def __call__(self, value):\n        for substring in self.substrings:\n            if value.startswith(substring):\n                return value[len(substring):]\n        return value", "relevance": 2}
{"query": "positions of substrings in string", "function": "def substitute_globals(string, globs=None):\n    sub = set(re.findall('\\{(.*?)\\}', string))\n    globs = globs or inspect.currentframe().f_back.f_globals\n    if sub:\n        for item in map(str, sub):\n            string = string.replace(\"${%s}\"%item, globs[item])\n        return string\n    else:\n        return False", "relevance": 1}
{"query": "postgresql connection", "function": "def initialize(g, app):\n    \"\"\"\n    If postgresql url is defined in configuration params a\n    scoped session will be created\n    \"\"\"\n    if 'DATABASES' in app.config and 'POSTGRESQL' in app.config['DATABASES']:\n        # Database connection established for console commands\n        for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n            init_db_conn(k, v)\n\n        if 'test' not in sys.argv:\n            # Establish a new connection every request\n            @app.before_request\n            def before_request():\n                \"\"\"\n                Assign postgresql connection pool to the global\n                flask object at the beginning of every request\n                \"\"\"\n                # inject stack context if not testing\n                from flask import _app_ctx_stack\n                for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n                    init_db_conn(k, v, scopefunc=_app_ctx_stack)\n                g.postgresql_pool = pool\n\n            # avoid to close connections if testing\n            @app.teardown_request\n            def teardown_request(exception):\n                \"\"\"\n                Releasing connection after finish request, not required in unit\n                testing\n                \"\"\"\n                pool = getattr(g, 'postgresql_pool', None)\n                if pool is not None:\n                    for k, v in pool.connections.items():\n                        v.session.remove()\n        else:\n            @app.before_request\n            def before_request():\n                \"\"\"\n                Assign postgresql connection pool to the global\n                flask object at the beginning of every request\n                \"\"\"\n                for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n                    init_db_conn(k, v)\n                g.postgresql_pool = pool", "relevance": 3}
{"query": "postgresql connection", "function": "def role(\n    state, host, name,\n    present=True,\n    password=None, login=True, superuser=False, inherit=False,\n    createdb=False, createrole=False, replication=False, connection_limit=None,\n    # Details for speaking to PostgreSQL via `psql` CLI\n    postgresql_user=None, postgresql_password=None,\n    postgresql_host=None, postgresql_port=None,\n):\n    '''\n    Add/remove PostgreSQL roles.\n\n    + name: name of the role\n    + present: whether the role should be present or absent\n    + login: whether the role can login\n    + superuser: whether role will be a superuser\n    + inherit: whether the role inherits from other roles\n    + createdb: whether the role is allowed to create databases\n    + createrole: whether the role is allowed to create new roles\n    + replication: whether this role is allowed to replicate\n    + connection_limit: the connection limit for the role\n    + postgresql_*: global module arguments, see above\n\n    Updates:\n        pyinfra will not attempt to change existing roles - it will either\n        create or drop roles, but not alter them (if the role exists this\n        operation will make no changes).\n    '''\n\n    roles = host.fact.postgresql_roles(\n        postgresql_user, postgresql_password,\n        postgresql_host, postgresql_port,\n    )\n\n    is_present = name in roles\n\n    # User not wanted?\n    if not present:\n        if is_present:\n            yield make_execute_psql_command(\n                'DROP ROLE {0}'.format(name),\n                user=postgresql_user,\n                password=postgresql_password,\n                host=postgresql_host,\n                port=postgresql_port,\n            )\n        return\n\n    # If we want the user and they don't exist\n    if not is_present:\n        sql_bits = ['CREATE ROLE {0}'.format(name)]\n\n        for key, value in (\n            ('LOGIN', login),\n            ('SUPERUSER', superuser),\n            ('INHERIT', inherit),\n            ('CREATEDB', createdb),\n            ('CREATEROLE', createrole),\n            ('REPLICATION', replication),\n        ):\n            if value:\n                sql_bits.append(key)\n\n        if connection_limit:\n            sql_bits.append('CONNECTION LIMIT {0}'.format(connection_limit))\n\n        if password:\n            sql_bits.append(\"PASSWORD '{0}'\".format(password))\n\n        yield make_execute_psql_command(\n            ' '.join(sql_bits),\n            user=postgresql_user,\n            password=postgresql_password,\n            host=postgresql_host,\n            port=postgresql_port,\n        )", "relevance": 2}
{"query": "postgresql connection", "function": "def connection(self):\n        \"\"\"\n        Convenience property for externally accessing an authenticated\n        connection to the server. This connection is automatically\n        handled by the appcontext, so you do not have to perform an unbind.\n\n        Returns:\n            ldap3.Connection: A bound ldap3.Connection\n        Raises:\n            ldap3.core.exceptions.LDAPException: Since this method is performing\n                a bind on behalf of the caller. You should handle this case\n                occuring, such as invalid service credentials.\n        \"\"\"\n        ctx = stack.top\n        if ctx is None:\n            raise Exception(\"Working outside of the Flask application \"\n                            \"context. If you wish to make a connection outside of a flask\"\n                            \" application context, please handle your connections \"\n                            \"and use manager.make_connection()\")\n\n        if hasattr(ctx, 'ldap3_manager_main_connection'):\n            return ctx.ldap3_manager_main_connection\n        else:\n            connection = self._make_connection(\n                bind_user=self.config.get('LDAP_BIND_USER_DN'),\n                bind_password=self.config.get('LDAP_BIND_USER_PASSWORD'),\n                contextualise=False\n            )\n            connection.bind()\n            if ctx is not None:\n                ctx.ldap3_manager_main_connection = connection\n            return connection", "relevance": 1}
{"query": "pretty print json", "function": "def json_pretty_print(d, file=None):\n    args = {'sort_keys': True, 'indent': 2, 'separators': (',', ': ')}\n    if file:\n        return json.dump(d, file, **args)\n    return json.dumps(d, **args)", "relevance": 4}
{"query": "pretty print json", "function": "def pretty_print(response):\n    parsed = json.loads(json.dumps(response))\n    click.echo(json.dumps(parsed, indent=4, sort_keys=True))", "relevance": 3}
{"query": "pretty print json", "function": "def output(results, output_format='pretty'):\n    if output_format == 'pretty':\n        for u, meta in results.items():\n            print('* {} can be imported from: {}'.format(u, ', '.join(meta['paths'])))\n    elif output_format == 'json':\n        print(json.dumps(results))", "relevance": 2}
{"query": "print model summary", "function": "def summary(model, input_size):\n    \"\"\" Print summary of the model \"\"\"\n    def register_hook(module):\n        def hook(module, input, output):\n            class_name = str(module.__class__).split('.')[-1].split(\"'\")[0]\n            module_idx = len(summary)\n\n            m_key = '%s-%i' % (class_name, module_idx + 1)\n            summary[m_key] = OrderedDict()\n            summary[m_key]['input_shape'] = list(input[0].size())\n            summary[m_key]['input_shape'][0] = -1\n            if isinstance(output, (list, tuple)):\n                summary[m_key]['output_shape'] = [[-1] + list(o.size())[1:] for o in output]\n            else:\n                summary[m_key]['output_shape'] = list(output.size())\n                summary[m_key]['output_shape'][0] = -1\n\n            params = 0\n            if hasattr(module, 'weight') and hasattr(module.weight, 'size'):\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                summary[m_key]['trainable'] = module.weight.requires_grad\n            if hasattr(module, 'bias') and hasattr(module.bias, 'size'):\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n            summary[m_key]['nb_params'] = params\n\n        if (not isinstance(module, nn.Sequential) and\n                not isinstance(module, nn.ModuleList) and\n                not (module == model)):\n            hooks.append(module.register_forward_hook(hook))\n\n    if torch.cuda.is_available():\n        dtype = torch.cuda.FloatTensor\n        model = model.cuda()\n    else:\n        dtype = torch.FloatTensor\n        model = model.cpu()\n\n    # check if there are multiple inputs to the network\n    if isinstance(input_size[0], (list, tuple)):\n        x = [Variable(torch.rand(2, *in_size)).type(dtype) for in_size in input_size]\n    else:\n        x = Variable(torch.rand(2, *input_size)).type(dtype)\n\n    # print(type(x[0]))\n    # create properties\n    summary = OrderedDict()\n    hooks = []\n    # register hook\n    model.apply(register_hook)\n    # make a forward pass\n    # print(x.shape)\n    model(x)\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    print('----------------------------------------------------------------')\n    line_new = '{:>20}  {:>25} {:>15}'.format('Layer (type)', 'Output Shape', 'Param #')\n    print(line_new)\n    print('================================================================')\n    total_params = 0\n    trainable_params = 0\n    for layer in summary:\n        # input_shape, output_shape, trainable, nb_params\n        line_new = '{:>20}  {:>25} {:>15}'.format(layer, str(summary[layer]['output_shape']),\n                                                  '{0:,}'.format(summary[layer]['nb_params']))\n        total_params += summary[layer]['nb_params']\n        if 'trainable' in summary[layer]:\n            if summary[layer]['trainable'] == True:\n                trainable_params += summary[layer]['nb_params']\n        print(line_new)\n    print('================================================================')\n    print('Total params: {0:,}'.format(total_params))\n    print('Trainable params: {0:,}'.format(trainable_params))\n    print('Non-trainable params: {0:,}'.format(total_params - trainable_params))\n    print('----------------------------------------------------------------')", "relevance": 3}
{"query": "print model summary", "function": "def _print_summary(module, case, summary):\n    try:\n        module.print_summary(case, summary)\n    except (NotImplementedError, AttributeError):\n        print(\"    Ran \" + case + \"!\")\n        print(\"\")", "relevance": 2}
{"query": "print model summary", "function": "def print_diff(self, summary1=None, summary2=None):\n        \"\"\"Compute diff between to summaries and print it.\n\n        If no summary is provided, the diff from the last to the current\n        summary is used. If summary1 is provided the diff from summary1\n        to the current summary is used. If summary1 and summary2 are\n        provided, the diff between these two is used.\n        \"\"\"\n        summary.print_(self.diff(summary1=summary1, summary2=summary2))", "relevance": 1}
{"query": "priority queue", "function": "def _flush_queue(self, q, ignore_priority=False):\n        \"\"\"\n        :param q: PriorityQueue instance holding GarbageCollector entries\n        :param ignore_priority: If True - all GarbageCollector entries should be resubmitted\n                If False - only those entries whose waiting time has expired will be resubmitted\n        \"\"\"\n        assert isinstance(q, PriorityQueue)\n\n        current_timestamp = compute_release_time(lag_in_minutes=0)\n        for _ in range(len(q)):\n            entry = q.pop()\n            assert isinstance(entry, PriorityEntry)\n\n            if ignore_priority or entry.release_time < current_timestamp:\n                self._resubmit_uow(entry.entry)\n            else:\n                q.put(entry)\n                break", "relevance": 3}
{"query": "priority queue", "function": "def requeue_job(self, job, queue, priority, delayed_for=None):\n        \"\"\"\n        Requeue a job in a queue with the given priority, possibly delayed\n        \"\"\"\n        job.requeue(queue_name=queue._cached_name,\n                    priority=priority,\n                    delayed_for=delayed_for,\n                    queue_model=self.queue_model)\n\n        if hasattr(job, 'on_requeued'):\n            job.on_requeued(queue)\n\n        self.log(self.job_requeue_message(job, queue))", "relevance": 2}
{"query": "priority queue", "function": "def enqueue(self, data, priority=None):\n        if priority:\n            raise NotImplementedError('Task priorities are not supported by '\n                                      'this storage.')\n        self.conn.lpush(self.queue_key, data)", "relevance": 1}
{"query": "randomly extract x items from a list", "function": "def weighted_random_choice(items):\n    \"\"\"\n    Returns a weighted random choice from a list of items.\n    :param items: A list of tuples (object, weight)\n    :return: A random object, whose likelihood is proportional to its weight.\n    \"\"\"\n    l = list(items)\n    r = random.random() * sum([i[1] for i in l])\n    for x, p in l:\n        if p > r:\n            return x\n        r -= p\n    return None", "relevance": 3}
{"query": "randomly extract x items from a list", "function": "def random_output(self, max=100):\n        \"\"\" Generate a list of elements from the markov chain.\n            The `max` value is in place in order to prevent excessive iteration.\n        \"\"\"\n        output = []\n        item1 = item2 = MarkovChain.START\n        for i in range(max-3):\n            item3 = self[(item1, item2)].roll()\n            if item3 is MarkovChain.END:\n                break\n            output.append(item3)\n            item1 = item2\n            item2 = item3\n        return output", "relevance": 2}
{"query": "randomly extract x items from a list", "function": "def __init__(self, query=None, extraction_sort=None, from_index=None, size=None, random_results=None,\n                 random_seed=None, score_relevance=None, return_max_score=None, timeout=None, **kwargs):\n        \"\"\"\n        Base class for all queries against datasets and the items that they contain on Citrination.\n\n        :param query: One or more :class:`DataQuery` objects with the queries to run.\n        :param extraction_sort: A single :class:`ExtractionSort` object for sorting.\n        :param from_index: Index of the first hit that should be returned.\n        :param size: Total number of hits the should be returned.\n        :param random_results: Whether to return a random set of records.\n        :param random_seed: The random seed to use.\n        :param score_relevance: Whether to use relevance scoring.\n        :param return_max_score: Whether to return the maximum score.\n        :param timeout: The number of milliseconds to wait for the query to execute.\n        \"\"\"\n        super(BaseReturningQuery, self).__init__(query=query, extraction_sort=extraction_sort, **kwargs)\n        if 'from' in 'kwargs':\n            self.from_index = kwargs['from']\n        self._from = None\n        self.from_index = from_index\n        self._size = None\n        self.size = size\n        self._random_results = None\n        self.random_results = random_results\n        self._random_seed = None\n        self.random_seed = random_seed\n        self._score_relevance = None\n        self.score_relevance = score_relevance\n        self._return_max_score = None\n        self.return_max_score = return_max_score\n        self._timeout = None\n        self.timeout = timeout", "relevance": 1}
{"query": "read properties file", "function": "def read_properties(self):\n        stream = self.dir.get('properties')\n        if stream is None:\n            return\n\n        s = stream.open()\n\n        # read the whole stream\n        f = BytesIO(s.read())\n\n        (byte_order, version, entry_count) = P_HEADER_STRUCT.unpack(f.read(4))\n\n        if byte_order != 0x4c:\n            raise NotImplementedError(\"be byteorder\")\n\n        props = array.array(str('H'))\n        if hasattr(props, 'frombytes'):\n            props.frombytes(f.read(6 * entry_count))\n        else:\n            props.fromstring(f.read(6 * entry_count))\n\n        if sys.byteorder == 'big':\n            props.byteswap()\n\n        for i in range(entry_count):\n            index = i * 3\n            pid = props[index + 0]\n            format = props[index + 1]\n            byte_size = props[index + 2]\n\n            data = f.read(byte_size)\n            p = property_formats[format](self, pid, format, version)\n            p.data = data\n            self.property_entries[pid] = p\n\n        for p in self.property_entries.values():\n            p.decode()\n            if isinstance(p, (properties.StrongRefSetProperty,\n                              properties.StrongRefVectorProperty,\n                              properties.WeakRefArrayProperty)):\n                p.read_index()", "relevance": 3}
{"query": "read properties file", "function": "def read(self, read_size=-1):\n        \"\"\"\n        Read given number of bytes from file.\n        :param read_size: Number of bytes to read. -1 to read all.\n        :return: Number of bytes read and data that was read.\n        \"\"\"\n        read_str = self.file.read(read_size)\n        return len(read_str), read_str", "relevance": 2}
{"query": "read properties file", "function": "def from_file(cls, file):\n        \"\"\"Create the object from a *file* or *file-like object*.\"\"\"\n        opf_xml = etree.parse(file)\n        # Check if ``file`` is file-like.\n        if hasattr(file, 'read'):\n            name = os.path.basename(file.name)\n            root = os.path.abspath(os.path.dirname(file.name))\n        else:  # ...a filepath\n            name = os.path.basename(file)\n            root = os.path.abspath(os.path.dirname(file))\n        parser = OPFParser(opf_xml)\n\n        # Roll through the item entries\n        manifest = opf_xml.xpath('/opf:package/opf:manifest/opf:item',\n                                 namespaces=EPUB_OPF_NAMESPACES)\n        pkg_items = []\n        for item in manifest:\n            absolute_filepath = os.path.join(root, item.get('href'))\n            properties = item.get('properties', '').split()\n            is_navigation = 'nav' in properties\n            media_type = item.get('media-type')\n            pkg_items.append(Item.from_file(absolute_filepath,\n                                            media_type=media_type,\n                                            is_navigation=is_navigation,\n                                            properties=properties))\n        # Ignore spine ordering, because it is not important\n        #   for our use cases.\n        return cls(name, pkg_items, parser.metadata)", "relevance": 1}
{"query": "read text file line by line", "function": "def readline(self, f):\n        \"\"\"A helper method that only reads uncommented lines\"\"\"\n        while True:\n            line = f.readline()\n            if len(line) == 0:\n                raise EOFError\n            line = line[:line.find('#')]\n            line = line.strip()\n            if len(line) > 0:\n                return line", "relevance": 3}
{"query": "read text file line by line", "function": "def reindent(text, filename):\n    new_lines=[]\n    k=0\n    c=0\n    for n, raw_line in enumerate(text.splitlines()):\n        line=raw_line.strip()\n        if not line or line[0]=='#':\n            new_lines.append(line)\n            continue\n\n        line3 = line[:3]\n        line4 = line[:4]\n        line5 = line[:5]\n        line6 = line[:6]\n        line7 = line[:7]\n        if line3=='if ' or line4 in ('def ', 'for ', 'try:') or\\\n            line6=='while ' or line6=='class ' or line5=='with ':\n            new_lines.append('    '*k+line)\n            k += 1\n            continue\n        elif line5=='elif ' or line5=='else:' or    \\\n            line7=='except:' or line7=='except ' or \\\n            line7=='finally:':\n                c = k-1\n                if c<0:\n                    # print (_format_code(text))\n                    raise ParseError(\"Extra pass founded on line %s:%d\" % (filename, n))\n                new_lines.append('    '*c+line)\n                continue\n        else:\n            new_lines.append('    '*k+line)\n        if line=='pass' or line5=='pass ':\n            k-=1\n        if k<0: k = 0\n    text='\\n'.join(new_lines)\n    return text", "relevance": 2}
{"query": "read text file line by line", "function": "def default_line(self, line):\n        if line.endswith(b\"\\n\"):\n            line = line[:-1]\n        if line.endswith(b\"\\r\"):\n            line = line[:-1]\n        try:\n            line = line.decode(\"ASCII\")\n        except UnicodeDecodeError:\n            pass\n        print(line)", "relevance": 1}
{"query": "reading element from html - <td>", "function": "def tag_to_dict(html):\n    \"\"\"Extract tag's attributes into a `dict`.\"\"\"\n\n    element = document_fromstring(html).xpath(\"//html/body/child::*\")[0]\n    attributes = dict(element.attrib)\n    attributes[\"text\"] = element.text_content()\n    return attributes", "relevance": 3}
{"query": "reading element from html - <td>", "function": "def get_body(doc):\n    [ elem.drop_tree() for elem in doc.xpath('.//script | .//link | .//style') ]\n    raw_html = tostring(doc.body or doc).decode(\"utf-8\")\n    print(raw_html)\n    cleaned = clean_attributes(raw_html)\n    try:\n        #BeautifulSoup(cleaned) #FIXME do we really need to try loading it?\n        return cleaned\n    except Exception: #FIXME find the equivalent lxml error\n        #logging.error(\"cleansing broke html content: %s\\n---------\\n%s\" % (raw_html, cleaned))\n        return raw_html", "relevance": 2}
{"query": "reading element from html - <td>", "function": "def fill_input_radio(self, value, skip_reset=False):\n        elm = self.form_elm.get_elm(xpath='//input[@type=\"radio\"][@name=\"%s\"][@value=\"%s\"]' % (\n            self.elm_name,\n            self.convert_value(value),\n        ))\n        self._click_on_elm_or_his_ancestor(elm)", "relevance": 1}
{"query": "readonly array", "function": "def _readonly(self, inplace=False):\n        # make arrays read only if possib;e\n        df = self if inplace else self.copy()\n        for key, ar in self.columns.items():\n            if isinstance(ar, np.ndarray):\n                df.columns[key] = ar = ar.view() # make new object so we don't modify others\n                ar.flags['WRITEABLE'] = False\n        return df", "relevance": 3}
{"query": "readonly array", "function": "def read(self, nrows=None):\n\n        if nrows is None:\n            nrows = self.nobs\n\n        read_lines = min(nrows, self.nobs - self._lines_read)\n        read_len = read_lines * self.record_length\n        if read_len <= 0:\n            self.close()\n            raise StopIteration\n        raw = self.filepath_or_buffer.read(read_len)\n        data = np.frombuffer(raw, dtype=self._dtype, count=read_lines)\n\n        df = pd.DataFrame(index=range(read_lines))\n        for j, x in enumerate(self.columns):\n            vec = data['s%d' % j]\n            ntype = self.fields[j]['ntype']\n            if ntype == \"numeric\":\n                vec = _handle_truncated_float_vec(\n                    vec, self.fields[j]['field_length'])\n                miss = self._missing_double(vec)\n                v = _parse_float_vec(vec)\n                v[miss] = np.nan\n            elif self.fields[j]['ntype'] == 'char':\n                v = [y.rstrip() for y in vec]\n\n                if self._encoding is not None:\n                    v = [y.decode(self._encoding) for y in v]\n\n            df[x] = v\n\n        if self._index is None:\n            df.index = range(self._lines_read, self._lines_read + read_lines)\n        else:\n            df = df.set_index(self._index)\n\n        self._lines_read += read_lines\n\n        return df", "relevance": 2}
{"query": "readonly array", "function": "def read_array(self):\n        array_length = unpack('>I', self.input.read(4))[0]\n        array_data = AMQPReader(self.input.read(array_length))\n        result = []\n        while array_data.input.tell() < array_length:\n            val = array_data.read_item()\n            result.append(val)\n        return result", "relevance": 1}
{"query": "regex case insensitive", "function": "def image_from(from_value):\n    \"\"\"\n    :param from_value: string like \"image:tag\" or \"image:tag AS name\"\n    :return: tuple of the image and stage name, e.g. (\"image:tag\", None)\n    \"\"\"\n    regex = re.compile(r\"\"\"(?xi)     # readable, case-insensitive regex\n        \\s*                          # ignore leading whitespace\n        (?P<image> \\S+ )             # image and optional tag\n        (?:                          # optional \"AS name\" clause for stage\n            \\s+ AS \\s+\n            (?P<name> \\S+ )\n        )?\n        \"\"\")\n    match = re.match(regex, from_value)\n    return match.group('image', 'name') if match else (None, None)", "relevance": 3}
{"query": "regex case insensitive", "function": "def regex_match(value, regex):\n    pattern = re.compile(regex)\n    if pattern.match(value):\n        return True", "relevance": 2}
{"query": "regex case insensitive", "function": "def match(self, string):\n        \"\"\"Returns True if the argument matches the constant.\"\"\"\n        if self.casesensitive:\n            return self.pattern == os.path.normcase(string)\n        else:\n            return self.pattern.lower() == os.path.normcase(string).lower()", "relevance": 1}
{"query": "replace in file", "function": "def replace_source(self, body, name):\n        logging.debug(_('Processing function body: %s'), name)\n        replaced = re.sub(\n            self.FUNC_NAME_RE, self._func_replacer, body)\n        replaced = re.sub(\n            self.STR_LITERAL_RE, self._string_replacer, replaced)\n        return self._build_strings() + replaced\n        return replaced", "relevance": 3}
{"query": "replace in file", "function": "def replace_f(self, path, arg_name=None):\n        \"\"\"Replace files\"\"\"\n        root, file = os.path.split(path)\n\n        pattern = re.compile(r'(\\<\\<\\<)([A-Za-z_]+)(\\>\\>\\>)')\n        file_path = path\n        fh, abs_path = mkstemp()\n        with open(abs_path, 'w') as new_file:\n            with open(file_path) as old_file:\n                for line in old_file:\n                    for (o, var_name, c) in re.findall(pattern, line):\n                        line = self.handle_args(line, var_name, arg_name)\n                    new_file.write(line)\n        os.close(fh)\n        # Remove original file\n        os.remove(file_path)\n        # Move new file\n        shutil.move(abs_path, file_path)\n\n        pattern = re.compile(r'(\\[\\[)([A-Za-z_]+)(\\]\\])')\n        for (o, var_name, c) in re.findall(pattern, file):\n            file = self.handle_args(file, var_name, isfilename=True)\n        os.rename(path, os.path.join(root, file))", "relevance": 2}
{"query": "replace in file", "function": "def name_replace(self, to_replace, replacement):\n        \"\"\"Replaces part of tag name with new value\"\"\"\n        self.name = self.name.replace(to_replace, replacement)", "relevance": 1}
{"query": "save list to file", "function": "def do_save(self, fname):\n        self.save_file(fname)\n        self.currentfile = fname\n        self.is_changed = False\n        logger.info(_('Project saved to {0}').format(fname))", "relevance": 3}
{"query": "save list to file", "function": "def save(self):\n        filename_list = self.output['source']['filenames']\n        file_path_list = [\n            os.path.join(\n                self.working_dir, filename)\n            for filename in filename_list]\n        self.import_manager.import_result_file_list(\n            self.output, file_path_list, retry=True)", "relevance": 2}
{"query": "save list to file", "function": "def save_app(self, si, logger, vcenter_data_model, reservation_id, save_app_actions, cancellation_context):\n        \"\"\"\n        Cretaes an artifact of an app, that can later be restored\n\n        :param vcenter_data_model: VMwarevCenterResourceModel\n        :param vim.ServiceInstance si: py_vmomi service instance\n        :type si: vim.ServiceInstance\n        :param logger: Logger\n        :type logger: cloudshell.core.logger.qs_logger.get_qs_logger\n        :param list[SaveApp] save_app_actions:\n        :param cancellation_context:\n        \"\"\"\n        results = []\n\n        logger.info('Save Sandbox command starting on ' + vcenter_data_model.default_datacenter)\n\n        if not save_app_actions:\n            raise Exception('Failed to save app, missing data in request.')\n\n        actions_grouped_by_save_types = groupby(save_app_actions, lambda x: x.actionParams.saveDeploymentModel)\n        # artifactSaver or artifactHandler are different ways to save artifacts. For example, currently\n        # we clone a vm, thenk take a snapshot. restore will be to deploy from linked snapshot\n        # a future artifact handler we might develop is save vm to OVF file and restore from file.\n        artifactSaversToActions = {ArtifactHandler.factory(k,\n                                                           self.pyvmomi_service,\n                                                           vcenter_data_model,\n                                                           si,\n                                                           logger,\n                                                           self.deployer,\n                                                           reservation_id,\n                                                           self.resource_model_parser,\n                                                           self.snapshot_saver,\n                                                           self.task_waiter,\n                                                           self.folder_manager,\n                                                           self.port_group_configurer,\n                                                           self.cs)\n                                   : list(g)\n                                   for k, g in actions_grouped_by_save_types}\n\n        self.validate_requested_save_types_supported(artifactSaversToActions,\n                                                     logger,\n                                                     results)\n\n        error_results = [r for r in results if not r.success]\n        if not error_results:\n            logger.info('Handling Save App requests')\n            results = self._execute_save_actions_using_pool(artifactSaversToActions,\n                                                            cancellation_context,\n                                                            logger,\n                                                            results)\n            logger.info('Completed Save Sandbox command')\n        else:\n            logger.error('Some save app requests were not valid, Save Sandbox command failed.')\n        return results", "relevance": 1}
{"query": "scatter plot", "function": "def scatter_plot(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot):\n        \"\"\" 2D or 3D scatter plot.\n\n            :param axes ax: matplotlib axes (use Axes3D if 3D data)\n\n            :param tuple topic_dims: list of (topic, dims) tuples, where topic is a string and dims is a list of dimensions to be plotted for that topic.\n\n            :param int t: time indexes to be plotted\n\n            :param dict kwargs_plot: argument to be passed to matplotlib's plot function, e.g. the style of the plotted points 'or'\n\n            :param bool ms_limits: if set to True, automatically set axes boundaries to the sensorimotor boundaries (default: True)\n        \"\"\"\n        plot_specs = {'marker': 'o', 'linestyle': 'None'}\n        plot_specs.update(kwargs_plot)\n\n        # t_bound = float('inf')\n        # if t is None:\n            # for topic, _ in topic_dims:\n                # t_bound = min(t_bound, self.counts[topic])\n            # t = range(t_bound)\n        # data = self.pack(topic_dims, t)\n        data = self.data_t(topic_dims, t)\n\n        ax.plot(*(data.T), **plot_specs)\n        if ms_limits:\n            ax.axis(self.axes_limits(topic_dims))", "relevance": 3}
{"query": "scatter plot", "function": "def _add_plots_to_output(out, data):\n    \"\"\"Add CNVkit plots summarizing called copy number values.\n    \"\"\"\n    out[\"plot\"] = {}\n    diagram_plot = _add_diagram_plot(out, data)\n    if diagram_plot:\n        out[\"plot\"][\"diagram\"] = diagram_plot\n    scatter = _add_scatter_plot(out, data)\n    if scatter:\n        out[\"plot\"][\"scatter\"] = scatter\n    scatter_global = _add_global_scatter_plot(out, data)\n    if scatter_global:\n        out[\"plot\"][\"scatter_global\"] = scatter_global\n    return out", "relevance": 2}
{"query": "scatter plot", "function": "def plotscatter(irrad: xarray.Dataset, c1: Dict[str, Any], log: bool = False):\n\n    fg = figure()\n    axs = fg.subplots(2, 1, sharex=True)\n\n    transtxt = 'Transmittance'\n\n    ax = axs[0]\n    ax.plot(irrad.wavelength_nm, irrad['transmission'].squeeze())\n    ax.set_title(transtxt)\n    ax.set_ylabel('Transmission (unitless)')\n    ax.grid(True)\n    ax.legend(irrad.angle_deg.values)\n\n    ax = axs[1]\n    if plotNp:\n        Np = (irrad['pathscatter']*10000) * (irrad.wavelength_nm*1e9)/(h*c)\n        ax.plot(irrad.wavelength_nm, Np)\n        ax.set_ylabel('Photons [s$^{-1}$ '+UNITS)\n    else:\n        ax.plot(irrad.wavelength_nm, irrad['pathscatter'].squeeze())\n        ax.set_ylabel('Radiance [W '+UNITS)\n\n    ax.set_xlabel('wavelength [nm]')\n    ax.set_title('Single-scatter Path Radiance')\n    ax.invert_xaxis()\n    ax.autoscale(True, axis='x', tight=True)\n    ax.grid(True)\n\n    if log:\n        ax.set_yscale('log')\n#        ax.set_ylim(1e-8,1)\n\n    try:\n        fg.suptitle(f'Obs. to Space: zenith angle: {c1[\"angle\"]} deg., ')\n        # {datetime.utcfromtimestamp(irrad.time.item()/1e9)}\n    except (AttributeError, TypeError):\n        pass", "relevance": 1}
{"query": "sending binary data over a serial connection", "function": "def read(self, size):\n\t\t\"\"\"\n\t\tRead raw data from the serial connection. This function is not\n\t\tmeant to be called directly.\n\n\t\t:param int size: The number of bytes to read from the serial connection.\n\t\t\"\"\"\n\t\tdata = self.serial_h.read(size)\n\t\tself.logger.debug('read data, length: ' + str(len(data)) + ' data: ' + binascii.b2a_hex(data).decode('utf-8'))\n\t\tself.serial_h.write(ACK)\n\t\tif sys.version_info[0] == 2:\n\t\t\tdata = bytearray(data)\n\t\treturn data", "relevance": 3}
{"query": "sending binary data over a serial connection", "function": "def parse_sgtin_96(sgtin_96):\n    '''Given a SGTIN-96 hex string, parse each segment.\n    Returns a dictionary of the segments.'''\n\n    if not sgtin_96:\n        raise Exception('Pass in a value.')\n\n    if not sgtin_96.startswith(\"30\"):\n        # not a sgtin, not handled\n        raise Exception('Not SGTIN-96.')\n\n    binary = \"{0:020b}\".format(int(sgtin_96, 16)).zfill(96)\n\n    header = int(binary[:8], 2)\n    tag_filter = int(binary[8:11], 2)\n\n    partition = binary[11:14]\n    partition_value = int(partition, 2)\n\n    m, l, n, k = SGTIN_96_PARTITION_MAP[partition_value]\n\n    company_start = 8 + 3 + 3\n    company_end = company_start + m\n    company_data = int(binary[company_start:company_end], 2)\n    if company_data > pow(10, l):\n        # can't be too large\n        raise Exception('Company value is too large')\n    company_prefix = str(company_data).zfill(l)\n\n    item_start = company_end\n    item_end = item_start + n\n    item_data = binary[item_start:item_end]\n    item_number = int(item_data, 2)\n    item_reference = str(item_number).zfill(k)\n\n    serial = int(binary[-38:], 2)\n\n    return {\n        \"header\": header,\n        \"filter\": tag_filter,\n        \"partition\": partition,\n        \"company_prefix\": company_prefix,\n        \"item_reference\": item_reference,\n        \"serial\": serial\n    }", "relevance": 2}
{"query": "sending binary data over a serial connection", "function": "def load_device(self, serial=None):\n        \"\"\"Creates an AndroidDevice for the given serial number.\n\n        If no serial is given, it will read from the ANDROID_SERIAL\n        environmental variable. If the environmental variable is not set, then\n        it will read from 'adb devices' if there is only one.\n        \"\"\"\n        serials = android_device.list_adb_devices()\n        if not serials:\n            raise Error('No adb device found!')\n        # No serial provided, try to pick up the device automatically.\n        if not serial:\n            env_serial = os.environ.get('ANDROID_SERIAL', None)\n            if env_serial is not None:\n                serial = env_serial\n            elif len(serials) == 1:\n                serial = serials[0]\n            else:\n                raise Error(\n                    'Expected one phone, but %d found. Use the -s flag or '\n                    'specify ANDROID_SERIAL.' % len(serials))\n        if serial not in serials:\n            raise Error('Device \"%s\" is not found by adb.' % serial)\n        ads = android_device.get_instances([serial])\n        assert len(ads) == 1\n        self._ad = ads[0]", "relevance": 1}
{"query": "set file attrib hidden", "function": "def sub_el(parent, tag, attrib=None):\n    attrib = attrib or {}\n    tag = get_nstag(tag)\n    attrib = update_attrib(attrib)\n    el = et.SubElement(parent, tag, attrib)  # , nsmap=NAMESPACES)\n    return el", "relevance": 3}
{"query": "set file attrib hidden", "function": "def set_file(self, file=None, is_modified=False, is_untitled=False):\n        \"\"\"\n        Sets the editor file.\n\n        :param File: File to set.\n        :type File: unicode\n        :param is_modified: File modified state.\n        :type is_modified: bool\n        :param is_untitled: File untitled state.\n        :type is_untitled: bool\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n\n        LOGGER.debug(\"> Setting '{0}' editor file.\".format(file))\n        self.__file = file\n        self.__is_untitled = is_untitled\n        self.set_modified(is_modified)\n        self.set_title()\n        return True", "relevance": 2}
{"query": "set file attrib hidden", "function": "def continuation(self, regexp, txt):\n        txt_ = self.txt_list.body.pop().get_text()\n        self.txt_list.body.append(ConsoleText((txt_[1][0][0], \n                                         txt_[0]+regexp.groups()[0])))\n        pos = len(self.txt_list.body)-1\n        self.txt_list.set_focus(pos)\n        return False", "relevance": 1}
{"query": "set working directory", "function": "def _set_WorkingDir(self, path):\n        \"\"\"Sets the working directory\n        \"\"\"\n        self._curr_working_dir = path\n        try:\n            mkdir(self.WorkingDir)\n        except OSError:\n            # Directory already exists\n            pass", "relevance": 3}
{"query": "set working directory", "function": "def set_workdir(self, workdir, chroot=False):\n        \"\"\"Set the working directory of the task.\"\"\"\n        super().set_workdir(workdir, chroot=chroot)\n        # Small hack: the log file of optics is actually the main output file.\n        self.output_file = self.log_file", "relevance": 2}
{"query": "set working directory", "function": "def register_work(self, work, deps=None, manager=None, workdir=None):\n        \"\"\"\n        Register a new :class:`Work` and add it to the internal list, taking into account possible dependencies.\n\n        Args:\n            work: :class:`Work` object.\n            deps: List of :class:`Dependency` objects specifying the dependency of this node.\n                  An empy list of deps implies that this node has no dependencies.\n            manager: The :class:`TaskManager` responsible for the submission of the task.\n                     If manager is None, we use the `TaskManager` specified during the creation of the work.\n            workdir: The name of the directory used for the :class:`Work`.\n\n        Returns:\n            The registered :class:`Work`.\n        \"\"\"\n        if getattr(self, \"workdir\", None) is not None:\n            # The flow has a directory, build the named of the directory of the work.\n            work_workdir = None\n            if workdir is None:\n                work_workdir = os.path.join(self.workdir, \"w\" + str(len(self)))\n            else:\n                work_workdir = os.path.join(self.workdir, os.path.basename(workdir))\n\n            work.set_workdir(work_workdir)\n\n        if manager is not None:\n            work.set_manager(manager)\n\n        self.works.append(work)\n\n        if deps:\n            deps = [Dependency(node, exts) for node, exts in deps.items()]\n            work.add_deps(deps)\n\n        return work", "relevance": 1}
{"query": "socket recv timeout", "function": "def recvall(self, timeout=0.5):\n        \"\"\"\n        Receive the RCON command response\n        :param timeout: The timeout between consequent data receive\n        :return str: The RCON command response with header stripped out\n        \"\"\"\n        response = ''\n        self.socket.setblocking(False)\n        start = time.time()\n        while True:\n            if response and time.time() - start > timeout:\n                break\n            elif time.time() - start > timeout * 2:\n                break\n\n            try:\n                data = self.socket.recv(4096)\n                if data:\n                    response += data.replace(self._rconreplystring, '')\n                    start = time.time()\n                else:\n                    time.sleep(0.1)\n            except socket.error:\n                pass\n\n        return response.strip()", "relevance": 4}
{"query": "socket recv timeout", "function": "def _SetSocketTimeouts(self):\n    \"\"\"Sets the timeouts for socket send and receive.\"\"\"\n    # Note that timeout must be an integer value. If timeout is a float\n    # it appears that zmq will not enforce the timeout.\n    timeout = int(self.timeout_seconds * 1000)\n    receive_timeout = min(\n        self._ZMQ_SOCKET_RECEIVE_TIMEOUT_MILLISECONDS, timeout)\n    send_timeout = min(self._ZMQ_SOCKET_SEND_TIMEOUT_MILLISECONDS, timeout)\n\n    self._zmq_socket.setsockopt(zmq.RCVTIMEO, receive_timeout)\n    self._zmq_socket.setsockopt(zmq.SNDTIMEO, send_timeout)", "relevance": 3}
{"query": "socket recv timeout", "function": "def recvfrom(self, bufsize, flags=0):\n        \"\"\"receive data on a socket that isn't necessarily a 1-1 connection\n\n        .. note:: this method will block until data is available to be read\n\n        :param bufsize:\n            the maximum number of bytes to receive. fewer may be returned,\n            however\n        :type bufsize: int\n        :param flags:\n            flags for the receive call. consult the unix manpage for\n            ``recv(2)`` for what flags are available\n        :type flags: int\n\n        :returns:\n            a two-tuple of ``(data, address)`` -- the string data received and\n            the address from which it was received\n        \"\"\"\n        with self._registered('re'):\n            while 1:\n                if self._closed:\n                    raise socket.error(errno.EBADF, \"Bad file descriptor\")\n                try:\n                    return self._sock.recvfrom(bufsize, flags)\n                except socket.error, exc:\n                    if not self._blocking or exc[0] not in _BLOCKING_OP:\n                        raise\n                    sys.exc_clear()\n                    if self._readable.wait(self.gettimeout()):\n                        raise socket.timeout(\"timed out\")\n                    if scheduler.state.interrupted:\n                        raise IOError(errno.EINTR, \"interrupted system call\")", "relevance": 2}
{"query": "sort string list", "function": "def sort(self, sort_list):\n        \"\"\" Sort \"\"\"\n        order = []\n        for sort in sort_list:\n            if sort_list[sort] == \"asc\":\n                order.append(asc(getattr(self.model, sort, None)))\n            elif sort_list[sort] == \"desc\":\n                order.append(desc(getattr(self.model, sort, None)))\n        return order", "relevance": 3}
{"query": "sort string list", "function": "def sort(posts: List[Post], by: str) -> List[Post]:\n    by_val  = by.replace(\"asc_\", \"\").replace(\"desc_\", \"\")\n\n    in_dict = (ORDER_NUM   if by_val in ORDER_NUM   else\n               ORDER_DATE  if by_val in ORDER_DATE  else\n               ORDER_FUNCS if by_val in ORDER_FUNCS else None)\n\n    if not in_dict:\n        raise ValueError(\n            f\"Got {by_val!r} as ordering method, must be one of: %s\" %\n            \", \".join(set(ORDER_NUM) | set(ORDER_DATE) | set(ORDER_FUNCS))\n        )\n\n    if in_dict == ORDER_FUNCS:\n        posts.sort(key=ORDER_FUNCS[by], reverse=(by != \"random\"))\n        return posts\n\n    by_full = by if by.startswith(\"asc_\") or by.startswith(\"desc_\") else \\\n              f\"%s_{by}\" % in_dict[by][0]\n\n    def sort_key(post: Post) -> int:\n        key = in_dict[by_val][1]\n        key = post.info[key] if not callable(key) else key(post.info)\n        return pend.parse(key) if in_dict == ORDER_DATE else key\n\n    posts.sort(key=sort_key, reverse=by_full.startswith(\"desc_\"))\n    return posts", "relevance": 2}
{"query": "sort string list", "function": "def get_sort_string(self, sort=None):\n        if not sort:\n            sort = self.sort\n        sort_string = ''\n        if not sort == self.default_sort:\n            sort_string = self.sort_parameter + '=' + sort\n        return sort_string", "relevance": 1}
{"query": "sorting multiple arrays based on another arrays sorted order", "function": "def sort_numpy(array, col=0, order_back=False):\n    \"\"\"\n    Sorts the columns for an entire ``ndarrray`` according to sorting\n    one of them.\n    \n    :param array: Array to sort.\n    :type array: ndarray\n    :param col: Master column to sort.\n    :type col: int\n    :param order_back: If True, also returns the index to undo the\n        new order.\n    :type order_back: bool\n    :returns: sorted_array or [sorted_array, order_back]\n    :rtype: ndarray, list\n    \"\"\"\n    x = array[:,col]\n    sorted_index = np.argsort(x, kind = 'quicksort')\n    sorted_array = array[sorted_index]\n    \n    if not order_back:\n        return sorted_array\n    else:\n        n_points = sorted_index.shape[0]\n        order_back = np.empty(n_points, dtype=int)\n        order_back[sorted_index] = np.arange(n_points)\n        return [sorted_array, order_back]", "relevance": 3}
{"query": "sorting multiple arrays based on another arrays sorted order", "function": "def permute_data(arrays, random_state=None):\n  \"\"\"Permute multiple numpy arrays with the same order.\"\"\"\n  if any(len(a) != len(arrays[0]) for a in arrays):\n    raise ValueError('All arrays must be the same length.')\n  if not random_state:\n    random_state = np.random\n  order = random_state.permutation(len(arrays[0]))\n  return [a[order] for a in arrays]", "relevance": 2}
{"query": "sorting multiple arrays based on another arrays sorted order", "function": "def from_arrays(cls, arrays, sortorder=None, names=None):\n        \"\"\"\n        Convert arrays to MultiIndex.\n\n        Parameters\n        ----------\n        arrays : list / sequence of array-likes\n            Each array-like gives one level's value for each data point.\n            len(arrays) is the number of levels.\n        sortorder : int or None\n            Level of sortedness (must be lexicographically sorted by that\n            level).\n        names : list / sequence of str, optional\n            Names for the levels in the index.\n\n        Returns\n        -------\n        index : MultiIndex\n\n        See Also\n        --------\n        MultiIndex.from_tuples : Convert list of tuples to MultiIndex.\n        MultiIndex.from_product : Make a MultiIndex from cartesian product\n                                  of iterables.\n        MultiIndex.from_frame : Make a MultiIndex from a DataFrame.\n\n        Examples\n        --------\n        >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']]\n        >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color'))\n        MultiIndex(levels=[[1, 2], ['blue', 'red']],\n                   codes=[[0, 0, 1, 1], [1, 0, 1, 0]],\n                   names=['number', 'color'])\n        \"\"\"\n        error_msg = \"Input must be a list / sequence of array-likes.\"\n        if not is_list_like(arrays):\n            raise TypeError(error_msg)\n        elif is_iterator(arrays):\n            arrays = list(arrays)\n\n        # Check if elements of array are list-like\n        for array in arrays:\n            if not is_list_like(array):\n                raise TypeError(error_msg)\n\n        # Check if lengths of all arrays are equal or not,\n        # raise ValueError, if not\n        for i in range(1, len(arrays)):\n            if len(arrays[i]) != len(arrays[i - 1]):\n                raise ValueError('all arrays must be same length')\n\n        from pandas.core.arrays.categorical import _factorize_from_iterables\n\n        codes, levels = _factorize_from_iterables(arrays)\n        if names is None:\n            names = [getattr(arr, \"name\", None) for arr in arrays]\n\n        return MultiIndex(levels=levels, codes=codes, sortorder=sortorder,\n                          names=names, verify_integrity=False)", "relevance": 1}
{"query": "string similarity levenshtein", "function": "def levenshtein(left, right):\n    \"\"\"Computes the Levenshtein distance of the two given strings.\n\n    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n    [Row(d=3)]\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))\n    return Column(jc)", "relevance": 4}
{"query": "string similarity levenshtein", "function": "def diff_levenshtein(self, diffs):\n    \"\"\"Compute the Levenshtein distance; the number of inserted, deleted or\n    substituted characters.\n\n    Args:\n      diffs: Array of diff tuples.\n\n    Returns:\n      Number of changes.\n    \"\"\"\n    levenshtein = 0\n    insertions = 0\n    deletions = 0\n    for (op, data) in diffs:\n      if op == self.DIFF_INSERT:\n        insertions += len(data)\n      elif op == self.DIFF_DELETE:\n        deletions += len(data)\n      elif op == self.DIFF_EQUAL:\n        # A deletion and an insertion is one substitution.\n        levenshtein += max(insertions, deletions)\n        insertions = 0\n        deletions = 0\n    levenshtein += max(insertions, deletions)\n    return levenshtein", "relevance": 3}
{"query": "string similarity levenshtein", "function": "def levenshtein_distance_metric(a, b):\n    \"\"\" 1 - farthest apart (same number of words, all diff). 0 - same\"\"\"\n    return (levenshtein_distance(a, b) / (2.0 * max(len(a), len(b), 1)))", "relevance": 2}
{"query": "string to date", "function": "def get_date(date_string):\n    d=None\n    try:\n        d=datetime.datetime.strptime(date_string, \"%d %B %Y\").date()\n    except:\n        d=datetime.datetime.strptime(date_string, \"%Y-%m-%d\").date()\n\n    if d:\n        return d.strftime(\"%Y-%m-%d\")\n    else:\n        return date_string", "relevance": 3}
{"query": "string to date", "function": "def timestamp_serializer(self, date):\n        if not date:\n            return ''\n\n        # Any serialized timestamp should be localized, we need to\n        # convert to UTC before converting to string (DATE_FORMAT uses UTC)\n        date = date.astimezone(pytz.utc)\n\n        return date.strftime(DATE_FORMAT)", "relevance": 2}
{"query": "string to date", "function": "def timestamp_serializer(self, date):\n        if not date:\n            return ''\n\n        # Any serialized timestamp should be localized, we need to\n        # convert to UTC before converting to string (DATE_FORMAT uses UTC)\n        date = date.astimezone(pytz.utc)\n\n        return date.strftime(DATE_FORMAT)", "relevance": 1}
{"query": "underline text in label widget", "function": "def _underline(self):\n        # Underline text\n        up = self.font.underline_position\n        ut = self.font.underline_thickness\n        w = self.font._string_width(self.text)\n        s = '%.2f %.2f %.2f %.2f re f' % (self.cursor.x,\n            self.cursor.y_prime - up, w, ut)\n        return s", "relevance": 3}
{"query": "underline text in label widget", "function": "def setShowRichText(self, state):\n        \"\"\"\n        Sets whether or not to display rich text for this button.\n        \n        :param      state | <bool>\n        \"\"\"\n        self._showRichText = state\n        text = self.text()\n        \n        if state:\n            label = self.richTextLabel()\n            label.setText(text)\n            label.show()\n            super(XPushButton, self).setText('')\n        else:\n            if self._richTextLabel:\n                self._richTextLabel.hide()\n            \n            super(XPushButton, self).setText(text)", "relevance": 2}
{"query": "underline text in label widget", "function": "def do_td_field(self, indent, value, **kwargs):\n        field_name = kwargs.pop('name', None)\n        field = getattr(self.form, field_name)\n        obj = self.form.fields[field_name]\n        if 'label' in kwargs:\n            label = kwargs.pop('label')\n        else:\n            label = obj.label\n        if label:\n            obj.label = label\n            label_text = obj.get_label(_class='field')\n        else:\n            label_text = ''\n            \n        display = field.data or '&nbsp;'\n        if 'width' not in kwargs:\n            kwargs['width'] = 200\n        td = begin_tag('td', **kwargs) + u_str(display) + end_tag('td')\n        return indent * ' ' + '<th align=right width=200>%s</th>%s' % (label_text, td)", "relevance": 1}
{"query": "unique elements", "function": "def weld_unique(array, weld_type):\n    \"\"\"Return the unique elements of the array.\n\n    Parameters\n    ----------\n    array : numpy.ndarray or WeldObject\n        Input array.\n    weld_type : WeldType\n        Type of each element in the input array.\n\n    Returns\n    -------\n    WeldObject\n        Representation of this computation.\n\n    \"\"\"\n    obj_id, weld_obj = create_weld_object(array)\n\n    weld_template = \"\"\"map(\n    tovec(\n        result(\n            for(\n                map(\n                    {array},\n                    |e| \n                        {{e, 0si}}\n                ),\n                dictmerger[{type}, i16, +],\n                |b: dictmerger[{type}, i16, +], i: i64, e: {{{type}, i16}}| \n                    merge(b, e)\n            )\n        )\n    ),\n    |e| \n        e.$0\n)\"\"\"\n\n    weld_obj.weld_code = weld_template.format(array=obj_id,\n                                              type=weld_type)\n\n    return weld_obj", "relevance": 3}
{"query": "unique elements", "function": "def weld_unique(array, weld_type):\n    \"\"\"Return the unique elements of the array.\n\n    Parameters\n    ----------\n    array : numpy.ndarray or WeldObject\n        Input array.\n    weld_type : WeldType\n        Type of each element in the input array.\n\n    Returns\n    -------\n    WeldObject\n        Representation of this computation.\n\n    \"\"\"\n    obj_id, weld_obj = create_weld_object(array)\n\n    weld_template = \"\"\"map(\n    tovec(\n        result(\n            for(\n                map(\n                    {array},\n                    |e| \n                        {{e, 0si}}\n                ),\n                dictmerger[{type}, i16, +],\n                |b: dictmerger[{type}, i16, +], i: i64, e: {{{type}, i16}}| \n                    merge(b, e)\n            )\n        )\n    ),\n    |e| \n        e.$0\n)\"\"\"\n\n    weld_obj.weld_code = weld_template.format(array=obj_id,\n                                              type=weld_type)\n\n    return weld_obj", "relevance": 2}
{"query": "unique elements", "function": "def union_elements(elements):\n    \"\"\"elements = [(chr, s, e, id), ...], this is to join elements that have a\n    deletion in the 'to' species\n    \"\"\"\n\n    if len(elements) < 2: return elements\n    assert set( [e[3] for e in elements] ) == set( [elements[0][3]] ), \"more than one id\"\n    el_id = elements[0][3]\n\n    unioned_elements = []\n    for ch, chgrp in groupby(elements, key=itemgetter(0)):\n        for (s, e) in elem_u( np.array([itemgetter(1, 2)(_) for _ in chgrp], dtype=np.uint) ):\n            if (s < e):\n                unioned_elements.append( (ch, s, e, el_id) )\n    assert len(unioned_elements) <= len(elements)\n    return unioned_elements", "relevance": 1}
{"query": "unzipping large files", "function": "def unzip(self, in_file, out_file):\n        with ZipFile(in_file) as zf:\n            zf.extract('collection.anki2', path=self.tempdir)\n        shutil.move(os.path.join(self.tempdir, 'collection.anki2'),\n                    out_file)\n\n        return out_file", "relevance": 3}
{"query": "unzipping large files", "function": "def file_unzipper(directory):\n   \"\"\" This function will unzip all files in the runroot directory and\n   subdirectories\n   \"\"\"\n   debug.log(\"Unzipping directory (%s)...\"%directory)\n   #FINDING AND UNZIPPING ZIPPED FILES\n   for root, dirs, files in os.walk(directory, topdown=False):\n      if root != \"\":\n         orig_dir = os.getcwd()\n         os.chdir(directory)\n         Popen('gunzip -q -f *.gz > /dev/null 2>&1', shell=True).wait()\n         Popen('unzip -qq -o \"*.zip\" > /dev/null 2>&1', shell=True).wait()\n         Popen('rm -f *.zip > /dev/null 2>&1', shell=True).wait()\n         os.chdir(orig_dir)", "relevance": 2}
{"query": "unzipping large files", "function": "def _lfs_add(files, git):\n    \"\"\"\n    Add any oversized files with lfs.\n    Throws error if a file is bigger than 2GB or git-lfs is not installed.\n    \"\"\"\n    # Check for large files > 100 MB (and huge files > 2 GB)\n    # https://help.github.com/articles/conditions-for-large-files/\n    # https://help.github.com/articles/about-git-large-file-storage/\n    larges, huges = [], []\n    for file in files:\n        size = os.path.getsize(file)\n        if size > (100 * 1024 * 1024):\n            larges.append(file)\n        elif size > (2 * 1024 * 1024 * 1024):\n            huges.append(file)\n\n    # Raise Error if a file is >2GB\n    if huges:\n        raise Error(_(\"These files are too large to be submitted:\\n{}\\n\"\n                      \"Remove these files from your directory \"\n                      \"and then re-run {}!\").format(\"\\n\".join(huges), org))\n\n    # Add large files (>100MB) with git-lfs\n    if larges:\n        # Raise Error if git-lfs not installed\n        if not shutil.which(\"git-lfs\"):\n            raise Error(_(\"These files are too large to be submitted:\\n{}\\n\"\n                          \"Install git-lfs (or remove these files from your directory) \"\n                          \"and then re-run!\").format(\"\\n\".join(larges)))\n\n        # Install git-lfs for this repo\n        _run(git(\"lfs install --local\"))\n\n        # For pre-push hook\n        _run(git(\"config credential.helper cache\"))\n\n        # Rm previously added file, have lfs track file, add file again\n        for large in larges:\n            _run(git(\"rm --cached {}\".format(shlex.quote(large))))\n            _run(git(\"lfs track {}\".format(shlex.quote(large))))\n            _run(git(\"add {}\".format(shlex.quote(large))))\n        _run(git(\"add --force .gitattributes\"))", "relevance": 1}
{"query": "write csv", "function": "def write_csvs(self,\n                   asset_map,\n                   show_progress=False,\n                   invalid_data_behavior='warn'):\n        \"\"\"Read CSVs as DataFrames from our asset map.\n\n        Parameters\n        ----------\n        asset_map : dict[int -> str]\n            A mapping from asset id to file path with the CSV data for that\n            asset\n        show_progress : bool\n            Whether or not to show a progress bar while writing.\n        invalid_data_behavior : {'warn', 'raise', 'ignore'}\n            What to do when data is encountered that is outside the range of\n            a uint32.\n        \"\"\"\n        read = partial(\n            read_csv,\n            parse_dates=['day'],\n            index_col='day',\n            dtype=self._csv_dtypes,\n        )\n        return self.write(\n            ((asset, read(path)) for asset, path in iteritems(asset_map)),\n            assets=viewkeys(asset_map),\n            show_progress=show_progress,\n            invalid_data_behavior=invalid_data_behavior,\n        )", "relevance": 3}
{"query": "write csv", "function": "def write_csv_header(mol, csv_writer):\n    \"\"\"\n\tWrite the csv header\n\t\"\"\"\n\n    # create line list where line elements for writing will be stored\n    line = []\n\n    # ID\n    line.append('id')\n\n    # status\n    line.append('status')\n\n    # query labels\n    queryList = mol.properties.keys()\n    for queryLabel in queryList:\n        line.append(queryLabel)\n\n    # write line\n    csv_writer.writerow(line)", "relevance": 2}
{"query": "write csv", "function": "def expand(self, datasets):\n    import json\n\n    tf_graph_predictions, errors = datasets\n\n    if self._output_format == 'json':\n      (tf_graph_predictions |\n       'Write Raw JSON' >>\n       beam.io.textio.WriteToText(os.path.join(self._output_dir, 'predictions'),\n                                  file_name_suffix='.json',\n                                  coder=RawJsonCoder(),\n                                  shard_name_template=self._shard_name_template))\n    elif self._output_format == 'csv':\n      # make a csv header file\n      header = [col['name'] for col in self._schema]\n      csv_coder = CSVCoder(header)\n      (tf_graph_predictions.pipeline |\n       'Make CSV Header' >>\n       beam.Create([json.dumps(self._schema, indent=2)]) |\n       'Write CSV Schema File' >>\n       beam.io.textio.WriteToText(os.path.join(self._output_dir, 'csv_schema'),\n                                  file_name_suffix='.json',\n                                  shard_name_template=''))\n\n      # Write the csv predictions\n      (tf_graph_predictions |\n       'Write CSV' >>\n       beam.io.textio.WriteToText(os.path.join(self._output_dir, 'predictions'),\n                                  file_name_suffix='.csv',\n                                  coder=csv_coder,\n                                  shard_name_template=self._shard_name_template))\n    else:\n      raise ValueError('FormatAndSave: unknown format %s', self._output_format)\n\n    # Write the errors to a text file.\n    (errors |\n     'Write Errors' >>\n     beam.io.textio.WriteToText(os.path.join(self._output_dir, 'errors'),\n                                file_name_suffix='.txt',\n                                shard_name_template=self._shard_name_template))", "relevance": 1}
{"query": "binomial distribution", "function": "def binomial_prefactor(s,ia,ib,xpa,xpb):\n    \"\"\"\n    The integral prefactor containing the binomial coefficients from Augspurger and Dykstra.\n    >>> binomial_prefactor(0,0,0,0,0)\n    1\n    \"\"\"\n    total= 0\n    for t in range(s+1):\n        if s-ia <= t <= ib:\n            total +=  binomial(ia,s-t)*binomial(ib,t)* \\\n                     pow(xpa,ia-s+t)*pow(xpb,ib-t)\n    return total", "relevance": 3}
{"query": "binomial distribution", "function": "def get_prob(self):\n        if self.total < 5:\n            return 0.\n        a, b = self.dup + 1, self.nodup + 1\n        n = a + b\n        p = a / n\n        q = b / n\n        # Lower edge of the 95% confidence interval, binomial distribution\n        return p - 1.96 * math.sqrt(p * q / n)", "relevance": 2}
{"query": "binomial distribution", "function": "def combineIndepDstns(*distributions):\n    '''\n    Given n lists (or tuples) whose elements represent n independent, discrete\n    probability spaces (probabilities and values), construct a joint pmf over\n    all combinations of these independent points.  Can take multivariate discrete\n    distributions as inputs.\n\n    Parameters\n    ----------\n    distributions : [np.array]\n        Arbitrary number of distributions (pmfs).  Each pmf is a list or tuple.\n        For each pmf, the first vector is probabilities and all subsequent vectors\n        are values.  For each pmf, this should be true:\n        len(X_pmf[0]) == len(X_pmf[j]) for j in range(1,len(distributions))\n\n    Returns\n    -------\n    List of arrays, consisting of:\n\n    P_out: np.array\n        Probability associated with each point in X_out.\n\n    X_out: np.array (as many as in *distributions)\n        Discrete points for the joint discrete probability mass function.\n\n    Written by Nathan Palmer\n    Latest update: 5 July August 2017 by Matthew N White\n    '''\n    # Very quick and incomplete parameter check:\n    for dist in distributions:\n        assert len(dist[0]) == len(dist[-1]), \"len(dist[0]) != len(dist[-1])\"\n\n    # Get information on the distributions\n    dist_lengths = ()\n    dist_dims = ()\n    for dist in distributions:\n        dist_lengths += (len(dist[0]),)\n        dist_dims += (len(dist)-1,)\n    number_of_distributions = len(distributions)\n\n    # Initialize lists we will use\n    X_out  = []\n    P_temp = []\n\n    # Now loop through the distributions, tiling and flattening as necessary.\n    for dd,dist in enumerate(distributions):\n\n        # The shape we want before we tile\n        dist_newshape = (1,) * dd + (len(dist[0]),) + \\\n                        (1,) * (number_of_distributions - dd)\n\n        # The tiling we want to do\n        dist_tiles    = dist_lengths[:dd] + (1,) + dist_lengths[dd+1:]\n\n        # Now we are ready to tile.\n        # We don't use the np.meshgrid commands, because they do not\n        # easily support non-symmetric grids.\n\n        # First deal with probabilities\n        Pmesh  = np.tile(dist[0].reshape(dist_newshape),dist_tiles) # Tiling\n        flatP  = Pmesh.ravel() # Flatten the tiled arrays\n        P_temp += [flatP,] #Add the flattened arrays to the output lists\n\n        # Then loop through each value variable\n        for n in range(1,dist_dims[dd]+1):\n            Xmesh  = np.tile(dist[n].reshape(dist_newshape),dist_tiles)\n            flatX  = Xmesh.ravel()\n            X_out  += [flatX,]\n\n    # We're done getting the flattened X_out arrays we wanted.\n    # However, we have a bunch of flattened P_temp arrays, and just want one\n    # probability array. So get the probability array, P_out, here.\n    P_out = np.prod(np.array(P_temp),axis=0)\n\n    assert np.isclose(np.sum(P_out),1),'Probabilities do not sum to 1!'\n    return [P_out,] + X_out", "relevance": 1}
