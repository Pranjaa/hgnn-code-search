Trains a k-nearest neighbors classifier for face recognition .
faces in given image using a trained KNN classifier
Shows the face recognition results visually .
Convert a dlib 'rect ' object to a plain tuple in ( top , right , bottom , left ) order
left ) order is within the bounds of the image .
Given a list of face encodings , compare them to a known face encoding and get a euclidean distance
Loads an image file (.jpg, .png, etc) into a numpy array
bounding boxes of human faces in a image
bounding boxes of human faces in a image using the cnn face detector
Given an image , returns a dict of face feature locations ( eyes , nose , etc ) for each face in the image
Given an image , return the 128-dimension face encoding for each face in the image .
Parses the given data type string to a : class : ` DataType ` .
Return the Catalyst datatype from the size of integers .
Infer the DataType from obj
Infer the schema from dict/namedtuple/object
is NullType in ` dt ` or not
Create a converter to drop the names of fields in obj
Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
Convert Spark data type to pyarrow type
Convert a schema from Spark to Arrow
Convert pyarrow type to Spark data type.
Convert schema from Arrow to Spark.
Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.
Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone
Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
Convert timestamp to timezone-naive in the specified timezone or local timezone
adding new elements to it to define the schema .
's heavy used in ` toInternal ` .
Return as an dict
Gets summary (e.g.
Evaluates the model on a test dataset .
Get all the directories
Choose one directory for spill by number n
Combine the items by creator and combiner
Merge (K,V) pair by mergeCombiner
partitioned data into disks .
merged items as iterator
partitioned items as iterator
merge the partitioned items and return the as iterator
do external sort when the memory
dump the values into disk
load a partition from disk, then sort and group by key
Called by a worker process after the fork ( ) .
returns consistent hash code for builtin types , especially
Parse a memory string in the format supported by Java ( e.g .
make it works
Persist this RDD with the default storage level (C{MEMORY_ONLY}).
persist its values across operations
remove all blocks for it from
Gets the name of the file to which this RDD was checkpointed
Return a new RDD by applying a function to each element of this RDD .
Return a new RDD by first applying a function to all elements of this
Return a new RDD by applying a function to each partition of this RDD .
Deprecated : use mapPartitionsWithIndex instead .
Return a new RDD containing the distinct elements in this RDD .
Return a sampled subset of this RDD .
splits this RDD with the provided weights .
Return a fixed-size sampled subset of this RDD .
sampling rate that guarantees a sample of
Return the union of this RDD and another one .
Return the intersection of this RDD and another one .
according to the given partitioner and , within each resulting partition ,
is assumed to consist of ( key , value ) pairs .
Sorts this RDD by the given keyfunc
Return the Cartesian product of this RDD and another one , that is , the
Return an RDD of grouped items.
created by piping elements to a forked external process .
Applies a function to all elements of this RDD.
Applies a function to each partition of this RDD.
Return a list that contains all of the elements in this RDD .
Reduces the elements of this RDD using the specified commutative and
Reduces the elements of this RDD in a multi-level tree pattern .
Aggregate the elements of each partition , and then the results for all
Aggregates the elements of this RDD in a multi-level tree
Find the maximum item in this RDD.
Find the minimum item in this RDD.
Add up the elements in this RDD.
Return a L { StatCounter } object that captures the mean , variance
Compute a histogram using the provided buckets .
Return the count of each unique value in this RDD as a dictionary of
Get the top N elements from an RDD .
Get the N elements from an RDD ordered in ascending order or as
Take the first num elements of the RDD .
Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
Save this RDD as a SequenceFile of serialized objects .
Save this RDD as a text file , using string representations of elements .
Merge the values for each key using an associative and commutative reduce function .
Merge the values for each key using an associative and commutative reduce function , but
Return a copy of the RDD partitioned using the specified partitioner .
combine the elements for each key using a custom
Aggregate the values of each key , using given combine functions and a neutral
Merge the values for each key using an associative function `` func ''
Group the values for each key in the RDD into a single sequence.
Pass each value in the key-value pair RDD through a flatMap function
Pass each value in the key-value pair RDD through a map function
Return a subset of this RDD sampled by key ( via stratified sampling ) .
has no pair with matching
is not contained in C { other } .
Return a new RDD that is reduced into ` numPartitions ` partitions .
returning key-value pairs with the
Zips this RDD with its element indices.
Zips this RDD with generated unique Long ids.
Get the RDD 's current storage level .
Returns the default number of partitions to use during reduce tasks ( e.g. , groupBy ) .
Return the list of values in the RDD for key ` key ` .
Return a JavaRDD of Object by unpickling
.. note:: Experimental
contains all of the elements in this RDD .
Convert a list of Column ( or names ) into a JVM Seq of Column .
Convert a list of Column ( or names ) into a JVM ( Scala ) List of Column .
Create a method for given unary operator
Create a method for given binary operator
Create a method for binary operator ( this object is on right side )
Return a : class : ` Column ` which is a substring of the column .
is evaluated to true if the value of this
aliased with a new name or names ( in the case of expressions that
Convert the column into type `` dataType `` .
Evaluates a list of conditions and returns one of multiple possible result expressions .
Define a windowing column .
Applies transformation on a vector or an RDD[Vector].
Computes the mean and variance and stores as a model to be used
Returns a ChiSquared feature selector.
Computes a [ [ PCAModel ] ] that contains the principal components of the input vectors .
Transforms the input document ( list of terms ) to term frequency
Computes the inverse document frequency .
Find synonyms of a word
Load a model from the given path .
Computes the Hadamard product of the vector .
Predict values for a single data point or an RDD of points using
Train a decision tree model for classification .
Train a decision tree model for regression .
Train a random forest model for binary or multiclass
Train a random forest model for regression .
Train a gradient-boosted trees model for classification .
Set a configuration property .
Set a configuration property , if not already set .
be passed to executors .
passed as a list of key-value pairs .
Get the configured value for some key , or return a default otherwise .
Get all values as a list of key-value pairs .
Does this configuration contain a given key ?
Returns a printable version of the configuration, as a list of
Returns a list of databases available across all sessions.
Returns a list of tables/views in the specified database.
registered in the specified database .
given table/view in the specified database .
Creates a table based on the dataset in a data source .
given socket , this is a blocking method thus only return when the socket
need to make sure; get or create global BarrierTaskContext .
Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called
annotates a function to append the version of Spark the function was added .
Returns a function with same code, globals, defaults, closure, and
forces keyword arguments in the wrapped method
Generates the header part for shared variables
shared param class .
Runs the bisecting k-means algorithm return the model .
Train a k-means clustering model .
Train a Gaussian Mixture clustering model .
r"""
Update the centroids , according to data
Set number of batches after which the centroids of that
be set before calling trainOn .
be random samples from
Train the model on the incoming dstream .
Make predictions on a dstream.
Make predictions on a keyed dstream.
Return the topics described by weighted terms .
Load the LDAModel from disk.
Train a LDA model .
object into Java
Call Java Function
Call API in PythonMLLibAPI
makes a class inherit documentation from its parents .
Call method of java_model
Return a new DStream in which each RDD has a single element
Return a new DStream containing only the elements that satisfy predicate .
Return a new DStream by applying a function to each element of DStream .
Return a new DStream in which each RDD is generated by applying
Return a new DStream by applying reduceByKey to each RDD .
Return a new DStream by applying combineByKey to each RDD .
Return a copy of the DStream in which each RDD are partitioned
Apply a function to each RDD in this DStream .
Print the first num elements of each RDD generated in this DStream .
Persist the RDDs of this DStream with the given storage level
Enable periodic checkpointing of RDDs of this DStream
Return a new DStream by applying groupByKey on each RDD .
Return a new DStream in which each RDD contains the counts of each
Save each RDD in this DStream as at text file, using string
Return a new DStream in which each RDD is generated by applying a function
Return a new DStream by unifying data of another DStream with this DStream .
Return a new DStream by applying 'cogroup ' between RDDs of this
unix_timestamp into Time
'end ' ( both included )
Return a new DStream in which each RDD contains all the elements in seen in a
Return a new DStream in which each RDD has a single element generated by reducing all
Return a new DStream in which each RDD has a single element generated
Return a new DStream in which each RDD contains the count of distinct elements in
Return a new DStream by applying ` groupByKey ` over a sliding window .
Return a new DStream by applying incremental ` reduceByKey ` over a sliding window .
Return a new `` state '' DStream where the state for each key is updated by applying
setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol="items", \
setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000, \
Return a CallSite representing the first Spark call in the current call stack .
Parse a line of text into an MLlib LabeledPoint object .
given label ( category ) if specified .
converting Spark SQL records to Pandas DataFrame , the inferred data type may be wrong .
Returns the content as an : class : ` pyspark.RDD ` of : class : ` Row ` .
DataFrame ` into a : class : ` RDD ` of string .
Returns the schema of this : class : ` DataFrame ` as a : class : ` pyspark.sql.types.StructType ` .
plans to the console for debugging purpose .
Return a new : class : ` DataFrame ` containing rows in this : class : ` DataFrame ` but
Prints the first ``n`` rows to the console.
enabled eager evaluation
Checkpointing can be used to truncate the
Checkpointing can be used to; checkpointed version of this Dataset .
tracks a point
Specifies some hint on the current DataFrame.
Returns all the records as a list of :class:`Row`.
contains all of the rows in this : class : ` DataFrame ` .
Limits the result count to the number specified .
Sets the storage level to persist the contents of the : class : ` DataFrame ` across
Get the : class : ` DataFrame ` 's current storage level .
DataFrame ` that has exactly ` numPartitions ` partitions .
partitioned by the given partitioning expressions .
Returns a sampled subset of this :class:`DataFrame`.
based on the
splits this : class : ` DataFrame ` with the provided weights .
Returns all column names and their data types as a list.
column based on the column name specified as a regex and returns it
DataFrame ` with an alias set .
Returns the cartesian product with another : class : ` DataFrame ` .
using the given join expression .
DataFrame ` with each partition sorted by the specified column ( s ) .
Return a JVM Seq of Columns from a list of Column or names
Return a JVM Seq of Columns from a list of Column or column names
Return a JVM Seq of Columns that describes the sort order
string columns .
specified statistics for numeric and string columns .
Returns the first ``n`` rows.
returns a new : class : ` DataFrame ` .
rows using the given condition .
using the specified columns ,
Return a new : class : ` DataFrame ` containing union of rows in this and another frame .
containing union of rows in this and another frame .
Return a new : class : ` DataFrame ` containing rows only in
Return a new : class : ` DataFrame ` containing rows in both this dataframe and other
Return a new : class : ` DataFrame ` containing rows in this frame
Return a new : class : ` DataFrame ` with duplicate rows removed ,
omitting rows with null values .
Replace null values, alias for ``na.fill()``.
replacing a value with another value .
Calculates the approximate quantiles of numerical columns of a
Calculates the correlation of two columns of a DataFrame as a double value .
Calculate the sample covariance for the given columns , specified by their names , as a
known as a contingency; Computes a pair-wise frequency table of the given columns .
Finding frequent items for columns , possibly with false positives .
adding a column or replacing the
renaming an existing column .
DataFrame ` that drops the specified column .
DataFrame ` that with new specified column names
chaining custom transformations .
Returns the contents of this : class : ` DataFrame ` as Pandas `` pandas.DataFrame `` .
Returns all records as a list of ArrowRecordBatches, pyarrow must be installed
Returns the : class : ` StatCounter ` members as a `` dict `` .
wrapped expression infos by name
returns a formatted string if ` usage `
returns a formatted string if ` arguments `
returns a formatted string if ` examples `
returns a formatted string if ` note ` is not
returns a formatted string if ` deprecated `
Generates a markdown file after listing the function information .
Predict values for a single data point or an RDD of points
Save this model to the given path .
Train a logistic regression model on the given data .
Train a Naive Bayes model given an RDD of ( label , features )
maintaining the heap invariant .
return the current smallest value , and add the new item .
followed by a heappop .
len ( x ) ) time .
Maxheap version of a heappop.
followed by a heappush .
Maxheap variant of _siftdown
Maxheap variant of _siftup
sorted inputs into a single sorted output .
Find the n smallest elements in a dataset.
Find the n largest elements in a dataset.
Compute the correlation matrix with specified method using dataset .
Given a list of metrics , provides a builder that it turns computes metrics from a column .
contains the summary of the column with the requested
Compute the correlation ( matrix ) for the input RDD ( s ) using the
Creates a list of callables which can be called from different threads to fit and evaluate
Sets the given parameters in this grid to fixed values .
Builds and returns all combinations of parameters specified
Return Python estimator, estimatorParamMaps, and evaluator from a Java ValidatorParams.
Return Java estimator, estimatorParamMaps, and evaluator from this Python instance.
Given a Java CrossValidator , create and return a Python wrapper of it .
Used for ML persistence .; Transfer this instance to a Java CrossValidator .
Creates a copy of this instance with a randomly generated uid
setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\
Given a Java TrainValidationSplit , create and return a Python wrapper of it .
Used for ML persistence .; Transfer this instance to a Java TrainValidationSplit .
Given a Java TrainValidationSplitModel , create and return a Python wrapper of it .
Used for ML persistence .; Transfer this instance to a Java TrainValidationSplitModel .
Returns the value of Spark runtime configuration property for the given key ,
is of type str .
Create a PySpark function by its name
creates a PySpark function that takes a column
print out deprecation warnings
Create a binary mathfunction by name
Create a window function by name
returns a new : class : ` Column ` for approximate distinct count of
Marks a DataFrame as small enough for use in broadcast joins.
Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.
returns the last value in a group .
col1 if it is not NaN , or col2 if col1 is NaN .
Generates a random column with independent and identically distributed ( i.i.d . )
given value to ` scale ` decimal places using HALF_UP rounding mode if ` scale ` > = 0
Shift the given value numBits left .
Signed ) shift the given value numBits right .
Parses the expression string into the column that it represents
Returns the first argument-based logarithm of the second argument.
Convert a number in a string column from one base to another .
returns the value that is ` offset ` rows before the current row , and
returns the ntile group id ( from 1 to ` n ` inclusive )
specified by the date
Returns the date that is ` days ` days after ` start `
Returns the number of days from ` start ` to ` end ` .
Returns the date that is ` months ` months after ` start `
Returns number of months between dates date1 and date2.
Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or
truncated to the unit specified by the format .
is later than the value of the date column .
Returns the last day of the month which the given date belongs to .
string with given pattern ( 'yyyy-MM-dd HH : mm : ss ' , by default )
is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE .
Bucketize rows into one or more time windows given a timestamp specifying column .
Calculates the hash code of given columns , and returns the result as an int column .
string columns together into a single string column ,
Computes the first argument into a string from a binary using the provided character set
rounded to d decimal places
returns the result as a string column .
Locate the position of the first occurrence of substr column in the given string .
Substring starts at ` pos ` and is of length ` len ` when str is String type or
Returns the substring from string str before count occurrences of the delimiter delim .
Computes the Levenshtein distance of the two given strings .
Locate the position of the first occurrence of substr in a string column , after position pos .
width ` len ` with ` pad ` .
Repeats a string column n times , and returns it as a new string column .
str around matches of the given pattern .
Extract a specific group matched by a Java regex , from the specified string column .
specified string value that match regexp with rep .
A function translate any character in the `srcCol` by a character in `matching`.
contain any common non-null element ; if not ,
containing all the elements in ` x ` from index ` start `
are replaced with; Concatenates the elements of ` column ` using the ` delimiter ` .
Concatenates multiple input columns together into a single column.
Locates the position of the first occurrence of the given value
given index in extraction if col is array .
Remove all elements that equal to element from the given array .
given array or map .
json object from a json string based on json path specified , and returns json string
Creates a new row for a json column according to the given field names .
Parses a column containing a JSON string into a : class : ` MapType ` with : class : ` StringType `
Parses a JSON string and infers its schema in DDL format .
Parses a CSV string and infers its schema in DDL format .
containing a : class : ` StructType ` into a CSV string .
returns the length of the array or map stored in the column .
sorts the input array in ascending or descending order according
creates an array containing a column repeated count times .
Returns the union of all the given maps .
Generate a sequence of integers from ` start ` to ` stop ` , incrementing by ` step ` .
Parses a column containing a CSV string to a row with the specified schema .
Creates a user defined function ( UDF ) .
Creates a vectorized user defined function ( UDF ) .
converts bool values to lower case strings .
named options ( filter out those the value is None )
Specifies the input data source format.
Specifies the input schema.
Adds an input option for the underlying data source .
Adds input options for the underlying data source.
returns it as a : class ` DataFrame ` .
returns the results as a : class : ` DataFrame ` .
returning the result as a : class : ` DataFrame ` .
returns a : class : ` DataFrame ` whose schema starts with a
Loads a CSV file and returns the result as a : class : ` DataFrame ` .
DataFrame ` representing the database table named `` table ``
Specifies the behavior when data or table already exists.
underlying output data source .
Adds an output option for the underlying data source .
Adds output options for the underlying data source.
given columns on the file system .
Sorts the output in each bucket by the given columns on the file system .
Saves the contents of the : class : ` DataFrame ` to a data source .
Inserts the content of the : class : ` DataFrame ` to the specified table .
Saves the content of the : class : ` DataFrame ` as the specified table .
Saves the content of the : class : ` DataFrame ` in JSON format
Saves the content of the : class : ` DataFrame ` in Parquet format at the specified path .
Saves the content of the DataFrame in a text file at the specified path .
Saves the content of the : class : ` DataFrame ` in CSV format at the specified path .
Saves the content of the : class : ` DataFrame ` in ORC format at the specified path .
Saves the content of the : class : ` DataFrame ` to an external database table via JDBC .
pulls messages from a Kinesis stream .
Prompt the user to choose who to assign the issue to in jira , given a list of candidates ,
Standardize the [ SPARK-XXXXX ] [ MODULE ] prefix
Parses a line in LIBSVM format into ( label , indices , values ) .
Converts a LabeledPoint to a string in LIBSVM format.
labeled data in the LIBSVM format into an RDD of
labeled data in LIBSVM format .
labeled points saved using RDD.saveAsTextFile .
Returns a new vector with `1.0` (bias) appended to
Converts vector columns in an input DataFrame from the
added is more .
Generate an RDD of LabeledPoints.
Train a linear regression model using Stochastic Gradient
Predict labels for provided features.
Save an IsotonicRegressionModel .
Load an IsotonicRegressionModel.
Train an isotonic regression model on the given data .
Compute similarities between columns of this matrix.
Compute the QR decomposition of this RowMatrix .
Computes the singular value decomposition of the RowMatrix .
Multiply this matrix by a local dense matrix on the right.
are the left
stored as an RDD of IndexedRows .
Convert this matrix to a BlockMatrix.
stored as an RDD of
The RDD of sub-matrix blocks
Persists the underlying RDD with the specified storage level .
Adds two block matrices together.
Transpose this BlockMatrix .
Returns the size of the vector .
string representation back into the DenseVector .
Compute the dot product of two Vectors .
Squared distance of two Vectors.
string representation back into the SparseVector .
Dot product with a SparseVector or 1- or 2-dimensional Numpy array.
Squared distance from a SparseVector or 1-dimensional NumPy array .
Returns a copy of this SparseVector as a 1-dimensional NumPy array.
Convert this vector to the new mllib-local representation.
Create a dense vector of 64-bit floats from a Python list or numbers .
Convert a vector from the new mllib-local representation .
Squared distance between two vectors .
Parse a string representation back into the Vector .
Check equality between sparse/dense vectors,
attributes which are array-like or buffer to array .
Return an numpy.ndarray
Convert to SparseMatrix
Convert this matrix to the new mllib-local representation.
Create a SparseMatrix
Convert a matrix from the new mllib-local representation .
Given a large dataset and an item , approximately find at most k items which have the
find all pairs of rows whose distance are smaller than
Construct the model directly from an array of label strings ,
Construct the model directly from an array of array of label strings ,
setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false, \
Loads the default stop words for the given language .
Find "num" number of words closest in similarity to "word".
capture some SQL exceptions in Java .
java type array
is not installed
launch jvm gateway
defined by the SocketAuthHelper class on the given
return a ( sockfile , sock ) for that connection .
is needed if the Java
Find the SPARK_HOME.
Calculates URL contributions to the rank of other URLs.
trained on the
Returns the image schema .
Returns the OpenCV type mapping supported .
Returns the schema for the image column .
Returns field names of image columns.
Returns the name of undefined image type for the invalid image .
Converts an image to an array with metadata.
Converts an array with metadata to a two-dimensional image.
Reads the directory of images from the local or remote source .
given Java classname and arguments
Create a Java array of given java_class type .
>>> _convert_epytext("L{A}")
string prefix-time ( .suffix )
Add a profiler for RDD ` id `
Dump the profile stats into directory ` path `
Print the profile stats to stdout
Print the profile stats to stdout , id is the RDD id
Dump the profile into path , id is the RDD id
profiles the method to_profile passed in .; is returned .
Get the existing SQLContext or create a new one with given SparkContext .
Sets the given Spark SQL configuration property .
Returns the value of Spark SQL configuration property for the given key .
Create a : class : ` DataFrame ` with single : class : ` pyspark.sql.types.LongType ` column named
An alias for :func:`spark.udf.register`.
An alias for :func:`spark.udf.registerJavaFunction`.
Creates a : class : ` DataFrame ` from an : class : ` RDD ` , a list or a : class : ` pandas.DataFrame ` .
Creates an external table based on the dataset in a data source .
containing names of tables in the given database .
Returns a list of names of tables in the database ``dbName``.
allows managing all the
corresponding catalyst value .
Get the absolute path of a file added through C { SparkContext.addFile ( ) } .
Get the root directory that contains files added through
Given a Java OneVsRestModel , create and return a Python wrapper of it .
Used for ML persistence .; Transfer this instance to a Java OneVsRestModel .
Return the message from an exception as either a str or unicode object .
Get argspec of a function.
fail on 'StopIteration ' by raising a 'RuntimeError '
Given a Spark version string , return the ( major version number , minor version number ) .
is initialized or not .
instantiate a SparkContext and register it as a singleton object .
Set a Java system property , such as spark.executor.memory .
Shut down the SparkContext.
Create a new RDD of int containing elements from ` start ` to ` end `
Distribute a local Python collection to form an RDD .
Using py4j to send a large dataset to the jvm is really slow , so we use either a file
saved using L { RDD.saveAsPickleFile } method .
Read a text file from HDFS , a local file system ( available on all
Read a directory of text files from HDFS , a local file system
Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS ,
Read a 'new API ' Hadoop InputFormat with arbitrary key and value class from HDFS ,
Build the union of a list of RDDs .
given initial value , using a given
Add a file to be downloaded with this Spark job on every node .
Add a .py or .zip dependency for all tasks to be executed on this
based on a pyspark.StorageLevel .
started by this thread until the group ID is set to a
Executes the given partitionFunc on the specified set of partitions ,
Train a matrix factorization model given an RDD of ratings by users
contains frequent itemsets .
Finds the complete set of frequent sequential patterns in the
be a RDD
Estimate the probability density at points
Start a TCP server to receive accumulator updates in a daemon thread , and returns it
Adds a term to this accumulator 's value
returns the result as a : class : ` DataFrame ` .
Pivots a column of the current : class : ` DataFrame ` and perform the specified aggregation .
using a pandas udf and returns the result
Creates a : class : ` WindowSpec ` with the partitioning defined .
Creates a : class : ` WindowSpec ` with the frame boundaries defined ,
Defines the frame boundaries , from ` start ` ( inclusive ) to ` end ` ( inclusive ) .
Generates an RDD comprised of i.i.d .
Generates an RDD comprised of vectors containing i.i.d .
Returns the active SparkSession for the current thread , returned by the builder .
Runtime configuration interface for Spark.
create , drop , alter or query underlying
Infer schema from list of Row or tuple.
Infer schema from an RDD of Row or tuple.
existing RDD , returns the RDD and schema .
Create an RDD for DataFrame from a list or pandas.DataFrame, returns
Used when converting a pandas.DataFrame to Spark using to_records ( ) , this will correct
Convert a pandas.DataFrame to list of records that can be used to make a DataFrame
Create a DataFrame from a given pandas.DataFrame by slicing it into partitions , converting
is called from shell.py; Initialize a SparkSession for a pyspark shell session .
DataFrame ` representing the result of the given query .
Returns the specified table as a : class : ` DataFrame ` .
Stop the underlying : class : ` SparkContext ` .
object , or None if the job info
object , or None if the stage
Restore an object of namedtuple
generated by namedtuple picklable
make it picklable
Load a stream of un-ordered Arrow RecordBatches , where the last iteration yields
given pandas.Series or list of Series ,
is a single series or
Deserialize ArrowRecordBatches to an Arrow table and return as a list of pandas.Series .
require a START_ARROW_STREAM before the Arrow stream is sent .
Waits for the termination of `this` query, either by :func:`query.stop()` or by an
updates for this query .
Returns the most recent : class : ` StreamingQueryProgress ` update of this streaming query or
associated SQLContext has terminated since the
Loads a data stream from a data source and returns it as a : class ` DataFrame ` .
Loads a JSON file stream and returns the results as a : class : ` DataFrame ` .
Loads a ORC file stream , returning the result as a : class : ` DataFrame ` .
Loads a Parquet file stream , returning the result as a : class : ` DataFrame ` .
Loads a text file stream and returns a : class : ` DataFrame ` whose schema starts with a
Loads a CSV file stream and returns the result as a : class : ` DataFrame ` .
is written to a streaming sink .
Specifies the name of the : class : ` StreamingQuery ` that can be started with
is not set it will run the query as fast
Sets the output of the streaming query to be processed using the provided writer `` f `` .
Sets the output of the streaming query to be processed using the provided
DataFrame ` to a data source .
Get the Python compiler to emit LOAD_FAST ( arg ) ; STORE_DEREF
is a Tornado coroutine function .
streamed into file
allocated in memory
Fills in the rest of function data into the skeleton function object
Put attributes from ` class_dict ` back on ` skeleton_class ` .
is special module that can not be imported by its
Save a code object
Registered with the dispatch to handle all function types .
Save a class that ca n't be stored as module global .
Pickles an actual func object.
Save a `` global '' .
Based off pickle.save_inst; logic to save instance .
needed for namedtuple support )
Copy the current param to a new parent , must be a dummy param .
Convert a value to a list , if possible .
Convert a value to list of floats , if possible .
Convert a value to list of ints , if possible .
Convert a value to list of strings , if possible .
Convert a value to a MLlib Vector , if possible .
Convert a value to a string , if possible .
defined on the class to current object .
ordered by name .
Explains a single param and returns its name , doc , and optional
Gets a param by its name .
is explicitly set by user .
has a default value .
contains a param with a given
Gets the value of a param in the user-supplied param map or its
Extracts the embedded default param values and user-supplied
Creates a copy of this instance with the same uid and some
Sets a parameter in the embedded param map.
belongs to this Params instance .
Resolves a param and validates the ownership .
Sets user-supplied params.
default params .
param values from this instance to another instance for
Changes the uid of this instance.
Return an JavaRDD of Object by unpickling
Return the broadcasted value
cached copies of this broadcast on the executors .
related to this broadcast variable .
docstring from func
including lambda function ) or a user-defined function
Register a Java user-defined function as a SQL function.
Register a Java user-defined aggregate function as a SQL function.
recreate a StreamingContext from checkpoint data or create a new StreamingContext .
is a context started
started but not stopped ) ,; return the active StreamingContext ( i.e .
Wait for the execution to stop.
Stop the execution of the streams , with option of ensuring all
is received using
monitors a Hadoop-compatible file system
Create an input stream from a queue of RDDs or list.
Create a new DStream in which each RDD is generated by applying
Create a unified DStream from multiple DStreams of the same
Add a [ [ org.apache.spark.streaming.scheduler.StreamingListener ] ] object for
Load tf checkpoints in a pytorch model
Constructs a ` GPT2Config ` from a json file of parameters .
Save this instance to a json file .
Initialize the weights .
Instantiate a GPT2PreTrainedModel from a pre-trained model file or a pytorch state dict .
Loads a data file into a list of ` InputFeature ` s .
Read a list of ` InputExample ` s from an input file .
Read a SQuAD json file into a list of SquadExample .
Loads a data file into a list of ` InputBatch ` s .
tokenized answer spans that better match the annotated answer .
Check if this is the 'max context ' doc span for the token .
Write final predictions to the json file and log-odds of null if needed.
Project the tokenized prediction back to the original text .
Get the n-best logits from a list .
Compute softmax probability over raw logits.
Reads a tab separated value file .
See base class .
Creates examples for the training and dev sets.
Masks everything but the k top entries as -infinity (1e10).
Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict .
tf pre-trained weights in a pytorch model ( from NumPy arrays here )
Constructs a ` OpenAIGPTConfig ` from a Python dictionary of parameters .
embedding matrice if needed
Update input and output embeddings with new embedding matrice
Performs a single optimization step.
Runs basic whitespace cleaning and splitting on a piece of text.
is a punctuation character .
using the vocab .
Save the tokenizer vocabulary to a directory or file .
Instantiate a PreTrainedBertModel from a pre-trained model file .
Tokenizes a piece of text .
Strips accents from a piece of text.
Adds whitespace around any CJK character.
is the codepoint of a CJK character .
Tokenizes a piece of text into its word pieces .
Output a list of tuples(story, 1st continuation, 2nd continuation, label)
containing lists of tuples ( story , 1st continuation , 2nd continuation , label )
Masking some random tokens for Language Model task with probabilities as in the original BERT paper .
Convert a raw sample ( pair of sentences as tokenized strings ) into a proper training sample with
are two subsequent sentences; Get one sample from corpus consisting of two sentences .
Get one sample from corpus consisting of a pair of two subsequent lines from the same doc .
Get random line from another document for nextSentence task.
starts over when reaching end of file
is mostly copied from the Google BERT repo , but; Creates the predictions for the masked LM objective .
is mostly a duplicate of the equivalent function from Google BERT 's repo .
embedding: an nn.Embedding layer
Params:
Computes log probabilities for all : math : ` n\_classes `
labels: [b1, b2]
A map of modules from TF to PyTorch.
Instantiate a TransfoXLPreTrainedModel from a pre-trained model file or a pytorch state dict .
Run this to be sure output and input ( adaptive ) softmax weights are tied
object from string or tuple representation
associated with rule name
Infer the most likely frequency given the input index .
string to describe the inferred
load a pickle, with a provided encoding
is called upon unpickling , rather than the default which does n't
Construct an index from sequences of data.
have an index from some index-like object .
Trims zeros and decimal points.
require that we have a dtype compat for the values .
Create a new Index inferring the class with passed value , do n't copy
check like `` is `` but that works through views .
handle NA filling of take .
Return the formatted data as a unicode string .
Render a string representation of the Index .
specified values of ` self ` and return them .
Actually format specific types of the index.
Return a summarized representation .
Create a Series with both index and values equal to the index keys
Create a DataFrame with a column containing the Index .
Handles the quirks of having a singular 'name ' parameter for general
has to be a hashable type .
Set Index or MultiIndex name.
Alter Index or MultiIndex name.
Validate index level.
For internal compatibility with with the Index API.
requested level ( s ) removed .
Return if each value is NaN .
duplicated index elements .
containing unique values .
be self ,
Form the union of two Index objects.
Form the intersection of two Index objects.
Return a new Index with elements from the index that are not in
Compute the symmetric difference of two Index objects .
works for monotonic decreasing
Get the indexer for the nearest index labels ; requires an index with
Parameters
Consistent invalid indexer message.
Create index with target's values (move/add/delete values
Create a new index with target 's values ( move/add/delete values as
* affects the level of the resulting
convert an array of data into an integer index .
Coerces data to ndarray.
need to coerce a scalar to a compat for our index type .
is valid for scalar op .
know ` name ` is a Python
Append a collection of Index options together .
Return a new Index of the values set with the mask .
Determine if two Index objects contain the same elements .
check that other comparable attributes are
Return the label from the index , or , if not present , the previous one .
Find the locations (indices) of the labels from the index for
Return a sorted copy of the index .
Return the integer indices that would sort the index .
Fast lookup of value from 1-dimensional ndarray.
Guaranteed return of an indexer even when non-unique.
given array of values .
using input correspondence ( a dict , Series , or function ) .
Return a boolean array where the index values are in ` values ` .
compute the slice indexer for input
have a float key and are not a floating index , then try to cast
are positional indexer , validate that we have appropriate
corresponds to given label .
Compute slice locations for input labels.
passed location ( -s ) deleted .
inserting new item at location .
Make new Index with passed list of labels deleted.
Add in comparison methods.
Add in the numeric add/sub methods to disable.
disable other than add/sub .
Validate if we can perform a numeric unary operation .
evaluate or raise TypeError if we are not of
Add in numeric methods.
Add in numeric unary methods.
Add in logical methods.
return a BaseGrouper , which is an internal
given an object and the specifications , setup the internal grouper
serialize ) object to file .
pickled pandas object ( or any object ) from file .
Return a masking array of same size/shape as arr
be 1-d , inputs
Passed off to scipy.interpolate.interp1d .; is scipy 's kind .
Convenience function for interpolate.BPoly.from_derivatives.
Convenience function for akima interpolation.
Perform an actual interpolation of values , values will be make 2-d if
Cast values to a dtype that algos.pad and algos.backfill can handle.
is a reversed op , then flip x , y
np.nan , regardless of the dtypes
caused by division by zero , casting to a diffferent dtype
Get indexers of values that won't be filled
[True, True, False, True, False], 2 ->
Return console size as tuple = (width, height).
're running in an interactive shell
ensure we can groupby for categoricals .
Reverse the codes_to_groupby to account for sort / observed .
return our implementation
Write a DataFrame to the parquet format .
Load a parquet object from the file path , returning a DataFrame .
using another array
Groupby iterator
dict {group name -> group indices}
Compute group sizes
dict {group name -> group labels}
Normalize semi-structured JSON data into a flat table .
Reshape long-format data to wide.
translate keys for
based selection .
provided name .
Try to cast the result to our obj original type ,
Parameters:
Shared func to call any / all Cython GroupBy implementations .
excluding missing values .
Compute group sizes.
Add numeric operations to the GroupBy generically.
resampling when using a TimeGrouper .
Return a rolling grouper , providing rolling functionality per group .
expanding grouper , providing expanding
Shared function for ` pad ` and ` backfill ` to call Cython method .
Take the nth row from each group if n is an int , or a subset of rows
given quantile , a la numpy.percentile .
Number each group from 0 to the number of groups - 1.
Number each item in each group from 0 to the length of that group - 1.
Provide the rank of values within each group .
Cumulative product for each group.
Cumulative min for each group.
Cumulative max for each group.
Get result for Cythonized functions.
Shift each group by periods observations.
Return first n rows of each group.
Return last n rows of each group.
falls on Saturday , use following Monday instead ;
For second holiday of two adjacent ones!
falls on Saturday or Sunday , use previous Friday instead .
falls on Sunday or Saturday ,
falls on Saturday , use day before ( Friday ) instead ;
used for observances
holidays observed between start date and end date
Get reference dates for the holiday.
Apply the given offset/observance to a DatetimeIndex of dates .
Returns a curve with holidays between start_date and end_date
calendars together .
Register an option in the package-wide pandas config object
deprecated , if code attempts to access this option ,
matching ` pat `
deprecated and a replacement key defined , will return the
Builds a formatted description of a registered option and prints it
contextmanager for multiple invocations of API with a common prefix
pairs from declarations
Create an array.
Try to do platform conversion , with special casing for IntervalArray .
Check if the object is a file-like object .
Check if the object is list-like .
Check if the object is list-like , and that all of its elements
Check if the object is dict-like .
Check if the object is a sequence of objects .
Return a fixed frequency DatetimeIndex .
Return a fixed frequency DatetimeIndex , with business day as the default
Return a fixed frequency DatetimeIndex , with CustomBusinessDay as the
conformed data .
Return a sliced object .
Return index as ndarrays.
Wrap a single result .
Wrap the results.
Center the result in the window.
return the window
moving window of type `` window_type `` on the data .
apply ; we are stripping all of the _apply kwargs and
Rolling statistical measure using supplied function .
Validate on is_monotonic.
Validate & return window frequency.
Get the window length over which to perform some operation .
Designed to be; Rolling statistical measure using supplied function .
weighted moving average .
weighted moving stddev .
weighted moving variance .
weighted sample covariance .
weighted sample correlation .
sure that time and panels are conformable .
Returns a multi-index suitable for a panel-like DataFrame.
Generate ND initialization; axes are passed
Construct Panel from dict of DataFrame objects.
Get my plane axes indexes : these are already
Write each DataFrame in Panel to a separate excel sheet.
retrieve single value at ( item , major , minor ) location .
set single value at ( item , major , minor ) location .
Unpickle the panel.
align with chosen axis pair .
Round each value in Panel to a specified number of decimal places.
holding passed axis constant .
selected axis .
stacked ) format as DataFrame whose
Apply function along axis (or axes) of the Panel.
iterating over the other axis .
Return the type for the ndim of the result .
Return number of observations over requested axis.
Shift index by desired number of periods with an optional time freq.
Join items with other Panel either on major and minor axes column.
using non-NA values from other Panel .
Return a list of the axis indices .
Return the slice dictionary for these axes .
Conform set of _constructor_sliced-like objects to either
gets the offsets into the hypothetical list
observed group ids
is intended to be a drop-in replacement for np.argsort which
return a diction of { labels } - > { indexers }
counting sort ` and it is at least
is offsets into cartesian product of all possible labels .
reorder corresponding `` labels `` .
prevent foot-shooting in a helpful way .
Run the engine on the expression
use for the given values and dtype .
return a new extended blocks , givin the result
be at least 1 d
have shape ` new_shape ` ,
Return a new ndarray , try to preserve dtype if possible .
ndim inference and validation.
have a astypeable to categorical ,
return an internal format , currently just the ndarray
Create a new block , with type inference propagate any values that are
given values in a block of same type as self .
return result as block .
Concatenate list of single blocks of the same type.
given loc ( -s ) from block in-place .
apply the function to my values ; return a block if we are not
fail , then convert to
split the block per-column , and apply the callable f
downcast each item to the dict of dtypes if present
Coerce to the new type
require the same dtype as ourselves
cast the result to our original type , we may have
provide coercion to our input arguments
slicing if desired
replace the to_replace value with value , possible to create new
returning a a maybe different typed block .
putmask the data to the block ; it is possible that we may create a
coerce the current block to a dtype compat for other
using the interpolate machinery
using scipy wrappers
Take values according to indexer and return them as a block.bb
return block for the diff of the values
shift the block by periods , possibly upcast
evaluate the block ; return result block ( s ) from the result
Return a list of unstacked blocks of self
compute the quantiles of the
corresponding to the given boolean array with another
putmask the data to the block ; we must be a single block and not
Get the placement , values , and mask for a Block unstack .
Unbox to an extension array.
returning a same-typed block .
Take values according to indexer and return them as a block .
return a slice of my values
Shift the block by ` periods ` .
return object dtype as boxed values, such as Timestamps/Timedelta
passed to __init__ .
has no effect
dtype 'i8 ' .
reverse of try_coerce_args
Modify Block in-place with new item value
Returns an ndarray of values.
localize and return i8 for the values
1st discrete difference
int64 , with null values converted to
reverse of try_coerce_args / try_operate
coerce any object types to better types return a copy of
given value .
generate an xlwt easy style string
converts a style_dict to an xlwt style object
converts a style_dict to an xlsxwriter format dict
Unstack an ExtensionArray-backed Series.
Convert DataFrame to Series with multi-level Index.
Convert categorical variable into dummy/indicator variables.
corresponding to designated axis
stacking multiple extension-arrays .
Parse a vector of float values representing IBM 8 byte floats into
Get number of records in file.
Reads lines from Xport file and returns as dataframe
raise a helpful message about our construction
return a single array of a block that has a single dtype ; if dtype is
return an array of blocks that potentially have different dtypes
return an array of blocks that potentially have different dtypes ( and
Find the common dtype for `blocks`.
having same dtype , exclude non-consolidating blocks
Compare two array_like inputs of the same shape or two scalar values
overlap , add suffixes to overlapping entries .
found in index .
Faster version of set(arr) for sequences of small numbers.
Concatenate block managers into one.
return an empty BlockManager with the items axis of len 0
Rename one of axes.
Update mgr._blknos / mgr._blklocs.
return a dict of the counts of the function in BlockManager
create a new block manager
applying quantile reduction .
do a list replace
return a new manager with the blocks
Make deep or shallow copy of BlockManager
Convert the blockmanager data into an numpy array.
Return ndarray from blocks with specified item order
Return a dict of str ( dtype ) - > BlockManager
get a cross sectional for a given location in the
blocks having same dtype
selected item ( ndarray or BlockManager ) .
Return the data as a SingleBlockManager if fastpath=True and possible
selected item ( items if non-unique ) in-place .
Does not consolidate .
selected position .
Conform block manager to new index.
Slice/take blocks along axis=0.
Take items along any axis .
Return a blockmanager with all blocks unstacked .
Delete single item from SingleBlockManager.
Concatenate a list of SingleBlockManagers into a single
Construct SparseSeries from array.
return my self as a sparse array , do not copy by default
perform a reduction operation
Return the i-th value or values in the SparseSeries by location
Return an object with absolute value taken.
occupying requested label , default to specified
passed index label
is not contained , a; set single value at passed label .
Convert SparseSeries to a Series.
Make a copy of the SparseSeries .
Conform sparse values to new SparseIndex
Cumulative sum of non-NA/null values.
fill_value=NaN , returns a dense Series
choosing the calling Series 's values
Create a cache of unique dates from an array of dates
box the result
was passed .
Helper function for to_datetime.
Convert argument to datetime.
specified fields from the arg ( DataFrame )
parse the YYYYMMDD/ % Y % m % d format , try to deal with NaT-like ,
using fixed strptime formats ( `` % H : % M '' ,
Return a new function that emits a deprecation warning on use .
deprecate a keyword argument of a function .
containing the paramenter list with defaults
Return a fixed frequency PeriodIndex , with day ( calendar ) as the default
Create RangeIndex from a range object.
Return a list of tuples of the ( attr , formatted_value )
The minimum value of the RangeIndex
The maximum value of the RangeIndex
Returns the indices that would sort the index and its
contain the same elements .
Returns the smallest element greater than or equal to the limit
Returns the largest element smaller than or equal to the limit
Extended Euclidean algorithms to solve Bezout 's identity :
Form the union of two Index objects and sorts if possible
specialized to RangeIndex
Convert the PandasArray to a :class:`numpy.ndarray`.
using the amount of space requested .
Perform ljust, center, rjust against string or list-like
use pprint_thing ( )
is the sanctioned way of converting objects
Return the formatted obj as a unicode string
Load data from Google BigQuery.
Draw a matrix of scatter plots .
Plot a multidimensional dataset in 2D.
Generate a matplotlib plot of Andrews curves , for visualising clusters of
Bootstrap plot on mean, median and mid-range statistics.
coordinates plotting .
Lag plot for time series.
Autocorrelation plot for time series.
Check a sequence of terms for instances of PandasObject .
Align a set of terms
given its type , raw value , and possibly empty
Plots a Series on the given Matplotlib axes or the current axes
Initialize axes for time-series plotting
Get the freq attribute of the ax object if set .
seconds to 'D days HH : MM : SS.F '
Pretty-formats the date axis (x-axis).
have the same type .
Return a html representation for a particular DataFrame .
Render a DataFrame to a console-friendly tabular output .
rows as ( index , Series ) pairs .
rows as namedtuples .
Compute the matrix mutiplication between the DataFrame and other .
Construct DataFrame from dict of array-like or dicts.
Convert the DataFrame to a NumPy array.
Convert the DataFrame to a dictionary.
Write a DataFrame to a Google BigQuery table .
structured or record ndarray to DataFrame .
Convert DataFrame to a NumPy record array.
Construct a DataFrame from a list of tuples.
Read CSV file.
Convert to SparseDataFrame.
Export DataFrame object to Stata dta format.
Write out the binary feather-format for DataFrames .
Write a DataFrame to the binary parquet format .
Render a DataFrame as an HTML table .
Print a concise summary of a DataFrame .
Return the memory usage of each column in bytes .
Transpose index and columns.
retrieve single value at passed column and index .
Put single value at passed column and index .
Query the columns of a DataFrame with a boolean expression.
Evaluate a string describing operations on DataFrame columns .
Return a subset of the DataFrame 's columns based on the column dtypes .
boxed values for a column .
do n't have an index , that we can create one from the
Add series to DataFrame in specified column.
specified location .
go into the BlockManager as new blocks ) are
Label-based "fancy indexing" function for DataFrame.
are guaranteed non-Nones in the axes .
specified labels from rows or columns .
Alter axes labels.
using existing columns .
Reset the index , or a level of it .
missing values .
removed , optionally only
denoting duplicate rows , optionally only
Return the first ` n ` rows ordered by ` columns ` in descending order .
Return the first ` n ` rows ordered by ` columns ` in ascending order .
Swap levels i and j in a MultiIndex on a particular axis.
using input order .
Perform column-wise combine with another DataFrame .
Update null elements with value in the same location in `other`.
using non-NA values from another DataFrame .
Stack the prescribed level ( s ) from columns to index .
Pivot a level of the ( necessarily hierarchical ) index labels , returning
First discrete difference of element.
Apply a function along an axis of the DataFrame .
Apply a function to a Dataframe elementwise .
rows of ` other ` to the end of caller , returning a new object .
Join columns of another DataFrame.
Round a DataFrame to a variable number of decimal places.
excluding NA/null values .
Compute pairwise correlation between rows or columns of DataFrame
Count non-NA cells for each column or row.
Count distinct observations over requested axis.
Return index of first occurrence of minimum over requested axis.
Let 's be explicit about this .
Get the mode ( s ) of each element along the selected axis .
given quantile over requested axis .
beginning * of period .
is contained in values .
return an integer array of the values .
cast the values to the dtype if they
array to numpy arrays with a mask
raise a TypeError if not
coerce to an ndarary of object dtype
Cast to a NumPy array or IntegerArray with 'dtype'.
containing counts of each category .
Return values for sorting.
return the length of a single non-tuple indexer which could be a slice
are index sliceable , then return my slicer , otherwise return None
are the same length .
is a dict
create a filtered indexer that does n't have any missing indexers
convert indices into valid , positive indices .
Perform bounds-checking for an indexer .
want to take the cross-product
does n't reduce to a Series or Scalar .
do n't break
check the key for valid keys across my indexer
enlarge its target
is the possibility to use `` _multi_take `` .
Create the indexers for the passed tuple of keys , and execute the take
Transform a list-like of keys into a new index and an indexer .
be a boolean
Check that indexer can be used to return a result ( e.g .
indexing key into something we can use to do actual fancy
Transform a list of keys into a new array ready to be used as axis of
is pretty simple as we just have to deal with labels
Translate any partial string timestamp matches in key, returning the
Check that 'key ' is a valid position in the desired axis .
Return Series values by list or array of integers
have to deal with our valid types
keys to be the same type as the index ( so we do n't
convert to label arguments )
return the block manager from a dataframe of series ,
makes sense when fill_value is NaN
Conform a set of SparseSeries ( with NaN fill_value ) to a common SparseIndex
Init self from ndarray or list of lists.
Init self from scipy.sparse matrix.
Return the contents of the frame as a sparse SciPy COO matrix .
Original pickle format
Convert to dense DataFrame
Get new SparseDataFrame applying func to each columns
Make a copy of this SparseDataFrame
Ratio of non-sparse points to total (dense) data points
Creates a new SparseArray from the input value .
Returns a row (cross-section) from the SparseDataFrame as a Series
Returns a DataFrame with the rows/columns switched.
Return SparseDataFrame of cumulative sums over requested axis.
Analogous to DataFrame.apply, for SparseDataFrame
Convert a conda package to its pip equivalent .
Generate the pip dependencies file from the conda file , or compare that
do platform conversion , allow ndarray or list here
return a boolean if we have a nested object , e.g .
cast to the specified dtype ( e.g .
upcasts the result .
is a convenience; interpret the dtype from a scalar or array .
interpret the dtype from a scalar
infer the dtype from a scalar or array
Try to infer an object 's dtype , for use in arithmetic ops
provide explicit type promotion and coercion
string like dtypes to object for
coerce the indexer input array to the smallest dtype possible
given a dtypes and a result set , coerce the result elements to the
given dtype a nan-safe manner .
have an object dtype , try to coerce dates and/or numbers
have a array ( or single object ) that is datetime like ,
cast the array/value to a datetimelike dtype , converting float
given dtypes .
filled with values
create a np.ndarray / pandas type of specified shape and dtype
Transform any list-like object in a 1-dimensional numpy array of object
Construct a new ndarray , coercing ` values ` to ` dtype ` , preserving NA .
returns the casted version , raising for when data is
Make a scatter plot from two DataFrame columns
Make a histogram of the DataFrame 's .
using matplotlib .
Grouped histogram
Make box plots from DataFrameGroupBy data.
check whether ax has data
Return result axes
Common post process for each axes
Common post process unrelated to data
is reasonably expensive and is
left ( primary ) or right ( secondary ) axes
based on column number and its label .
return the actual errorbar data
merge BoxPlot/KdePlot properties to passed kwds
Plot DataFrame columns as lines.
Vertical bar plot.
Make a horizontal bar plot .
Draw one histogram of the DataFrame's columns.
Draw a stacked area plot .
Create a scatter plot with varying marker point size and color .
Generate a hexagonal binning plot .
combined index : return intersection or union ( depending on the
Return a list with distinct elements of `` objs '' ( different ids ) .
Return the union or intersection of indexes .
Return the union of indexes .
Verify the type of indexes and convert lists to Index .
Give a consensus 'names ' to indexes .
Determine if all indexes contain the same elements .
args to DBAPI2.0 compliant format .
Process parse_dates argument for read_sql functions
be read as such .
Wrap result set of query in a DataFrame.
Execute the given SQL query using the provided connection object .
Read SQL database table into a DataFrame.
Read SQL query into a DataFrame.
Read SQL query or database table into a DataFrame.
stored in a DataFrame to a SQL database .
Check if DataBase has named table .
is a string )
return the correct PandasSQL subclass based on the
Get the SQL db table schema for the given frame .
Execute SQL statement inserting data
Return generator through chunked result set.
Make the DataFrame 's column types align with the SQL table
Return generator through chunked result set
Return a list of SQL statements that creates a table reflecting the
is given .
Helper for membership check for ``key`` in ``cat``.
turn values into codes given the specified categories
Convert a set of codes for to a new set of categories
Factorize an input `values` into `categories` and `codes`.
A higher-level wrapper over `_factorize_from_iterable`.
Coerce this type to another dtype
Construct a Categorical from inferred values.
Make a Categorical type from codes and categories or dtype .
Get the codes .
Sets new categories inplace
updating the CategoricalDtype
Set the ordered attribute to the boolean value.
be ordered .
be unordered .
specified new_categories .
Rename categories.
specified in new_categories .
Add new categories.
Remove the specified categories .
are not used .
using input correspondence ( dict , Series , or function ) .
Shift Categorical by desired number of periods.
Memory usage of my values
Return a Series containing counts of each category .
Return the values .
returning a new
ranking ordered categorical data .; See GH # 15420
values using the specified method .
Take elements from the Categorical .
Return a slice of myself .
displaying only max_vals and an optional ( but default
return the base repr for the categories
Returns a string representation of the footer.
return an indexer coerced to the codes dtype
Compute the inverse of a categorical , returning
The minimum value of the object.
Returns the mode ( s ) of the Categorical .
Return the `` Categorical `` which `` categories `` and `` codes `` are
True if categorical arrays are equal .
True if categoricals are the same dtype
Describes this Categorical
are contained in Categorical .
Convert argument to timedelta.
string ' r ' to a timedelta object .
Convert a list of objects to a timedelta index object .
Generates a sequence of dates corresponding to the specified time
Vectorized apply of DateOffset to DatetimeIndex,
provided date backward to next offset only if not on offset .
provided date forward to next offset only if not on offset .
Used for moving to next business day .
is positive , return tomorrow 's business day opening time .
Return business hours in a day by seconds.
using calculated values .
be called in apply method .
Add days portion of offset to DatetimeIndex i.
given DatetimeIndex , specialized for case where
has the same
was on a fiscal year
objects along a particular axis with optional set logic
be used along concatenation axis .
Compute the vectorized membership of `` x in y `` if possible , otherwise
Compute the vectorized membership of `` x not in y `` if possible ,
Cast an expression inplace.
search order for local (i.e., @variable) variables:
Evaluate a binary operation * before * being passed to the engine .
datetimes to a comparable value in an expression .
Compute a simple cross tabulation of two ( or more ) factors .
considering index levels .
based on left and right data .
left / right DataFrames in specified layout .
appropriate for table outplot
values into discrete intervals .
Quantile-based discretization function.
passed data is of datetime/timedelta type ,
passed bin is of datetime/timedelta type ,
bins to a DatetimeIndex or TimedeltaIndex if the orginal dtype is
based on the dtype , return our labels
preprocessing for cut where we convert passed
post processing for the cut method where
Round the fractional part of the given number
Infer an appropriate precision for _round_frac
Try to find the most capable encoding supported by the console .
has length of at most 'compat_args ' .
Check that the keys in ` arg_val_dict ` are mapped to their
passed into a function
Checks whether 'kwargs' contains any keys that are not
passed to the * * kwargs argument in a
passed to the * args and * * kwargs argument in a
argument passed in arg_name is of type bool .
columns / axis functions
Validate the keyword arguments to 'fillna ' .
have a deprecation warning , show it
Create a TimeGrouper and return our resampler .
Return our appropriate resampler when grouping as well .
Adjust the `first` Timestamp to the preceeding Timestamp that resides on
Adjust the provided `first` and `last` Periods to the respective Period of
Utility frequency conversion method for Series/DataFrame.
Is the resampling from a DataFrame column or MultiIndex level .
Setup our binners.
Create the BinGrouper , assume that self.set_grouper ( obj )
producing a like-indexed Series on each group and return
Re-evaluate the obj with a groupby aggregation.
loffset is set , offset the result index .
Return the correct class for resampling with groupby .
wrap any results .
according to different methods .
_upsample ; we are stripping all of the _upsample kwargs and
defined function .
Adjust our binner when upsampling.
raise if we have an invalid axis .
Return a data hash of the Index/Series/DataFrame
Hash an MultiIndex / list-of-tuples efficiently
Hash a single tuple efficiently
hashing its categories , and then mapping the codes
Given a 1d array , return an array of deterministic integers .
Hash scalar value
sure the provided value for -- single is a path to an existing
Execute a command as a OS terminal .
Call sphinx to build documentation .
Open a browser tab showing single
Open the rst file ` page ` and extract its title .
Create in the build directory an html file with a redirect,
Build HTML documentation.
Build PDF documentation.
generated files .
Compress HTML documentation into a zip file.
Render a DataFrame to a LaTeX tabular/longtable environment output .
Print clines after multirow-blocks are finished
Checks whether the 'name' parameter for parsing is either
Check if the ` names ` parameter contains duplicates .
Generic reader of line files.
Check whether or not the ` columns ` parameter
Check whether or not the 'usecols ' parameter
are present in a given
Validate the 'usecols ' parameter .
Check whether or not the 'parse_dates ' parameter
return a stringified and numeric for these values
Get the NaN values for a given column .
return the names , index_names , col_names
Infer types of values, possibly casting
Cast values to specified type
undergo dtype conversions .
Sets self._col_indices
begins with the BOM character .
Alert a user about a malformed row .
iterating through ` self.data ` ( CSV source ) .
remove any that are
Try several cases to get lines :
skipping as specified .
Determine the URL corresponding to Python object
use : :
write it to ` stream `
given block manager and indexers .
use when concatenating specified units .
Check if the join units consist of blocks of uniform type that can
Reduce join_unit's shape along item axis to length.
Combine multiple concatenation plans into one.
set a parameter value using the with statement .
Convert from SIF to datetime.
Convert from datetime to SIF.
Checks the dtypes of the columns of a pandas DataFrame for
Returns the byte of the given ordinal .; stata types .
stata 's default format for this type .
Takes a bytes instance and pads it with null bytes until it 's length chars .
Map between numpy and state dtypes
Converts categorical columns to Categorical type.
call encode before writing to file for Python 3 compat .
retain categorical information for
floating point data columns for nans , and replaces these with
column names to ensure that they are valid Stata column names .
Close the file if it was created by the writer .
Generates the GSO lookup table for the DataFRame
Generates the binary blob of GSOs that is written to the dta file .
Surround val with <tag></tag>
Write the file header
populates the values in; Called twice during file write .
strl if they might have been
Convert columns to StrLs if either very large or in the
Register Pandas Formatters and Converters with matplotlib
Remove pandas' formatters and converters
Convert :mod:`datetime` to the Gregorian date as UTC float days,
spacing between consecutive ticks for annual data .
Returns the indices where the given period changes .
is at least one label
Return the : class : ` ~matplotlib.units.AxisInfo ` for * unit * .
Pick the best locator based on a distance .
Set the view limits to include the data range .
Returns the default locations of ticks .
Sets the view limits to the nearest multiples of base that contain the
Returns the default ticks spacing .
Sets the locations of the ticks
'index ' for regular , or 'level_x ' for Multi
corresponding NumPy / pandas type
Create a Table schema from `` data `` .
Builds a DataFrame from a given schema
pin to an operation result .
Try to find a name to attach to the result of an operation between
pandas types to unify behavior of arithmetic
Return a binary method that always raises a TypeError .
arguments to pass to numexpr for the given operation .
use when filling in undefined values
pass to numexpr for this
attach to this method according to conventions
Make the appropriate substitutions for the given operation and class-typ
is given , replace null entries in left and right
Apply the function ` op ` to only non-null points in x and y .
given arithmetic operation fails , attempt it again on
has mismatched types and is not necessarily meaningful ,
dispatch to its
Evaluate the frame operation func ( left , right ) by evaluating
left in the given index_class to delegate the operation op
Assume that left or right is a Series backed by an ExtensionArray ,
use when defining flex/special
Adds the full suite of special arithmetic methods ( `` __add__ `` ,
Adds the full suite of flex arithmetic methods ( `` pow `` , `` mul `` , `` add `` )
align lhs and rhs Series
is an Index object ) and; has a non-None name ( e.g .
returns a tuple of like indexed series instead of a single series .
Wrapper function for Series arithmetic operations, to avoid
self , other using alignment and fill
meet lhs dims if input is list , tuple or np.ndarray
float64 if the result is expected
is passed to the Datetime/Timedelta Array/Index
Check that the ` closed ` argument is among [ None , `` left '' , `` right '' ]
passes a freq and another freq is inferred from passed data ,
Comparing a DateOffset to the string `` infer '' raises , so we need to
coercing an input scalar or array to i8 .
Construct a scalar type from a string.
Unbox the integer value of a scalar `value`.
Verify that ` self ` and ` other ` are compatible .
using specified date_format .
be inserted to maintain order .
Repeat elements of an array.
Return a Series containing counts of unique values .
is compatible with the values of a given
Add a timedelta-like , Tick or TimedeltaIndex-like object
Add a delta of a timedeltalike
Add a delta of a TimedeltaIndex
Add pd.NaT to self
Subtract pd.NaT from self
is only valid if self; Subtract a Period Array/Index from self .
Add or subtract array-like of integers equivalent to applying
Add or subtract array-like of DateOffset objects
Shift each value by `periods`.
are re-localized .
Return the minimum value of the Array or minimum along
Return the maximum value of the Array or maximum along
convert Period-like to PeriodDtype
render a consistent error message when raising
Construct a new PeriodArray from a sequence of Period scalars .
are available , ensure they match .
Convert an datetime-like array to values Period ordinals.
Construct a PeriodArray from a datetime64 array
Cast to DatetimeArray/Index.
specified frequency ` freq ` .
actually format my specific types
Add a timedelta-like , Tick , or TimedeltaIndex-like object
Arithmetic operations with timedelta-like scalars or array `other`
Option change callback for na/inf behaviour
left and right , have equal non-NaN elements , and NaNs
infer the fill value for the nan/NaT from the provided
have a compatible fill_value and arr dtype , then fill
Return a dtype compat na value
containing only true/non-NaN values , possibly empty .
convert DataFrame and Series to matplotlib.table
Create a figure with a set of subplots already made .
Render tempita templates before calling cythonize
Fast transform path for aggregations
Return a copy of a DataFrame excluding elements from groups that
common agg/transform wrapping logic
fast version of transform, only applicable to
Return a copy of a Series excluding elements from groups that
Return number of unique elements in the group.
excluding missing values
Calcuate pct_change of each value to previous entry in group
sub-classes to define
have categorical groupers , then we want to make sure that
join grouped columns in output
Return DataFrame with number of distinct observations per group for
Extract the ndarray or ExtensionArray from a Series or Index .
nested sequence .
is a valid boolean indexer .
avoid numpy DeprecationWarnings , cast float to integer where valid .
array , for use in Index .
have a null slice .
have a full length slice .
using obj and kwargs if it is callable ,
standardize a supplied mapping .
processing random_state arguments .
Apply a function `` func `` to object `` obj `` either by passing obj as the
map names/labels , dependent if mapper
return the correct fill value for the dtype of the values
get the values view , mask , dtype
wrap our results if needed
Return the missing value for ` values `
Check if any elements along an axis evaluate to True .
Check if all elements along an axis evaluate to True .
Sum the elements along an axis ignoring NaNs
Compute the mean of the element along an axis ignoring NaNs
Compute the standard deviation along given axis while ignoring NaNs
Compute the variance along given axis while ignoring NaNs
Compute the standard error in the mean along given axis while ignoring NaNs
Compute the sample skewness .
Compute the sample excess kurtosis
a, b: ndarrays
skips missing values , specialized to
skips missing values .
writting a formatted < th > cell .
write text representation of object to the system clipboard
Get an iterator given an integer , slice or container .
Try to read from a url , file or string .
simulate bs4 's ability to pass in kwargs to
Choose the parser based on the input flavor .
tables into a `` list `` of `` DataFrame `` objects .
return all tables from the DOM .
Given a table , return parsed header , body , and foot .
Given a list of < tr > s , return a list of text rows .
removing hidden elements
Raises
return appropriate class of Series concat
return appropriate class of DataFrame-like concat
is a single
Concatenate an object/categorical array of arrays, each of which is a
Combine list-like of Categorical-like, unioning categories.
provide concatenation of an datetimelike array of arrays each of which is a
concat DatetimeIndex with the same tz
concat all inputs as object.
provide concatenation of an sparse/dense array of arrays each of which is a
Concatenates multiple RangeIndex instances.
Rewrite the message of an exception .
Given an index , find the level length for each element .
Convert the DataFrame in `self.data` and the attrs from `_build_styles`
Format the text display value of cells.
Render the built up styles to HTML .
Update the state of the Styler .
Execute the style functions built up in ` self._todo ` .
Apply a function column-wise , row-wise , or table-wise ,
Apply a function elementwise , updating the HTML
Hide columns from rendering.
Shade the background `` null_color `` for missing values .
Color the background in a gradient according to
according to the data .
setting one or more non-data dependent
Draw bar chart in dataframe cells.
Draw bar chart in the cell backgrounds.
shading the background .
Highlight the min or max in a Series or DataFrame.
creating a subclass of `` Styler ``
provide compat for construction of strings to numpy datetime64 's with
provide compat for construction of an array of strings to a
incoming data can be represented as ints .
want to get an index value , never a value
have bytes , decode them to unicode
ensure that the where is a Term or a list of Term
close it if we opened it
Read from the store , close it if we opened it .
Check if a given group is a metadata group for a given parent_group .
get/create the info for this name
return an encoded zone
coerce the values to a DatetimeIndex if tz is set
take a string-like that is object dtype and coerce to a fixed size
inverse of _convert_string_array
Open the file in the specified mode
buffered modifications to be written to disk .
stored in file
stored in file , optionally based on where
return the selection as an Index
is generally only useful to; return a single column from the table .
Retrieve pandas objects from multiple tables
Store object in HDFStore
specifying the where condition
exist and be Table; Table in file .
Append to multiple tables
Create a pytables index on the table
return a list of all the top-level nodes ( that are not themselves a
Walk the pytables group hierarchy for pandas objects
return the node with the key or None if it does not exist
return the storer object for a key , raise if not in the file
existing store to a new file , upgrading in place
Print detailed information on the store.
return the new kwargs
return a suitable class to operate
set the name of this indexer
set the position of this column in the Table
am an indexed column
return a new object
set the values from this selection : take = take ownership
set a string col itemsize :
return the compared against itemsize
set/update the info for this indexable with the key/value
set my state from the passed info
does not change the categories
set the meta data
return a new datacol with the block i
record the metadata
setup my atom from the block b
return the PyTables column class for this column
have the same order as the existing & same dtype
set the data from this selection ( and convert to the correct dtype
get the data for this column
set the data for this column
set our version
set my pandas type & version
infer the axes of my storer
deleting the node in its entirety ( only ) - where
remove table keywords from kwargs and return
set our object attributes
retrieve our attributes
read an array for the specified node ( off of group
write a 0-len array
do n't support start , stop kwds in Sparse
write it as a collection of individual sparse series
validate against an existing table
create / validate metadata
store the multi-index ; reset and return the
based on our axes , compute the expected nrows
return a tuple of my permutated axes , non_indexable at the front
return a dict of the kinds allowable columns for this object
return the metadata pathname for this key
write out a meta data array to the key as a fixed-format Series
return the meta data array for this key
set our table type & indexables
are we trying to operate on an old version ?
validate the min_itemisze does n't contain items that are not in the
do n't exist
Create a pytables index on the specified columns
return the axes sniffed from the table : return boolean
take the input data_columns and min_itemize and create a data
return the axes
process axes filters
create the description of the table from the axes & values
row numbers ) from a table ; return the
return a single column from the table , generally only indexables
have n indexable columns , with an arbitrary number of data
form the data into a 2-d including indexes , values , mask
are going to write this as a frame table
create the indexables from the table description
be a : dict , list , tuple , string
generate the selection
Cast to a NumPy array with 'dtype'.
Return the indices that would sort this array .
Shift values by desired number.
Compute the ExtensionArray of unique values .
missing value suitable for factorization .
Encode the extension array as an enumerated type.
Take elements from an array .
Formatting function for scalar values .
Return a scalar result of performing the reduction operation .
returns a method that will correspond to an
Make an alias for a method of the underlying ExtensionArray.
Create a comparison method that dispatches to `` cls.values `` .
Create the join wrapper methods .
sorted copy of Index .
Return the minimum value of the Index or minimum along
Returns the indices of the minimum values along an axis .
Return the maximum value of the Index or maximum along
Returns the indices of the maximum values along an axis .
Return a list of tuples of the ( attr , formatted_value ) .
do n't allow integer or float indexing on datetime-like when using
have to override the
is found in the
has the same class .
Shift index by desired number of time frequency increments.
using the fill method specified when no
Return a tuple of the doc parms .
passed a manager and a axes dict
validate the passed dtype
Provide axes setup for the major PandasObjects.
Return an axes dictionary for myself.
Return an axes dictionary for the passed axes.
supplied in args/kwargs .
Map the axis to the block_manager axis.
Return the space character free column resolvers of a dataframe .
Return a tuple of axis dimensions
Permute the dimensions of the % ( klass ) s
axes appropriately .
Return DataFrame with requested index / column level(s) removed.
Return item and drop from frame.
Squeeze 1 dimensional axis objects into scalars.
Swap levels i and j in a MultiIndex on a particular axis
Alter axes input function or functions.
Set the name of the axis for the index or columns.
Set the name(s) of the axis.
Return the bool of a single element PandasObject .
is a level reference for a given axis .
is a label reference for a given axis .
is a label or level reference for a given axis .
is ambiguous .
Return a 1-D array of values associated with ` key ` , a label or level
and/or levels for the given ` axis ` .
is empty .
use the same
Convert the object to a JSON string.
contained data to an HDF5 file using HDFStore .
Serialize object to input file path using msgpack format .
Return an xarray object from the pandas object.
Create an indexer like _name in the class.
given key ( DataFrame column , Panel slice ,
Return the cached item , item represents a label indexer .
calling object with a weakref to
Return the cached item , item represents a positional indexer .
See if we need to update our parent cacher if clear , then clear our
Construct a slice of this container.
Check if we are a view , have a cacher , and are of mixed type .
Return the elements in the given * positional * indices along an axis .
Return cross-section from the Series/DataFrame .
corresponding to axis labels matching criteria .
matching indices as other object .
Used in the `` drop `` method; specified axis .
Replace self internals with result.
string ` prefix ` .
string ` suffix ` .
Sort by the values along either axis.
Sort object by labels (along an axis).
Conform % ( klass ) s to new index with optional filling logic , placing
Perform the reindex for all the axes .
Check if we do need a multi reindex .
indicates an internal call here
rows or columns of dataframe according to labels in
Return a random sample of items from an axis of object .
add the string-like attributes from the info_axis .
have changed , then clear the
Consolidate data in place and return None
Compute NDFrame with "consolidated" internals (data of each dtype
allow in-place setting with this type of value
Convert the frame to its Numpy-array representation.
Return a Numpy representation of the DataFrame .
Return counts of unique ftypes in this object.
Return the dtypes in the DataFrame .
Return the ftypes ( indication of sparse/dense and dtype ) in DataFrame .
Convert the frame to a dict of dtype -> Constructor Types that each has
Return a dict of dtype - > Constructor Types that
Cast a pandas object to a specified dtype `` dtype `` .
Make a copy of this object 's indices and data .
infer better dtype for object columns
infer better dtype for object columns .
infer better dtypes for object columns .
Return the last row ( s ) without any NaNs before ` where ` .
Trim values at input threshold(s).
given threshold .
using a mapper or by a Series of columns .
specified frequency .
Select values at particular time of day (e.g.
Select values between particular times of the day (e.g., 9:00-9:30 AM).
Resample time-series data.
subsetting initial periods of time series data
subsetting final periods of time series data
Compute numerical data ranks (1 through n) along axis.
Equivalent to public method `where`, except that `other` is not
shifted data will; shift ` without copying data .
Shift the time index , using the index 's frequency if available .
Truncate a Series or DataFrame before and after some index value .
target time zone .
Localize tz-naive index of a Series or DataFrame to target time zone .
summarize the central tendency ,
used by describe and quantile ) .
Add the operations to the cls ; evaluate the doc strings again
Add the series only operations to the cls ; evaluate the doc
Add the series or dataframe only operations to the cls ; evaluate
Retrieves the index of the first valid value .
cached properties .; is passed , only clears that key .
arg is a string , then try to operate on it :
provide an implementation for the aggregators
return a new object with the replacement attributes
Return the size of the dtype of the item of the underlying data .
Return the base object if the memory of the underlying data is shared .
The ExtensionArray of the data backing this Series or Index.
representing the values in this Series or Index .
losing information .
Return the maximum value of the Index .
Return an ndarray of the maximum argument indexer.
Return the minimum value of the Index .
Return a ndarray of the minimum argument indexer .
Return a list of the values .
perform the reduction type operation if we can
maps values using the input
Return number of unique elements in the object.
Memory usage of the values
Return the argument with an initial component of ~ or ~user
convert a path-like object to a string .
is a url , translate and return the buffer .
Get the compression method for filepath_or_buffer .
given path/buffer and mode .
convert timedelta-like to timedelta64
timedelta64 [ ns ] dtype , treating
Convert a object-dtyped or string-dtyped array into an
Add DatetimeArray/Index or ndarray[datetime64] to TimedeltaArray.
Return a dataframe of the components ( days , hours , minutes ,
Add engine to the excel writer registry.io.excel.
Convert Excel column name like 'AB' to 0-based column index.
separated list of column names and ranges to indices .
parsing in ` parsers.py ` .
Forward fill blank entries in row but only inside the same parent index.
Pop the header name for MultiIndex parsing.
are grabbing the correct scope .
Replace a number with its hexadecimal representation .; Used to tag
Return the padded hexadecimal id of `` obj `` .
Return a prettier version of obj
Resolve a variable name in a possibly local context
Replace a variable name , with a potentially new value .
scoped variables from a list of stack frames .
Update the current scope by going back ` level ` levels .
Add a temporary variable to the scope .
Return the full scope for use with passing to engines transparently
stored as either XPORT or SAS7BDAT format files .
Install the scalar coercion methods.
Derive the `` _data '' and `` index '' attributes of a new Series from a
Construct Series from array.
want to set the _typ here .
contains boxed values .
selected slices of an array along given axis as a Series .
Return the * integer * indices of the elements that are non-zero .
Create a new view of the Series .
Return the i-th value or values in the Series by location .
Repeat elements of a Series.
Generate a new DataFrame or Series with the index reset .
Render a string representation of the Series .
label - > value } dict or dict-like object .
Convert Series to DataFrame.
Convert Series to SparseSeries.
Set the Series name .
Return number of non-NA/null observations in the Series.
Return Series with duplicate values removed.
Return the row label of the minimum value .
Return the row label of the maximum value .
given number of decimals .
given quantile .
Compute the dot product between the Series and the columns of other .
Concatenate two or more Series.
Perform generic binary operation with optional fill value.
Combine the Series with a Series or scalar according to ` func ` .
choosing the calling Series 's values first .
using non-NA values from passed
Sort by the values.
Sort Series by index labels.
omitting NA/null values ,
Return the largest ` n ` elements .
Return the smallest ` n ` elements .
Swap levels i and j in a MultiIndex.
according to input correspondence .
Invoke function on values of Series.
Perform a reduction operation .
Alter Series index labels or name.
filling logic .
Return the memory usage of the Series .
are contained in Series .
left < = series < = right .
Return a new Series with missing values removed .
Return Series without null values.
PeriodIndex with desired
Convert argument to a numeric type.
Create a 0-dim ndarray containing the fill value
Perform a binary operation between two arrays .
op result to have correct dtype
be SparseSeries or SparseArray
return an ndarray for our input ,
Convert ndarray to sparse format
The percent of non- ``fill_value`` points, as decimal.
missing values with ` value ` .
Get the location of the first missing value .
containing counts of unique values .
Change the dtype of a SparseArray .
Tests whether all elements evaluate True
Tests whether at least one of elements evaluate True
Sum of non-NA/null values
Mean of non-NA/null values
Tokenize a Python source code string .
Replace ``&`` with ``and`` and ``|`` with ``or`` so that bitwise
Replace local variables with a syntactically valid name.
surrounded by backticks .
Compose a collection of tokenization functions
are subclasses of `` superclass `` .
Return a function that raises a NotImplementedError with a passed node
disallow certain nodes from parsing .
Return a function to create an op class with its symbol already passed .
add default implementation of ops .
Get the names in an expression
return a boolean whether I can attempt conversion to a TimedeltaIndex
Return a fixed frequency TimedeltaIndex , with day as the default
concatenated to the end of self .
removed from self .
insert ` value ` so as to maintain order .
based on type and coerce into matrices .
create the manager .
Return list of arrays, columns.
return an ndarray of the underlying , pass
specified , coerce to the
is passed .
Evaluate a Python expression as a string using various backends .
Transform combination(s) of uint64 in one uint64 (each), in a strictly
arrays to MultiIndex .
Convert list of tuples to MultiIndex.
Make a MultiIndex from the cartesian product of multiple iterables .
Make a MultiIndex from a DataFrame .
Set new levels on MultiIndex.
Set new codes on MultiIndex.
Make a copy of this object .
is defined as a copy with the same identity
return a boolean if we need a qualified .info display
return the number of bytes in the underlying data
is monotonic increasing ( only equal or
return the hash for the provided key
Return vector of label values for requested level,
Create a DataFrame with the levels of the MultiIndex as columns .
Return a MultiIndex reshaped to conform to the
versionadded : : 0.20.0
Create a new MultiIndex from the current that removes
handle NA filling of take
Append a collection of Index options together
Make new MultiIndex with passed list of codes deleted
Swap level i with level j.
using input order .; drop or duplicate levels
categorizing our codes by using the
Sort MultiIndex at the requested level.
Create index with target's values (move/add/delete values as necessary)
compute the slice locations for input
Get location for a label or a tuple of labels as an integer, slice or
Get both the location for the requested label ( s ) and the
given label/slice/list/mask or a sequence of such as
/ tuples , return new MultiIndex
have the same labeling information
are the same
Form the union of two MultiIndex objects
Form the intersection of two MultiIndex objects.
set difference of two MultiIndex objects
inserting new item at location
passed location deleted
ensure that our data is of the correct
reverse of _ensure_data
ensure that we are arraylike if not already
Compute locations of to_match into values
are returned in order
Compute the isin boolean array
Factorize an array-like to labels and uniques .
Compute a histogram of the counts of non-null values .
denoting duplicate values .
Returns the mode ( s ) of an array .
given axis .
checks for underflow and overflow .
Compute sample quantile or quantiles of the input array.
take which sets NaN values in one pass
difference of n between self,
For arbitrary (MultiIndexed) SparseSeries return
Convert a SparseSeries to a scipy.sparse.coo_matrix using index
Convert a scipy.sparse.coo_matrix to a SparseSeries .
Timestamp-like => dt64
convert datetime-like to datetime64
Convert data to array of timestamps.
based on dtype conventions , issuing deprecation warnings
is inferred from data , check that it is compatible with
Check that a dtype , if passed , represents either a numpy datetime64 [ ns ]
given dtype is a DatetimeTZDtype , extract the implied
is not explicitly given via ` tz ` , see if one can
Localize a start or end Timestamp to the timezone of the corresponding
subtract DatetimeArray/Index or ndarray[datetime64]
Convert tz-aware Datetime Array/Index from one time zone to another.
Localize tz-naive Datetime Array/Index to tz-aware
Convert times to midnight.
Cast to PeriodArray/Index at a particular frequency.
Calculate TimedeltaArray of difference between index
Return the month names of the DateTimeIndex with specified locale .
Returns numpy array of datetime.time.
float64 ndarray of Julian Dates .
Yield information about all public API items.
Validate the docstring .
Validate the docstring for the given func_name
Execute the validation of all docstrings , and return a dict with the
Import Python object from its name as string.
object that contains the source code of the object .
is implemented ( e.g .
Check if the docstrings method can return something .
Convert numpy types to Python types for the Excel writers.
path 's extension against the Writer 's supported
specified sheet ( s ) into a DataFrame
is of the right type .
checking if s is a pytables-acceptable expression
inplace conform rhs
return the op string for this TermValue
convert the expression that is in the term to something that is
invert the filter
return the numexpr condition and filter
quote the string if not encoded
overcomes the NPY_MAXARGS ( 32 )
is called via the 'numpy ' library ,
'Series.argmax ' is called via the 'numpy ' library ,
is called via the 'numpy ' library , the
'NDFrame.clip ' is called via the numpy library , the third
is called via the 'numpy ' library , the third
be empty , except for allowed
be empty because all of
passed to min , max , argmin , or argmax is
serialize ) object to input file path
Load msgpack pandas object from the specified
return my dtype mapping , whether number or name
Convert strings to complex number instance with specified numpy type.
convert the numpy values to a list
deserializing numpy data types .
return the packed bytes .
Unpack a packed object , return an iterator
Convert a JSON string to pandas object .
Try to format axes if they are datelike .
has a ` read ` attribute ( e.g .
The function read_json accepts three input types:
objects into one JSON object .
Read the whole JSON input into a pandas object .
Parses a json document into a pandas object .
has only the appropriate keys for orient='split ' .
Try to convert axes .
Take a conversion function and possibly recreate the frame .
Format an array for printing.
rounded and formatted percentiles .
Return a formatter function for a range of timedeltas .
Separates the real and imaginary parts from the complex number , and
leaving just one before the decimal points if need be .
is formatted in DataFrame .
returns lengths of indexes .
Appends lines to a buffer.
considering unicode East Asian Width
Render a DataFrame to a list of columns ( as lists of strings ) .
Render a DataFrame to a html table .
be applied on each value to format it
Returns the float values converted into strings using
have DO NOT have a TZ
have a TZ
Given an Interval or IntervalIndex , return the corresponding interval with
check if start/end are valid types
check type compat of start/end/freq
Return a fixed frequency IntervalIndex
Create the writer & save
delegated names to a class using a class decorator .
Add additional __dir__ for this object.
cls from the delegate class .
return a boolean if we WILL be using numexpr
return the expression of the op on a and b
evaluate the where condition cond on a and b
writer : string or ExcelWriter object
Write a DataFrame to the feather-format
Load a feather-format object from the file path
Generate a range of dates with the spans between dates described by
Calculate the second endpoint for passing to np.arange , checking
A special case for _generate_range_overflow_safe where `periods * stride`
setting a locale .
see if we can set a locale , and subsequently get the locale ,
Return a list of normalized locales that do not throw an `` Exception ``
Get all the locales that are available on the system .
has a float dtype if possible .
is a Categorical ( if not already ) .
Ensure that an dtype array of some integer dtype
is a subclass of the klasses
is a 1-D pandas sparse array .
is a scipy.sparse.spmatrix instance .
Check if obj or all elements of list-like is DateOffset
is a periodical index .
Check whether the provided array or dtype is of the string dtype .
is a periodical array-like or PeriodIndex .
is a datetime array-like or DatetimeIndex .
is a datetime-like array-like .
Check if two dtypes are equal .
Check whether two arrays have compatible dtypes to do a union .
Check whether the provided array or dtype is of the datetime64 [ ns ] dtype .
Check if we are comparing a string-like object to a numeric ndarray .
Check if we are comparing a datetime-like object to a numeric object .
Check if we are comparing a datetime-like object to an object instance .
Check whether the array or dtype should be converted to int64 .
Check whether the provided array or dtype is of a boolean dtype .
is of a pandas extension class instance .
Check if an object is a pandas extension array type .
Return a boolean if the condition is satisfied for the arr_or_dtype .
Get the dtype instance associated with an array
Get a numpy dtype.type-style object for a dtype object .
Raises an error if invalid .; Check whether the dtype is a date-like dtype .
Convert input into a pandas only dtype object or a numpy dtype object.
are always performing a left-by type operation
designed for ordered
is similar to a left-join except that we; Perform an asof merge .
is an internal non-public method *
specified as ` on ` parameters
return the join indexers
Create a join index by rearranging one index to match another
has side effects ( copy/delete key columns )
Check if we match 'dtype ' .
Auxiliary function for :meth:`str.cat`
Count occurrences of pattern in each string of the Series/Index.
is contained within a string of a Series or Index .
matches a pattern .
Duplicate each string in the Series or Index.
Determine if each string matches a regular expression .
Used in both extract_noexpand and extract_frame
using passed regular
For each subject string in the Series, extract groups from the
return a DataFrame
Find all occurrences of pattern or regular expression in the Series/Index.
Return indexes in each strings in the Series/Index where the
Pad strings in the Series/Index up to width.
Slice substrings from each element in the Series or Index.
Replace a positional slice of a string with another value .
including newlines ) from each string in the
Extract element from each component at specified position.
using indicated encoding .
Copy a docstring from another source function ( if present )
Auxiliary function for :meth:`str.cat`.
given separator .
prepending ' 0 ' characters .
Return the Unicode normal form for the strings in the Series/Index .
Returns system information as a dict
Yields all GroupBy member defs for DataFrame/Series names in whitelist.
Dispatch to apply.
Convert bytes and non-string into Python 3 str
Bind the name/qualname attributes of the function
Raise exception with existing traceback .
converts a style_dict to an openpyxl style object
Convert a style_dict to a set of kwargs suitable for initializing
color_spec `` to an openpyxl v2 Color object
Convert ``font_dict`` to an openpyxl v2 Font object
Convert ``fill_dict`` to an openpyxl v2 Fill object
Convert ``side_spec`` to an openpyxl v2 Side object
Convert ``border_dict`` to an openpyxl v2 Border object
return a row or column based frame apply object
compute the results
have an empty result ; at least 1 axis is 0
apply to the values as a numpy array
return the results for the rows
return the results for the columns
infer the results to the same shape as the input object
Numpy version of itertools.product.
Returns the url without the s3 : // part
tested yet - use at own risk .
returns a RNN_Learner object , that wraps an instance of the RNN_Encoder module .
used to instantiate a LanguageModelData object that can be used for a
have a suffix in ` extensions ` ; optionally ` recurse ` .
exported file in ` path/fname ` with optional ` tfms ` .
self ` .
item ` .
Reconstruct one of the underlying item for its data `t`.
Create a new ` ItemList ` from ` items ` , keeping the same attributes .
have a suffix in ` extensions ` .
Create an `ItemList` in `path` from the inputs in the `cols` of `df`.
Create an `ItemList` in `path` from the inputs in the `cols` of `path/csv_name`
Use only a sample of `sample_pct`of the full dataset and an optional `seed`.
fn ` in ` self.path ` .
keep elements for which ` func ` returns ` True ` .
keep filenames in ` include ` folder or reject the ones in ` exclude ` .
Keep random sample of `items` with probability `p` and an optional `seed`.
Do n't split the data and create an empty validation set .
Split the data between `train` and `valid`.
Split the data between `train_idx` and `valid_idx`.
according to the indexes in ` valid_idx ` .
depending on the folder ( ` train ` or ` valid ` ) in which the filenames are .
putting ` valid_pct ` in the validation set , optional ` seed ` can be passed .
set with size ` train_size * n ` and valid set with size ` valid_size * n ` .
returns ` True ` for validation set ) .
using the names in ` valid_names ` for validation .
override ` self.path ` .; using the names in ` fname ` for the validation set .
Split the data from the `col` in the dataframe in `self.inner_df`.
guess one from the first element of ` labels ` .
Label `self.items` with `labels`.
Label `self.items` from the values in `cols` in `self.inner_df`.
Label every item with `const`.
Label every item with an `EmptyLabel`.
get its label .
Give a label to each filename depending on its folder .
Apply the re in ` pat ` to determine the label of every filename .
taking the sorted unique values .
overwrite the default .; Use the labels in ` train_labels ` and ` valid_labels ` to label the data .
be applied to the xs of the train and validation set .
be applied to the ys of the train and validation set .
Read the default class processors if none have been set .
Process the inner datasets.
override ` self.path ` , ` kwargs ` are passed to ` DataBunch.create ` .
Create a ` LabelLists ` with empty sets from the serialized ` state ` .
Create a ` LabelLists ` with empty sets from the serialized file in ` path/fn ` .
briefly replace the dataset with one that only contains ` item ` .
containing ` items ` from ` self.x ` and ` self.y ` .
Save `self.to_df()` to a CSV file in `self.path`/`dest`.
Return the minimal state for export .
save it in ` fn ` to load an empty version for inference .
Load the state in ` fn ` to create an empty ` LabelList ` for inference .
Create a ` LabelList ` from ` state ` .
Launch the processing on `self.x` and `self.y` with `xp` and `yp`.
be applied to the inputs and targets .
be applied to the targets only .
Parse the docstring into its components .
's defined and not an empty string , or return Unknown
Print user's setup information
is available on pypi
improve the setup to speed things up
start ` to ` end ` as pct goes from 0.0 to 1.0 .
Helper function for `anneal_poly`.
Create an `optim.Optimizer` from `opt_func` with `lr`.
Create a new ` OptimWrapper ` from ` self ` with another ` layer_groups ` but the same hyper-parameters .
weight decay and step optimizer .
makes sense for given optimizer ) .
weight decay .
Read the values inside the optimizer for the hyper-parameters .
Set `val` inside the optimizer dictionary at `key`.
Read a hyperparameter ` key ` in the optimizer dictionary .
Return the inner state minus the layer groups .
Return the inner state of the ` Callback ` , ` minimal ` or not .
calculate updated smoothed value .
Update metric computation with `last_output` and `last_target`.
Set the final result in `last_metrics`.
annealed schedule .
Build anneal schedule for all of the parameters.
Initialize our optimization params based on our annealing schedule .
Take one step forward on the annealing schedule for the optim params .
Distributed training of Imagenette .
in_size ` x ` in_size ` .
images ` n_channels ` x ` in_size ` x ` in_size ` .
Define loss functions for a GAN from `loss_gen` and `loss_crit`.
train a ` GAN ` .
expanding ` y_true ` to the size of ` y_pred ` .
Put the model in generator mode if ` gen_mode ` , in critic mode otherwise .
Evaluate the ` output ` with the critic then uses ` self.loss_funcG ` to combine it with ` target ` .
compare them to ` real_pred ` in ` self.loss_funcD ` .
Create the optimizers for the generator and critic if necessary , initialize smootheners .
's not None , return the correct input .
Record `last_loss` in the proper list.
Put the various losses in the recorder and show a sample image .
Switch the model , if ` gen_mode ` is provided , in the desired mode .
Switch the model if necessary .
Create a GAN from ` learn_gen ` and ` learn_crit ` .
Create a WGAN from ` data ` , ` generator ` and ` critic ` .
Shows `ys` (target images) on a figure of `figsize`.
Multiply the current lr if necessary .
Put the LR back to its value if necessary .
Get the indexes of the layers where the size of the activation changes .
matching ` search_term ` and ` size ` requirements ,
return them as a string .
Return a Google Images Search URL for a given search term .
Parse the Google Images Search for urls and return the image metadata as tuples ( fname , url ) .
Parse the google images html to img tuples containining ` ( fname , url ) `
label_path ` .
Downloads a single image from Google Search results to ` label_path `
Initialize the widget UI and return the UI .
preview pane .
Check if input value is empty .
Download button click handler: validate search term and download images.
Display a few preview images in the notebook
Initialize optimizer and learner hyperparameters.
Determine if loss has runaway and we should stop .
learn model weights disturbed during LRFinder exploration .
is determined by passed argument 'sz ' .
defined in self.weights , the corresponding
Uses pytorch 's built-in dropout function to apply dropout to the parameters of
be file-like ( file or buffer ); Load a saved ` DataBunch ` from ` path/file ` .
Create a ` DataBunch ` from ` train_ds ` , ` valid_ds ` and maybe ` test_ds ` with a batch size of ` bs ` .
Returns appropriate `Dataset` for validation, training, or test (`ds_type`).
need a specific DeviceDataLoader , access via the relevant property ( ` train_dl ` , ` valid_dl ` , etc ) as the index of DLs in this list is not guaranteed to remain constant .
be file-like ( file or buffer ); Save the ` DataBunch ` in ` self.path/file ` .
Get one batch from the data loader of ` ds_type ` .
Get `item` into a batch.
Show a batch of data in ` ds_type ` on a few ` rows ` .
be file-like ( file or buffer )
Check the underlying data in the training set can be properly loaded .
Instantiate a ` OneCycleScheduler ` with ` lr_max ` .
following the 1cycle policy .
end_lr ` over ` num_it ` iterations in ` learn ` .
Put ` learn ` in FP16 precision mode .
Put ` learn ` back to FP32 precision mode .
https : //arxiv.org/abs/1710.09412 to ` learn ` .
Add gradient clipping of `clip` during training.
Create a ` ClassificationInterpretation ` object from ` learner ` on ` ds_type ` with ` tta ` .
have ` last_metrics ` plot them in our pbar graph
Clip the gradient before the optimizer step.
check if loss is reduction
accumulate samples and batches
result in no stepping
step the rest of the accumulated grads if not perfectly divisible
Create an instance of `ClassificationInterpretation`
Confusion matrix as an `np.ndarray`.
using ` cmap ` .
Sorted descending list of largest non-diagonal entries of confusion matrix , presented as actual , predicted , number of occurrences .
defaulting to all losses ( sorted by ` largest ` ) .
Calculates the F-beta score ( the weighted harmonic mean of precision and recall ) .
is if you run with : python -m fastai.launch; Distributed training of Imagenet .
Get the metadata associated with ` arch ` .
cut the model as specified by ` cut ( model ) ` ( function ) .
takes ` nf ` features , runs through ` lin_ftrs ` , and about ` nc ` classes .
Create custom convnet architecture
Build convnet style learner.
Build Unet learner from `data` and `arch`.
indicates if we want to use Test Time Augmentation .
Show images in `top_losses` along with their prediction, actual, loss, and probability of actual class.
Show images in `top_losses` along with their prediction, actual, loss, and probability of predicted class in a multilabeled dataset.
Gets indices with top losses.
dataset by top losses and returns dataset and sorted indices .
resize each image to ` size ` using ` resize_method ` and ` padding_mode ` .
Gets the indices for the most similar images .
Gets the indices for the most similar images in ` ds_type ` dataset
specified by ` hook ` , applies ` pool ` of dim ` pool_dim ` and concatenates
Computes the similarity function between each embedding of ` t1 ` and ` t2 ` matrices .
Returns the ` n ` largest indices from a numpy array ` arr ` .
return the indexes in pairs ordered by highest similarity .
specified file name ` img ` .
Return a Button widget with specified ` handler ` .
Return a Dropdown widget with specified ` handler ` .
Make a horizontal box with ` children ` and ` layout ` .
Make a vertical box with ` children ` and ` layout ` .
Create a list of images , filenames and labels but first removing files that are not supposed to be displayed .
moving from parent dir with old label ` class_old ` to parent dir with new label ` class_new ` .
flagged images and renders next batch .
Flag this image as delete or keep.
Create and format widget set.
Check if current batch contains already deleted images .
Re-render Jupyter cell for batch of images.
Shift the line i of ` x ` by p-i elements to the left , is ` mask ` puts 0s on the diagonal .
Split a RNN ` model ` in groups for differential learning rates .
Reset the internal memory .
Make report in form of two notebooks.
have parameters in single precision .
Creates a fp32 copy of model parameters and sets optimizer parameters
Start coverage reporting in kernel.
Finish coverage reporting in kernel.
Returns the coverage object of pytest-cov .
Create a suffix for nbval data file depending on pytest-cov config .
Merge nbval coverage data into pytest-cov data.
Yield successive `n`-sized chunks from `l`.
raises exception if not convertible
Return `True` if `a` is one-dimensional
sorted unique values of ` x ` .
List of label subdirectories in imagenet-style `folder`.
Given ` arrs ` is [ a , b , ... ] and ` mask ` index - return [ ( a [ mask ] , a [ ~mask ] ) , ( b [ mask ] , b [ ~mask ] ) , ... ] .
creating validation set .; split ` arrs ` with ` valid_pct ` ratio .
Make `p` listy and the same length as `q`.
snake style .
start ` to ` stop ` in ` n ` steps .
Extract the keys in ` names ` from the ` kwargs ` .
iterables ` a ` in equal parts of size ` sz `
Split data in `a` equally among `n_cpus` cores
Categorifies the columns `col_names` in `df`.
dest ` unless it exists and not ` overwrite ` .
Return `Path(path)/Path(fname)`, `path` defaults to current dir.
Join `path` to every file name in `fnames`.
Return `ndarray` of `str` of lines of text from `path`.
Save in `fname` the content of `texts`.
Return the column indexes of ` names ` in ` df ` .
One-hot encode `x` with `c` classes.
Return the slice of ` a ` corresponding to ` idxs ` .
Return the arguments of ` func ` .
kwargs ` between those expected by ` func ` and the others .
are passed to ` np.array ` with ` dtype ` .; handles generators .
Put the texts in ` items ` in an HTML table , ` widths ` are the widths of the columns in % .
Call ` func ` on every element of ` arr ` in parallel using ` max_workers ` .
kwargs ` passed to ` fig.suptitle ` with ` title `
Return the representation of the first ` n_max ` elements in ` items ` .
return a tmp filename , optionally at a specific path .; done with it .
want to customize the way this ` ItemBase ` is shown on ` ax ` .
Init layer parameters.
Create a seuence Conv2d- > BatchNorm2d- > LeakyReLu layer .
has ` num_blocks ` ` ResLayer `
Create a Learner for collaborative filtering on ` data ` .
Create a ` DataBunch ` suitable for collaborative filtering from ` ratings ` .
based on ` is_item ` ) for all in ` arr ` .; cpu ` and no grad . )
Draws a representation of a random forest in IPython .
Gets a random sample of n rows from df , without replacement .
converts a column of df from a datetime64 to many columns containing
Change any columns of strings in a panda's dataframe to a column of
using trn as
missing data in a column of df with the median , and add a { name } _na column
's integer codes .
takes a data frame df and splits off the response variable , and
give each tree a random sample of
produced by set_rf_samples .
assigned variables .
Execute notebook `fname` with `metadata` for preprocessing.
Create the documentation notebook for module ` mod_name ` in path ` dest_path `
given ` path_dir ` and return all the modules contained inside except those in ` exclude `
Read a notebook in ` fname ` and return its corresponding json
Build a dictionary containing the position of the ` cells ` .
Create documentation links for all cells in markdown with backticks.
Return the position to insert a given function doc in a notebook .
Update the ` pos_dict ` by moving all positions after ` start_key ` by ` nbr ` .
Insert the function doc `cells` at their correct position and updates `pos_dict`.
jekyll metadata for given notebook path .
gives notebook imports priority; sorted by submodules > top level modules > manual imports .
Update the documentation notebook of a given module .
be a directory or a file .; Assume all modules reside in the fastai directory .
Return a dropout mask of the same type as ` x ` , size ` sz ` , with probability ` p ` to cancel an element .
Convert a value ` x ` from 0 to 1 ( inclusive ) to an RGBA tuple according to ` cmap ` times transparency ` alpha_mult ` .
Apply dropout to the raw weights .
Return one hidden state.
Reset the hidden states .
Calculate the intrinsic attention of the input w.r.t to an output ` class_id ` , or the classification given by the model if ` None ` .
Create a tabulation showing the first ` k ` texts in top_losses along with their prediction , actual , loss , and probability of
Initialize the schedulers for training .
Take a step in lr , mom sched , start next stepper when the current one is complete .
pass multiple vector elements directly .
put them on the CPU if ` cpu=True ` .
wrapped data .
Recursively map lists of tensors in `b ` to the cpu.
Recursively map lists of tensors in `b ` to FP16.
put ` b ` on ` device ` .
tensor data .
is not set return ` requires_grad ` of first param , else set ` requires_grad ` on all params as ` b `
Return list of trainable params in `m`.
Return the children of ` m ` and its direct parameters not registered in modules .
according to the indexes in ` idxs ` .
according to the layers in ` splits ` .
Separate the parameters in ` layer_groups ` between ` no_wd_types ` and bias ( ` bias_types ` ) from the rest .
Set bn layers in eval mode for all recursive children of `m`.
is batchnorm do n't use half precision .
set ` bias ` to 0 .
Initialize the non-batchnorm layers of ` m ` with ` init_func ` .
Initialize all non-batchnorm layers of ` m ` with ` init_func ` .
Return the shape of the first weight layer in ` m ` .
Return the torch type corresponding to ` dtype ` .
Tranform numpy array `a` to a tensor of the same type.
Compute PCA of `x` with `k` dimensions.
Grab the ` i ` -th batch in ` x ` , ` batch_first ` stating the batch dimension .
clamped to avoid inf
shape= ` size ` random floats from uniform dist : min= ` low ` , max= ` high ` .
shape= ` size ` random floats from uniform dist : min=log ( ` low ` ) , max=log ( ` high ` ) .
shape= ` size ` random booleans ( ` True ` occuring with probability ` p ` ) .
included ) .
Try to convert ` o ` to int , default to ` o ` if not possible .
Return the model maybe wrapped inside ` model ` .
Check that ` out ` and ` targ ` have the same number of elements and flatten them .
create new OrderedDict that does not contain ` module. `
Return a dictionary for updating ` last_metrics ` with ` mets ` .
Collects iterables lazily, rather than immediately.
Generate documentation for fastai library in HTML (asciidoctor required)
Retrieves new batch of DatasetType , and detaches it .
is caching the result as an optimization .; is extremely slow with large datasets .
Writes gradient statistics to Tensorboard .
Writes training loss to Tensorboard .
Writes model weight histograms to Tensorboard .
Writes single scalar value to Tensorboard .
Writes training metrics to Tensorboard .
writes batch end appropriate data to Tensorboard .
writes backward end appropriate data to Tensorboard .
writes epoch end appropriate data to Tensorboard .
Writes gradient statistics for generator to Tensorboard .
Writes gradient statistics for critic to Tensorboard .
Writes model generated , original and real images to Tensorboard .
Writes model generated , original and real images to Tensorboard
Queues up an asynchronous write request to Tensorboard.
queued up write requests asynchronously to Tensorboard .
convert a batch of model images to a list of ModelImageSet .
Writes single model histogram to Tensorboard .
Writes model histograms to Tensorboard .
Writes a single scalar value for a gradient statistic to Tensorboard .
Writes the average norm of the gradients to Tensorboard .
Writes the median norm of the gradients to Tensorboard .
Writes the maximum norm of the gradients to Tensorboard .
Writes the minimum norm of the gradients to Tensorboard .
Writes the number of zeroes in the gradients to Tensorboard .
Writes the average of the gradients to Tensorboard .
Writes the median of the gradients to Tensorboard .
Writes the maximum of the gradients to Tensorboard .
Writes the minimum of the gradients to Tensorboard .
Writes model gradient statistics to Tensorboard .
Writes list of images as tensors to Tensorboard .
generated and real ( target ) images .
Writes original , generated and real ( target ) images to Tensorboard .
Writes training and validation batch images to Tensorboard .
Writes batch images of specified DatasetType to Tensorboard .
Writes single model graph to Tensorboard .
Writes model graph to Tensorboard .
keep track of a running mean and
detach them from their history .
Returns a SequentialRNN model.
Invoked during the forward propagation of the RNN_Encoder module .
Replace repetitions at the character level in `t`.
Replace word repetitions in `t`.
List of replacements from html strings in `x`.
add ` TK_UP ` before .
add ` TK_MAJ ` before .
Process one text `t` with tokenizer `tok`.
Process a list of `texts` in one process.
Process a list of `texts`.
Convert a list of tokens ` t ` to their ids .
Convert a list of ` nums ` to their tokens .
Create a vocabulary from a set of ` tokens ` .
contained in ` path `
plots loss function as function of iterations.
learning rate in jupyter notebook or console , depending on the enviroment of the learner .
Plots the loss function with respect to learning rate , in log scale .
Plots the lr rate/momentum schedule
is NaN and interrupts training .
Initializes the best value .
Pick the monitored value .
Compare the value monitored to its best score and maybe save the model .
Load the best model.
Initialize inner arguments.
Compare the value monitored to its best and maybe reduce lr .
Convert a notebook ` fname ` to html file in ` dest_path ` .
modified notebooks in ` folder ` to html pages in ` dest_path ` .
adds padding .
Read the text in ` fn ` .
Create the ragged array that will be filled when we ask for items .
overlap ! = 1 has not been implemented
are passed to the dataloader creation .; Create a ` TextDataBunch ` from ids , labels and a ` vocab ` .
are passed to the dataloader creation .; Load a ` TextDataBunch ` from ` path/cache_name ` .
are passed to the dataloader creation .; Create a ` TextDataBunch ` from tokens and labels .
are passed to the dataloader creation .; Create a ` TextDataBunch ` from DataFrames .
are passed to the dataloader creation .; Create a ` TextDataBunch ` from texts in csv files .
Create a ` TextDataBunch ` from text files in folders .
Create a ` TextDataBunch ` in ` path ` from the ` datasets ` for language modelling .
transform the ` datasets ` in a ` DataBunch ` for classification .
A special labelling method for language models.
Get the list of files in ` path ` that have a text suffix .; search subfolders .
Show the ` xs ` ( inputs ) and ` ys ` ( targets ) .; is the maximum number of tokens displayed .
r"""InceptionV4 model architecture from the
is necessary because otherwise the learner method accesses the wrong model when it is called
Distrubuted training of CIFAR-10 .
Set default values for options.
Return the saved feature indexes that will be concatenated
Return a ` Hook ` that stores activations of ` module ` in ` self.stored `
Return `Hooks` that store activations of all `modules` in `self.stored`
Create a dummy batch to go through ` m ` with ` size ` .
Pass a ` dummy_batch ` in evaluation mode in ` m ` with ` size ` .
Pass a dummy input through the model ` m ` to get the various sizes of activations .
Return the number of output features for ` model ` .
hooks ) if ` full `; Pass a dummy input through the model to get the various sizes .
Print a summary of ` m ` using a output text width of ` n ` chars
module ` , ` input ` , ` output ` .
Remove the hook from the model .
Register the `Hooks` on `self.modules`.
Take the mean and std of ` o ` .
Take the stored results and puts it in ` self.stats `
given image files .
Displays the images and their probabilities of belonging to a certain class
Extracts the first 4 most correct/incorrect indexes from the ordered list of probabilities
Extracts the first 4 most uncertain indexes from the ordered list of probabilities
predicted classes which correspond to the selected class ( y ) and to the specific case ( prediction is correct - is_true=True , prediction is wrong - is_true=False )
Plots the images which correspond to the selected class ( y ) and to the specific case ( prediction is correct - is_true=True , prediction is wrong - is_true=False )
predicted classes which correspond to the selected class ( y ) and have probabilities nearest to 1/number_of_classes ( eg .
distributed training launch helper that spawns multiple distributed processes
Add the metrics names to the ` Recorder ` .
Initialize the metrics for this epoch .
Update the metrics if not ` train `
Finish the computation and sends the result to the Recorder .
Create the various optimizers .
Steps through the generators then each of the critics.
Put the various losses in the recorder .
Prepare file with metric names.
Add a line with ` epoch ` number , ` smooth_loss ` and ` last_metrics ` .
Return two lists, one for the model parameters in FP16 and one for the master parameters in FP32.
Copy the ` model_params ` gradients to ` master_params ` for the optimizer step .
model_params ` .
Prepare the master model .
prevent underflow .
divide them by the scale .
Update the params from master to model and zero grad .
is of size targ .
x by a factor of z+1 while retaining the original image size and proportion .
retaining the original image size and proportion .
Perform any of 8 permutations of 90-degrees rotations or flips for image x.
Adjust image balance and contrast
Return a squared resized image
Return a center crop of an image
returns a squared resized image of size targ
Cut out n_holes number of square holes of size length in image at random locations .
scaling with aspect ratio
crop image into a square of size sz,
Convert mask YY to a bounding box, assumes 0 as background nonzero object
Transforming coordinates to pixels .
Apply a collection of transformation functions : fns : to images
Generate a standard set of transformations
Given the statistics of the training image sets , returns separate training and validation transform functions
Returns separate transformers of images for training and validation.
are images .; filter to ` image_extensions ` .
Open a COCO style json in ` fname ` and returns the lists of filenames ( with maybe ` prefix ` ) and labelled bboxes .
labelled bboxes and adds padding with ` pad_idx ` .
Normalize `x` with `mean` and `std`.
Denormalize `x` with `mean` and `std`.
`b` = `x`,`y` - normalize `x` array of imgs and `do_y` optionally `y`.
func using ` mean ` and ` std ` , can specify ` do_y ` and ` device ` .
flatten remaining axes
listed in text file ` urls ` to path ` dest ` , at most ` max_pics `
resize to , to hit ` targ_sz ` at same aspect ratio , in PIL coords ( i.e w * h )
Check if the image in ` file ` exists , maybe resize it and copy it in ` dest ` .
Check if the images in ` path ` are n't broken , maybe resize them and copy it in ` dest ` .
Call ` train_tfm ` and ` valid_tfm ` after opening image , before converting from ` PIL.Image `
Resize images to ` size ` using ` RandomResizedCrop ` , passing along ` kwargs ` to train transform
Create an `ImageDataBunch` from `LabelLists` `lls` with potential `ds_tfms`.
dataset in ` path ` with ` train ` , ` valid ` , ` test ` subfolders ( or provide ` valid_pct ` ) .
Create from a `DataFrame` `df`.
Create from a csv file in `path/csv_labels`.
Create from list of `fnames` in `path`.
Create from list of `fnames` in `path` with `label_func`.
Create from list of `fnames` in `path` with re expression `pat`.
used for inference .
Grab a batch of data and call reduction function ` func ` per channel
using ` stats ` ( defaults to ` DataBunch.batch_stats ` )
Open image in `fn`, subclass and overwrite for custom behavior.
Get the list of files in ` path ` that have an image suffix .; search subfolders .
Get the filenames in ` cols ` of ` df ` with ` folder ` in front of them , ` suffix ` at the end .
Get the filenames in ` path/csv_name ` opened with ` header ` .
Show the ` xs ` ( inputs ) and ` ys ` ( targets ) on a figure of ` figsize ` .
Show `xs` (inputs), `ys` (targets) and `zs` (predictions) on a figure of `figsize`.
add ` background ` .
is not passed , currently selected torch device is used; get total , used and free memory ( in MBs ) for gpu ` id ` .
get [ gpu_id , its_free_ram ] for the first gpu with highest available RAM
runs ` GPUMemTrace ` w/ report on func
modify the data type
depending on this and constructor arguments
Put ` learn ` on distributed training with ` cuda_id ` .
Constructs a XResNet-18 model .
Constructs a XResNet-50 model .
call out to callbacks as necessary .
using ` dl ` , max batches ` n_batch ` .
Calculate `loss_func` of `model` on `dl` in evaluation mode.
using optim ` opt ` and loss function ` loss_func ` .
learn using ` loss_func ` and ` opt ` .
be file-like ( file or buffer ); Load a ` Learner ` object saved with ` export_state ` in ` path/file ` with empty data , optionally add ` test ` and load on ` cpu ` .
beginning of training .
beginning of batch .
has a chance to modify it .
Save epoch info: num_batch, smooth_loss, metrics.
Format stats before printing.
Add `names` to the inner metric names.
learning rate , ` show_moms ` to include momentum .
learning rate and losses , trimmed between ` skip_start ` and ` skip_end ` .
Plot training and validation losses.
collected during training .
annotated with ` Param ` ) in func and return an ` ArgumentParser `
create a simple CLI from ` func ` using ` anno_parser `
create a simple CLI from ` func ` using ` plac `
Takes in text tokens and returns int2tok and tok2int converters
is greater than 1 and you save previous xs , you must reset at the beginning of each new sequence .
Start a new kernel , and return its Manager and Client
KernelSpec ` instance for the given kernel_name .
is used to get a message from the iopub channel .
Executes a string of python code in cell input .
poll the kernel 'shell ' stream for messages until :
is received for the given parent ID
Instructs the kernel process to stop channels
Get a list of index values for Validation set from a dataset
shrink a single image to scale , such that the smaller of the height or width dimension is equal to targ .
shrink a set of images in the same directory to scale , such that the smaller of the height or width dimension is equal to targ .
path ` for all files within ` folder `
associated by extrapolation of directory names .
are 0 , except for the indecies in ids; encoding by index .
Returns the filenames and labels for a folder within a path
Parse filenames and label sets from a CSV file.
True if the fn points to a DICOM image
Opens an image using OpenCV given the file path .
passed as * a , to a pair of arrays like this ( elements selected by idxs , the remaining elements )
resize all images in the dataset and save them to ` new_path `
Reverse the normalization done to a batch of images .
Return a copy of this dataset resized
given size .
Read in images and their labels given as numpy arrays
Read in images and their labels given as sub-folder names
Read in images and their labels given as a CSV file .
Read in images given a sub-folder and their labels given a numpy array
Is the code running in the ipython environment ( jupyter including )
avoid circular reference leading to gc.collect ( ) unable to reclaim memory
happened , or execution was interrupted
Fits a model
Computes the loss on the next minibatch of the validation set .
Create link to documentation.
Check if ` t ` belongs to ` module_name ` .
function param to ` param1 : Type=val ` .
link ` func ` definition to show in documentation
Formatted enum documentation .
Supported types : class , Callable , and enum .
Show `show_doc` info in preview window along with link to full docs.
format the docstring definition with ` arg_comments ` and ` alt_doc_string ` .
docstring ` for backticks and attempt to link those functions to respective documentation .
matching from last component .; resolve keywords such as Learner.lr_find .
Return module from `mod_name`.
see ` show_doc ` .
Return all the functions of module `mod`.
List the inner functions of a class .
given ` mod_name ` .
notebook documentation of ` ft ` .; link to source code
link to pytorch docs of ` ft ` .
Returns github link for given file
link to ` ft ` in source code .
affect nbval 's behaviour
outputs with shared names into single streams
having key be the stream name
Trim and hash base64 strings
Intent each line with indent
Called by pytest to setup the collector cells in .
were specified as command line options
provided by the user on the command line .
Gets a message from the iopub channel of the notebook kernel .
is required by pytest and is used to yield pytest
called when self.runtest ( ) raises an exception .
Format an output for printing
sanitize a string for comparison .
Computes the outputs for several augmented inputs for TTA
predict on ` ds_type ` dataset .
Computes the f_beta between ` preds ` and ` targets `
is bs * n_classes .
y_pred ` and ` y_true ` are the same size .
Computes the Top-k accuracy ( target is in the top k predictions ) .
iou metric , classic for segmentation problems .
Exp RMSE between `pred` and `targ`.
Mean absolute error between `pred` and `targ`.
squared error between ` pred ` and ` targ ` .
squared logarithmic error between ` pred ` and ` targ ` .
Explained variance between ` pred ` and ` targ ` .
R2 score (coefficient of determination) between `pred` and `targ`.
Using trapezoid method to calculate the area under roc curve
Returns the false positive and true positive rates
convert iterable object into numpy array
Convert numpy array into a pytorch tensor.
create_variable , which creates a pytorch tensor
creates a single or a list of pytorch tensors , depending on input x .
given an input of np.array , list , tuple , torch variable or tensor .
pytorch variable to gpu , if cuda is available and USE_GPU is set to true .
returns sequence pieces , seperated by indexes specified in idxs .
iterables a in equal parts of size sz
yields chunks of iterable , chunk_size at a time .
Apply `change` in brightness of image `x`.
Rotate image by `degrees`.
height - ` c ` , ` r ` focus col , row .
Zoom image by `scale`.
Squish image by `scale`.
Replace pixels by random neighbors at `magnitude`.
Flip `x` horizontally.
based on ` k ` .
padding ` pixels .
Cut out ` n_holes ` number of square holes of size ` length ` in image at random locations .
Randomize one of the channels of the input image
Crop `x` to `size` pixels.
pad tfm - ` row_pct ` , ` col_pct ` sets focal point .
Fixed `mode` `padding` and random crop of `size`
Randomized version of ` zoom ` .
Randomized version of ` crop_pad ` .
Randomly zoom and/or crop.
mentioned [ here ] ( https : //web.archive.org/web/20150222120106/xenia.media.mit.edu/~cwren/interpolator/ ) .
Transform `coords` with `coeffs`.
c ` ` FlowField ` .
c ` .
Tilt `c` field with random `direction` and `magnitude`.
create a list of flip , rotate , ` zoom ` , warp , lighting transforms .
compute zoom/squish matrix .
resize and crop the image to a ratio in ` ratios ` after a zoom of ` max_scale ` .
Sets the learning rate to the initial LR decayed by 10 every 30 epochs
does some preparation before finally delegating to the 'fit ' method for
returns an instance of the LayerOptimizer class , which
gets an instance of LayerOptimizer and delegates to self.fit_gen ( .. )
find an optimal learning rate for a model .
does n't do; helps find the best learning rate .
Args:
Predict with Test Time Augmentation (TTA)
send them to model.fit ( .. )
Save the extra outputs for later and only returns the true output .
last_loss ` .
go with a new vocabulary .
Create a language model from ` arch ` and its ` config ` , maybe ` pretrained ` .
Create a ` Learner ` with a language model from ` data ` and ` arch ` .
Create a text classifier from ` arch ` and its ` config ` , maybe ` pretrained ` .
Create a ` Learner ` with a text classifier from ` data ` and ` arch ` .
Save the encoder to ` name ` inside the model directory .
Load the encoder `name` from the model directory.
Load a pretrained model and adapts it to the data vocabulary .
depending on ` ds_type ` .
Return the ` n_words ` that come after ` text ` .
Return the ` n_words ` that come after ` text ` using beam search .
Show `rows` result of predictions on `ds_type` dataset.
Concatenate the ` arrs ` along the batch dimension .
initialized depending on ` norm_type ` .
followed by ` actn ` .
initialize a ` nn.Conv1d ` layer with spectral normalization .
initialize ` nn.Conv2d ` layer .; ks//2 ` .
Create `nn.ConvTranspose2d` layer.
Return a relu activation , maybe ` leaky ` and ` inplace ` .
Create a sequence of convolutional ( ` ni ` to ` nf ` ) , ReLU ( if ` use_activ ` ) and batchnorm ( if ` bn ` ) layers .
are passed to ` conv_layer ` .
Sigmoid function with range `(low, high)`
ICNR init of `x`, with `scale` and `init` function.
flattens input and target .
defined by ` actns ` , ` kernel_szs ` and ` strides ` , plus batchnorm if ` bn ` .
Truncated normal initialization .
embedding layer .
Prepare MLflow experiment and log params
Send loss and metrics values to MLFlow after each epoch
Store the notebook and stop run
torch style image tensor .
numpy/matplotlib style .
bounding box points from ( width , height , center ) to ( height , width , top , left ) .
TensorImageSize ` to ( height , width ) of an image .
bounding box onto image ` Patch ` .
bounding box on ` ax ` .
created from image in file ` fn ` .
divides pixel values by 255 .
encoded string in ` mask_lre ` with size in ` shape ` .
encoding string from ` img ` .
encoded string ` mask_rle ` with ` shape ` .
Display `Image` in notebook.
-1/1 or the image size depending on ` to_unit ` .
Resample pixels in `coords` from `x` by `mode`, with `padding_mode` in ('reflection','border','zeros').
adjust for rectangular shaped ` c ` .
described in ` m ` to ` c ` .
Calc `x` to nearest multiple of `mult`.
Calc crop shape of `target_px` to nearest multiple of `mult`.
fit in ` crop_target ` - adjust based on ` do_crop ` .
Shortcut for `enumerate(subplots.flatten())`
Call ` func ` for every combination of ` r , c ` on a subplot
Call ` func ( i , j ) .show ( ax ) ` for every combination of ` r , c `
using ` r ` rows
do_resolve ` picks value for random args .
have been sent to the ` Image ` .
Save the image to ` fn ` .
applying queued affine transforms .
Equivalent to `image = sigmoid(func(logit(image)))`.
image.px = func ( image.px ) ` .
image.flow = func ( image.flow , image.size ) ` .
image.affine_mat = image.affine_mat @ func ( ) ` .
Resize the image to ` size ` , size can be a single int .
Get the affine matrix that will be applied by ` refresh ` .
using ` cmap ` if single-channel , overlaid with optional ` y `
Show the ` ImageSegment ` on ` ax ` .
Mimic the behavior of torch.clone for `ImagePoints` objects.
applying queued affine and coord transforms .
Put ` func ` with ` args ` and ` kwargs ` in ` self.flow_func ` for later .
self = func_flow ( self ) ` .
Return the points associated to this object .
Show the ` ImagePoints ` on ` ax ` .
Mimic the behavior of torch.clone for `Image` objects.
Create an ImageBBox object from `bboxes`.
Show the ` ImageBBox ` on ` ax ` .
wrapping it if necessary .
Change `url` to a path.
model path to ` filename ` , checking locally first then in the config file .
filename ` , checking locally first then in the config file .
destination ` fname ` .
fname ` if ` dest ` does n't exist , and un-tgz to folder ` dest ` .
Get the path to ` key ` in the config file .
Retrieve the ` Config ` in ` fpath ` .
Creates a ` Config ` from ` fpath ` .
mixup to ` last_input ` and ` last_target ` if ` train ` .
is of the right date type .
produced by ` cyclic_dt_features ` .
Calculate the cos and sin of date/time cycles .
adds trigonometric date/time features to a date in the column ` field_name ` of ` df ` .
adds columns relevant to a date in the column ` field_name ` of ` df ` .
returns column names of cont and cat variables from given df .
Transform `self.cat_names` columns in categorical.
Compute the means and stds of ` self.cont_names ` columns to normalize them .
depending on ` classes ` if not given in ` sz_dict ` .
Get a ` Learner ` using ` data ` , with ` metrics ` , including a ` TabularModel ` created using the remaining params .
are passed to ` DataBunch.create ` .; Create a ` DataBunch ` from ` df ` and ` valid_idx ` with ` dep_var ` .
Get the list of inputs in the ` col ` of ` path/csv_name ` .
Return the default embedding sizes suitable for this data or takes the ones in ` sz_dict ` .
Show the ` xs ` ( inputs ) and ` ys ` ( targets ) .
Load the classifier and int to string mapping
Numpy Softmax, via comments on https://gist.github.com/stober/1946926
Do the actual prediction on the text using the
Loads a model and produces predictions on arbitrary input .
Makes a W3C alwaysMatch capabilities object.
Overrides the current file detector (if necessary) in limited context.
Creates a new session with the desired capabilities .
Creates a web element with the specified ` element_id ` .
Sends a command to be executed by a command.CommandExecutor .
Finds an element by id.
Finds multiple elements by id.
Finds an element by xpath.
Finds multiple elements by xpath.
Finds an element by link text.
Finds elements by link text.
Finds an element by a partial match of its link text.
Finds elements by a partial match of their link text.
Finds an element by name.
Finds elements by name.
Finds an element by tag name.
Finds elements by tag name.
Finds an element by class name.
Finds elements by class name.
Finds an element by css selector.
Finds elements by css selector.
Executes JavaScript in the current window/frame .
Quits the driver and closes every associated window .
Returns the handle of the current window .
Returns the handles of all windows within the current session .
Maximizes the current window that webdriver is using
Get a single cookie by name .; Returns the cookie if found , None if not .
wait for an element to be found ,
wait during an
wait for a page load to complete
given a By strategy and locator .; Prefer the find_element_by_ * methods when
Prefer the find_elements_by_ * methods when; given a By strategy and locator .
Saves a screenshot of the current window to a PNG image file .
Sets the width and height of the current window .
Gets the width and height of the current window .
Sets the x , y position of the current window .
Gets the x , y position of the current window .
Sets the x , y coordinates of the window as well as height and width of
Set the file detector to be used when sending keyboard input .
Sets the current orientation of the device
does not have an error .
Calls the method provided with the driver as an argument until the \
Quits the driver and close every associated window .
Sets the context that Selenium commands are running in using
Installs Firefox addon.
Sets location of the browser binary, either by string or
use , either by string
Sets the headless argument
Marshals the Firefox options to a `moz:firefoxOptions`
Set the network connection for the remote device.
Unzip zipfile to a temporary directory.
given element .
given coordinates .
held tap to specified location .
issued tap 'and hold ' command at specified location .
moving by xoffset and yoffset .
starting at on_element , moving by xoffset and yoffset .
Long press on an element.
starting anywhere on the screen .
starting at on_element , and moving by the xoffset and yoffset
Creates a capabilities with all the options that have been set and
Returns the element with focus , or BODY if nothing has focus .
focus to the specified frame , by index , name , or webelement .
Switches to a new top-level browsing context.
focus to the specified window .
Performs all stored actions.
are already stored locally and on the remote end
Holds down the left mouse button on an element.
Performs a context-click (right click) on an element.
Double-clicks an element.
Holds down the left mouse button on the source element,
Sends a key press only , without releasing it .
Moving the mouse to an offset from current mouse position .
Moving the mouse to the middle of an element .
Move the mouse by an offset of the specified element .
Pause all inputs for the specified duration in seconds
Releasing a held mouse button on an element .
Sends keys to current focused element.
Sends keys to an element.
Sets the options Browser Attach Timeout
Sets the options Element Scroll Behavior
Sets the options File Upload Dialog Timeout value
Marshals the IE options to the correct object.
be loaded into chrome
Adds the path to the extension to a list that will be used to extract it
Creates a capabilities with all the options that have been set
Looks up an element.
Gets the text of the Alert .
Dismisses the alert available .
Accepts the alert available.
Send Keys to the Alert.
Sets the port that WebDriver will be running on
encoded string of profile directory
writes the current user prefs dictionary to disk
addon from a filepath , url
Returns a dictionary of details about the addon.
Submits a form .
Gets the given property of the element .
Gets the given attribute or property of the element .
typing into the element .
is visible to a user .
Use this to discover
The size of the element.
The location of the element in the renderable canvas.
A dictionary with the size and location of the element.
Saves a screenshot of the current element to a PNG image file .
Executes a command against the underlying HTML element .
Closes the browser and shuts down the WebKitGTKDriver executable
Starts the Service .
Stops the service.
Launches the browser for the given profile name .
Kill the browser.
is connectable in the firefox .
Return the command to start firefox .
Returns the fully qualified path by searching Path of the given
Get headers for remote request.
Send a command to the remote server .
Send an HTTP request to the remote server .
selected options belonging to this select tag
have a value matching the argument .; is , when given `` foo '' this
is done by examing the `` index '' attribute of an; Select the option at the given index .
is , when given `` Bar '' this; display text matching the argument .
selected entries .; is only valid when the SELECT supports multiple selections .
Deselect the option at the given index .; is done by examing the `` index '' attribute of an
Uploads a file to Google Cloud Storage.
Runs the OAuth 2.0 installed application flow .
autodetect setting .
ftp proxy setting .
http proxy setting .
Sets noproxy setting.
proxy autoconfig url setting .
https proxy setting .
proxy setting .
socks proxy username setting .
socks proxy password setting .
Adds proxy information as capability in specified capabilities.
Resolve a hostname to an IP , preferring IPv4 addresses .
Joins a hostname and port together .
connect to the server at port to see if it is running .
connect to the HTTP server at /status path
Processes the values that will be typed in the element .
saving the current state as a displaCy
Get the tokens from the original Doc that are also in the comparison Doc .
files into JSON format for use with train cli .
Render displaCy visualisation.
Serve displaCy visualisation.
Generate dependency parse in {'words': [], 'arcs': []} format.
named entities in [ { start : i , end : i , label : 'label ' } ] format .
is called around the generated
Evaluate a model .; render a sample of parses in a HTML file , set an
Create a symlink for models within the spacy/data directory .
installed version of spaCy is compatible
Profile a spaCy pipeline , to find out which functions take the most time .
add a field to the POS tag for UD mapping .
based on Janome .
Used for model shortcut links .; Create a symlink .
Used for model shortcut links .; Remove a symlink .
Check if a specific configuration of Python version and operating system
Used to load models from a directory .
compiling patterns that have ranges
Import and load a Language class.
Load a model from a shortcut link , package or data path .
Load a model from a shortcut link , or directory in spaCy data path .
Load a model from an installed package .
Load a model from a data directory path .
use in the ` load ( ) ` method of a model package 's
validate its contents .
Get the path to an installed package .
registered entry points from other packages for a given key , e.g .
Check if registered entry point is available for a given name and
Check if user is running spaCy from a Jupyter notebook by detecting the
Compile a sequence of suffix rules into a regex object.
Compile a sequence of infix rules into a regex object.
overwrite exceptions .
replace string .
be an iterator ,
compounding values .
step from a start value to a
decaying values .
given number of words .
works by holding ` bufsize ` items back
given JSON schema ( see https : //json-schema.org ) .
validate serialization args and manage transition from
present in the match patterns .
Get all patterns that were added to the entity ruler .
be a token; patterns to the entitiy ruler .
Load the entity ruler from a bytestring.
Load the entity ruler from a file.
Save the entity ruler patterns to a directory .
Read the CONLLU format into ( Doc , GoldParse ) tuples .
Get out the annoying 'tuples ' format used by begin_training , given the
resembles a number
serialized binders into one byte string .
Add a doc 's annotations to the binder for serialization .
objects from the annotations , using the given vocab .
Extend the annotations of this binder with the annotations from another .
Serialize the binder 's annotations into a byte string .
Deserialize the binder 's annotations from a byte string .
Load data from the IMDB dataset.
including meta and required
Check whether we 're dealing with an uninflected paradigm , so we can
train the new entity .
format into JSON format for use with
Create a new model , set up the pipeline and train the tagger .
Create a new model from raw data , like word frequencies , Brown clusters
Handle .gz, .tar.gz or unzipped files
set up the pipeline and train the entity recognizer .
Pre-train the 'token-to-vector ' ( tok2vec ) layer of pipeline components ,
Perform an update over a single batch of documents .
Compute a mean-squared error loss between the documents ' vectors and
add an output layer onto; Define a network for the pretraining .
Round large numbers as integers, smaller numbers as decimals.
Detect base noun phrases.
Reused in Doc , Token and Span .
Check if an extension attribute is writable .
Check whether we 're on OSX > = 10.10
indicating the position of the word in the document .
using pip .
files into JSON format for use with train command and other
Load a specific spaCy model
Load a generic spaCy model and add the sentencizer for sentence tokenization
Turn a list of errors into frequency-sorted tuples thresholded by a certain total number
determine whether the treebank has blinded texts or not
Fetch the txt files for all treebanks for a given set of languages
Run an evaluation of a model nlp on a certain specified treebank
specified models and treebanks
run evaluations with .
Detect base noun phrases from a dependency parse.
Wrap a model that should run on CPU , transferring inputs and outputs
Build a simple CNN text classifier , given a token-to-vector model as inputs .
Compose two or more models `f`, `g`, etc, such that their outputs are
Convert a model into a BERT-style masked language model
using width from tensorizer in pipeline .
Render complete markup.
Render SVG.
Render individual word.
Render individual arrow.
Render individual arc.
Render individual arrow head.
height `` levels '' .
Render entities in text.
Merge noun chunks into a single token.
Merge entities into a single token.
subtokens into a single token .
be formatted in spaCy 's; update a spaCy model .
mean score between tasks in pipeline that can be used for early stopping .
Load pre-trained weights for the 'token-to-vector' part of the component
Convert conllu files into JSON format for use with train cli.
Check the 10th column of the first token to determine if the file contains
obtained from the dataset in order to follow Wikipedia
Print info about spaCy installation.
Print data in GitHub-flavoured Markdown format for issues etc.
set up the pipeline and train the parser .
Get a pipeline component for a given component name .
Create a pipeline component from a factory .
Add a component to the processing pipeline .
Replace a component in the pipeline .
Rename a pipeline component .
Remove a component from the pipeline .
Update the models in the pipeline .
Make a `` rehearsal '' update to the models in the pipeline , to prevent
be called before training to pre-process gold data .
acquire a trainer and
training a pre-trained model .
provided in the
Process texts as a stream, and yield `Doc` objects in order.
is loaded , this; Save the current state to a directory .
Loads state from a directory.
Serialize the current state to a binary string .
Load state from a binary string.
was created .
Yields all available rules.
import paths .
enabled rules .
sorted commands without duplicates .
corrected commands .
Used when ` thefuck ` called without arguments .
command output from shell logger .
Returns list of history entries.
using shell-like syntax .
Return a shell-escaped version of the string s .
Returns the name and version of the current shell
shell history .
get brew default commands on local environment
get tap 's specific commands
git aliases and supports testing for both git and hub .
Yields actions for pressed keys.
Returns:
Create a spawned process .
Logs shell output to the `output`.
Get output of the script.
Adds arguments to parser .
's too dangerous to use ` -y ` and ` -r ` together .
Prepares arguments by:
Get custom npm scripts.
Fills `settings` with values from `settings.py` and env.
representing the user config resource
create it when it does n't exist .
Loads settings from file.
list from env-string to python .
priority pairs from env .
Transforms env-strings to python.
Loads settings from env.
Loads settings from args.
is wrong first argument will be destination .
sudo before calling fn and adds it after .
kill the process otherwise just logs a debug message , the
get output of the command in the
Runs the script and obtains stdin/stderr .
Reads script output from log.
Gets the packages that provide the given command using ` pkgfile ` .
given parent directory
rebuild the path string by spellchecking the directories .
Returns new command with replaced fields.
Creates instance of `Command` from a list of script parts.
rule instance from path .
Returns `True` when rule enabled.
matches the command .
Returns generator with corrected commands.
fixed commands script .
command from rule for passed command .
Returns parent process pid.
shell pid to tracker file .
know that ` fuck ` called second time .
Returns `True` when alias already in shell config.
alias to shell config .
Shows useful information about how-to configure alias on a first run
Caches previous calls to the function.
Adds default values to settings if it not presented.
Returns closest match or just first from possibilities.
controle argument ` n ` .
command line argument .
Helper for *_no_command rules.
is call to one of passed app names .
matching script is for on of app names .
Caches function result in temporary file.
Creates single script from a list of script parts.
given observations information , and takes them in environment .
Executes model.
Gets current model step.
Saves the model
saved model to .nn format for Unity embedding .
Gets the list of the output nodes present in the graph for inference
Resets all the local local_buffers
Appends the buffer of an agent to the update buffer .
Appends the buffer of all agents to the update buffer .
training session .
Get an action using this trainer 's current policy .
training statistics to Tensorboard .
text to Tensorboard .
A dict from brain name to the brain's curriculum's lesson number.
Attempts to increments all the lessons of all the curriculums in this
Sets all the curriculums in this meta curriculum to a specified
Get the combined configuration of all curriculums in this
Sends a signal to reset the unity environment .
moves the environment dynamics forward accordingly ,
Converts arrays to list.
experience information from all external brains in environment at current step .
is done .
step in environment .
has started .
Write Training Metrics to CSV
ops to track and increment recent average cumulative reward .
Creates state encoders for current and future observations.
inverse model TensorFlow ops for Curiosity module .
model TensorFlow ops for Curiosity module .
Creates training-specific Tensorflow ops for PPO models .
Evaluates policy for the agent experiences provided.
model using buffer .
used for Curiosity-based training .
Generates value estimates for bootstrapping .
reward value for policy .
Adds experiences to each agent's experience history.
processing condition , and processes them as necessary .
be reset .; has ended .
Updates the policy .
ops to track and increment global training step .
Creates image input op.
Creates ops for vector observation input.
Builds a set of hidden state encoders.
Builds a set of visual (CNN) encoders.
Creates a masking layer for the discrete actions
encoding stream for observations .
Builds a recurrent encoder for either state or observations (LSTM).
Creates Continuous control actor-critic model .
control actor-critic model .
see REF .
be either single string or list of strings
Preserves the order of elements in the list
Convert from NHWC|NCHW => HW
Converts a TensorFlow model into a Barracuda model.
uses it to fill training buffer .
parses a demonstration file .
checkpoint folder .
Write all CSV metrics
saved models to .nn format for Unity embedding .
Initialization of the trainers
Resets the environment .
Sends a shutdown signal to the unity environment , and closes the socket connection .
] ) ;
- Ht = f(Xt*Wi + Ht_1*Ri + Wbi + Rbi)
- zt = f(Xt*Wz + Ht_1*Rz        + Wbz + Rbz)
Full:
update on model .
depending on the progress given .
correspond to the lesson .
generalized advantage estimate for use in updating policy .
Updates the last reward
Constructs a BrainInfo which contains the most recent previous experiences for all agents info
has enough elements to run update model
demonstration_buffer to update the policy .
returns an initial observation .
Run one timestep of the environment's dynamics.
Creates a Dict that maps discrete actions ( scalars ) to branched actions ( lists ) .
Creates the GRPC server .
bind to the requested communicator port , checking if it is already in use .
Sends a shutdown signal to the unity environment , and closes the grpc connection .
byte array observation image into numpy array , re-sizes it ,
Converts list of agent infos to BrainInfo.
Converts brain parameter proto to BrainParameter object.
Creates a new , blank dashboard and redirects to it in edit mode
tags a given object has .
Add new tags to an object.
Remove tags from an object.
Imports the datasource from the object to the database .
Run migrations in 'online' mode.
based on the query object
Building a query object
is made out of the key/values in ` query_obj ` , plus any
is the data object serialized to the js layer
Returns the query object for this visualization
Returns the chart data
Compute the partition at each ` level ` from the dataframe .
Nest values at each level on the back-end with
sent to the frontend
Update ORM one-to-many list from object list
Update datasource from a data structure
Converting metrics to numeric when pandas.read_sql can not
Returns a payload of metadata and data
caching around the df paylod retrieval
used to render slice in templates
Creates :py:class:viz.BaseViz object from the url_params_multidict.
slc in the database .
Imports the dashboard from the object to the database .
Get the effective user , especially during impersonation .
Generates a `` select * `` statement in the proper dialect
need to be passed as keyword arguments .
Allowing to lookup grain by either label or duration
log user actions
label an endpoint as an API .
Use it after the @ api decorator above; catch superset exceptions .
be used in ` pre_update ` hooks on models to enforce ownership
Customize how fields are bound by stripping all whitespace .
sent to the client
implement diferent logic
Returns a set of tuples with the perm name and view menu name
Returns the details of view_menus for a perm name
Destroy a driver
Given a schedule , delivery the dashboard as an email report
Given a schedule , delivery the slice as an email report
Find all active schedules and schedule celery tasks for
meant to be invoked hourly
De-duplicates a list of string by suffixing a counter
Given a numpy dtype , Returns a generic database type
metadata about columns for data visualization .
Getting the time component of the query
Convert datetime object to a SQL expression string
Takes a sql alchemy column object and adds label info if supported by engine .
Runs query against sqla to retrieve some
Apply config's SQL_QUERY_MUTATOR
Turn an adhoc metric into a sqlalchemy column.
Querying any sqla table from this common interface
merges it in
Loading lat/long data from a csv file in the repo
column info from the source system
Returns a list of non empty values or None
has access to the database or all datasource
owned by current user if
redirect to explore view after saving
Get/cache a language pack
Build `form_data` for chart GET request from dashboard's `default_filters`.
warming up a given chart/table cache .
Warm up cache.
Retrieve the logs produced by the execution of the query .
Refresh metadata of all datasources in the cluster
Fetches metadata for the specified datasources and
based on the column metadata
stored in the db .
specified as ` postagg ` returns the
Return a list of metrics that are post aggregations
Retrieve some values for the given column
aggregation json objects
specs with their ` dimension `
Runs a query against Druid and returns a dataframe .
Converting all GROUPBY columns to strings
Given Superset filter data structure , returns pydruid Filter ( s )
Get the environment variable or raise exception .
Returns datasource with columns and metrics.
Loading a dashboard featuring misc charts
Loads the world bank health dataset, slices and a dashboard
Loading data for map with country map
Returns a list of SQL statements as strings, stripped
Reformats the query into the create table as query .
returns the query with the specified limit
Read a url or post parameter and use it in your SQL Lab query
Gets a values for a particular filter as a list
Processes a sql template
handling of datasource info
Protecting from has_access failing from missing perms/view
missing perms for datasources , schemas and metrics
leaves faulty permissions that need to be cleaned up
Inits the Superset application with security roles and such
supported import/export schema to a dictionary
Exports databases and druid clusters to a dictionary
Imports databases and druid clusters from dictionary
Takes a query_obj constructed in the client and returns payload data response
Get the formdata stored in the database for existing slice .
demonstrate the feature
Get a mapping of foreign name to the local name of foreign keys
Get all ( single column and multi column ) unique constraints
schema as a dictionary
Import obj from a dictionary
Export obj to dictionary
Overrides the plain fields of the dashboard.
Move since and until to time_range.
Use this decorator to cache functions that have predefined first arg .
Check if user can access a cached response from explore_json .
Check if user can access a cached response from slice_json .
Applies the configuration's http headers to all responses
Updates the role with the give datasource permissions .
request that GET or POST form_data
using json instances from the file .
Deprecated endpoint , here for backward compatibility of urls
retrieve values for specified column .
overwrite a slice
checking/unchecking any boolean in a sqla model
fetch the list of tables for given database
Save a dashboard 's metadata
save slices to a dashboard
Recent activity (actions) for a given user
lets us use a user 's username to pull favourite dashboards
created , or faved
created by this user
Favorite slices for a user
Warms up the cache for the slice or table .
Toggle favorite stars on Slices and Dashboard
rendering for a dashboard
Syncs the druid datasource in main db with the provided config.
Returns if a key from cache exist
Serves a key off of the results backend
Runs arbitrary sql and returns and json
Download the query results as csv.
Get the updated queries .
Used for Sqllab Query Search; run sqllab queries .
Personalized welcome page
User profile page
exposes an API endpoint to
Provide a transactional scope around a series of operations .
caching views and handling etag conditional requests .
apply a LIMIT clause
Modify the SQL Alchemy URL object with the user to impersonate if applicable .
quote a sql column/expression label .
exceeds the max length supported by the engine ,
consider foreign tables for PostgreSQL
is unable to identify mixed case column names unless they
Extract error message for queries
Returns a list of tables [schema1.table1, schema2.table2, ...]
Updates progress information
Returns a partition query
creates a superset datasource in Hive .
Return a configuration dictionary that can be merged with other configs
start with a letter or underscore and contain only
requires us to not use backtick in the fieldname which are
Loading time series data from a zip file in the repo
Imports dashboards from a stream to databases
Returns all dashboards metadata as a json dump
handling error while processing the SQL
get the query and retry if it can not
Executes the sql query returns the results .
Executes a single SQL statement
logging call if not
Converts a string to an int/float
is in minus
Returns ``datetime.datetime`` from human readable strings
be passed into json.loads obj_hook parameter
Returns ``datetime.datetime`` from natural language time deltas
datetime to take less room when it is recent
json serializer that deals with dates
Translate exception into error message
find a constraint name in alembic migrations
find a foreign-key constraint name in alembic migrations
find foreign-key constraint names in alembic migrations
find a unique constraint name in alembic migrations
Send an email with html content , eg :
Setup the flask-cache on a flask app
Compress things in a py2/3 safe fashion
Decompress things to a string in a py2/3 safe fashion
Given a user ORM FAB object , returns a label
string representations of
Backwards compatibility hack.
form data to restructure the adhoc filters in the form of the four base
related dataset to use with sankey and graphs
Loading random time series data from a zip file in the repo
Starts a Superset web server .
Prints the current version number
Refresh druid datasources
Import dashboards from JSON
Export dashboards to JSON
Import datasources from YAML
Export datasources to YAML
Export datasource YAML schema to stdout
Refresh sqllab datasources cache
Starts a Superset worker for async SQL query execution .
Runs a Celery Flower web server
Loading birth name dataset from a zip file in the repo
refreshes druid datasources metadata
reversed ) linked list .
converts the non-negative number list into a string.
:type root: TreeNode
is the same as DFS , which is O ( V + E )
:type nums: List[int]
is list of positive/negative numbers
:type words: list
Insertion Sort
cycle_sort
Cocktail_shaker_sort
:type people: List[List[int]]
:type s: str
Shell Sort
Return prefix common of 2 strings
Euler's totient function or Phi function.
builds up a dictionary where the keys are the values of the list ,
[summary]
given strings s1 and s2
:type root: Node
:type n: int
find the nth digit of given number .
Return the 'hailstone sequence ' from n to 1
n is a prime number
Find the length of the longest substring
Push the item in the priority queue .
Calculates factorial iteratively.
Calculates factorial recursively.
Selection Sort
Time Complexity: O(N)
Time Complexity: O(N^2)
replace u with v
find the max node when node regard as a root node
find the minimum node when node regard as a root node
Computes (base ^ exponent) % mod.
:type intervals: List[Interval]
:type path: str
Jump Search
multi dimensional iterable and
Bidirectional BFS!!
get every convolution window per loop iteration .
fit the number of dimensions if
Merge intervals in the form of a list.
Merge two intervals into one.
Print out the intervals.
Rotate the entire array ' k ' times
followed by the entire array
:type matrix: List[List[int]]
Quick sort
:type digits: List[int]
:type head: ListNode
:type head: Node
uses a max heap to sort an array in ascending order
heapify helper for max_heap_sort
uses a min heap to sort an array in ascending order
heapify helper for min_heap_sort
the RSA key generating algorithm
Return square root of n, with maximum absolute error epsilon
Counting_sort
Calculate the powerset of any iterable .
Optimal algorithm - DONT USE ON BIG INPUTS - O(2^n) complexity!
be used on large
:type val: int
n: int
:type pattern: str
Bogo Sort
Insert new key into node
inserting or deleting a node ,
Update tree height
Calculate tree balance factor
In-order traversal of the tree
:type low: str
:type words: List[str]
is a suboptimal , hacky method using eval ( ) , which is not
is a brute force method where we keep a dict the size of the list
is an optimal method using iteration .
Wortst Time Complexity: O(NlogN)
:type array: List[int]
Encodes a list of strings to a single string.
Decodes a single string to a list of strings .
:type A: List[List[int]]
calculates nCr .
calculates nCr using memoization method .
Pancake_sort
:type prices: List[int]
input: [7, 1, 5, 3, 6, 4]
:type num: int
:type input: str
is sorting algorithm to use multiple process , but this code not containing parallel process
Computes the strongly connected components of a graph
Builds the implication graph from the formula
improves average time complexity .
depth + 1
:type head: RandomListNode
Dynamic Programming Algorithm for
:type num: str
Merge Sort
Merge helper
Bucket Sort
Initialize max heap with first k points.
use int ( ) built-in function instread of this .
A slightly more Pythonic approach with a recursive generator
Calculate operation result
Apply operation to the first 2 items of the output queue
Return array of parsed tokens in the expression
Calculate result of expression
Return list of all primes less than n,
returns a list with the permuations.
returns a perumation by each call .
Extended GCD algorithm .
type root: root class
internal library initializer.
Initialize the rabit library with arguments
Print message to the tracker.
Get the processor name .
Broadcast object from one node to all other nodes.
Normalize UNIX path to a native path.
internal training function
Train a booster with given parameters .
Make an n-fold list of CVPack from random indices .
Aggregate cross-validation results.
given parameters .
Update the boosters for one iteration
Evaluate the CVPack for one iteration .
is cv or train
format metric string
Create a callback that print evaluation result .
Create a call back that records the evaluation history into * * eval_result * * .
Reset learning rate after iteration 1
Create a callback that activates early stoppping .
Run the doxygen make command in the designated folder .
Decorate an objective function
Set the parameters of this estimator.
Get xgboost type parameters .
Load the model from a file.
Fit the gradient boosting model
Predict with `data`.
Return the predicted leaf every tree for each sample .
Feature importances property
Coefficients property
Intercept (bias) property
Fit gradient boosting classifier
being of a given class .
Convert a list of Python str to C pointer
Revert C pointer to Python str
Load xgboost Library.
Convert a ctypes pointer array to a numpy array .
ctypes pointer to buffer type .
Convert a python string to c array .
Extract internal data from pd.DataFrame for DMatrix data
Validate feature names and types if data table
Extract numpy array from single column data table
Initialize data from a CSR matrix.
Initialize data from a CSC matrix.
Initialize data from a 2-D numpy matrix.
Initialize data from a datatable Frame.
Set float type property into the DMatrix.
Set float type property into the DMatrix
Set uint type property into the DMatrix.
Saved binary can be later loaded
used for ranking ) .
Get feature names (column labels).
Set feature names (column labels).
Set feature types (column types).
Initialize the model by load from rabit checkpoint .
Get attribute string from the Booster.
attributes stored in the Booster as a dictionary .
Set the attribute of the Booster.
Set parameters into the Booster.
Evaluate the model on mat .
Predict with data.
Save the model to a file .
Dump model into a text or JSON file.
Returns the model dump as a list of strings .
Get feature importance of each feature.
Parse a boosted tree model text dump into a pandas DataFrame structure .
are identical .
split value histogram of a feature
based on fitted trees .
parse dumped node
parse dumped edge
specified tree to graphviz instance .
Create a new action and assign callbacks , shortcuts , etc .
Sort the list into natural alphanumeric order.
Update line with last point and current coordinates.
Select the first shape created which contains this point .
Moves a point x,y to within the boundaries of the canvas.
formed by ` points ' , yield the intersection
Standard boilerplate Qt application code.
depend on an opened image .
drawing , toggling between modes should be disabled .
handle difficult examples
give focus to the label editor .
specified file , or the last opened file if None .
fit the main widget .
py2/py3 unicode helper
Return a pretty-printed XML string for the Element .
Return XML root
Perform a HTTP request and return decoded JSON data
A better wrapper over request for deferred signing
is the entry point for all generated methods
matching error strings exactly vs broadly
A helper-wrapper for the safe_value_2() family.
Deprecated, use decimal_to_precision instead
Deprecated , todo : remove references from subclasses
is not the same character repeated or an empty sequence
r'''
Reads a .wav file .
Writes a .wav file .
Generates audio frames from PCM audio data .
Initialise the runner function with the passed args , kwargs
Creates a sparse representention of `` sequence `` .
Generate a function to download a file based on given parameters
Generate a function to extract a tar file based on given parameters
Increments the counter by the given amount
Parse command line parameters
Setup basic logging
allowing external calls
Downloads the data associated with this instance
associated with this instance to wav 's
Wrapper for the CTC Beam Search Decoder.
Wrapper for the batched CTC beam search decoder.
support our native processing sampling rate , so
Return a block of audio data resampled to 16000hz , blocking if necessary .
yields all audio frames from microphone .
yields series of consecutive audio frames comprising each utterence , separated by yielding a single None .
cut ` function that supports parallel processing .
Change the module 's ` cut ` and ` cut_for_search ` functions to the
segments an entire sentence that contains
Finer segmentation for search engines.
personalized dict to improve detect rate .
Add a word to dictionary .
force the characters in a word to be
Tokenize a sentence and yields tuples of ( word , start , end )
using TextRank algorithm .
using TF-IDF algorithm .
raw ( English , other ) pairs from a ParaCrawl V3.0 data file .
Generates Unicode strings, one for each <seg> in a ParaCrawl data file.
Generates a cleaned-up stream of ( English , other ) translation pairs .
Obtain a list of image paths corresponding to training or eval case .
target_dir ` .
Create a numpy array with specified shape and masked fraction .
Base problem example generator for Allen Brain Atlas problems.
Set of hyperparameters.
Hyper parameters specifics for long sequence generation.
model with moe .
formulate a seq2seq problem as language modeling .
Applies residual function for RevNet.
using a 1x1 convolution filter .
using average pooling .
used as first RevNet block .
bottleneck RevNet unit from authors ' RevNet architecture .
Converts activations from last RevNet block to pre-logits.
optimized RevNet block to build a RevNet .
Default hparams for Revnet.
Tiny hparams suitable for CIFAR/etc.
tuning revnet .
Basic 2-frame conv model.
Basic 2-frame conv model with pixel noise.
Basic conv model with scheduled sampling.
Conv autoencoder, tiny set for testing.
Tiny for testing.
Basic conv model with L1 modality.
Basic conv model with L2 modality.
tuning grid .
Series of architectures for language modeling.
using the appropriate env .
Compute the designated learning rate factor from hparams .
Learning rate schedule based on hparams .
Backwards-compatible learning-rate schedule.
is used .
learning rate according to the given schedule .
Learning rate decay multiplier .
Learning rate warmup multiplier .
find ` is a subtree of ` expr ` .
Generate a random expression tree with a required variable .
Generate a random expression tree .
given var in an expression .
be encoded .
generate an algebra inverse dataset sample .
generate an algebra simplify dataset sample .
generate a symbolic integral dataset sample .
required objects to generate symbolic math datasets .
Generate the algebra inverse dataset .
Generate the algebra simplify dataset .
Generate the calculus integrate dataset .
is a subtree .
Preprocessing steps common to all models .
Use input modality, vocab, and space id for target.
Swap input/output modalities, vocab, and space ids.
A set of basic model hyperparameters.
Batch size in examples per TPU core.
preprocessing on the whole dataset .
Get filepattern for data files for mode.
is '_rev ' .
Build a Dataset for this problem .
Return a dict of Tensors from a serialized tensorflow.Example .
Retrieve dict<feature name, FeatureInfo>.
wrapped for Estimator .
Which part of the training data to read.
input pipeline for problem .
serving export , starting from serialized example .
Get hyper-parameters file path .
given checkpoint as tfhub module with given spec .
Exports the last checkpoint from the directory as tfhub module .
Build the graph required to fetch the attention weights .
representing the attentions from a build model .
dict , ready for inference .
List of ints to str.
List of ints to list of str.
Constructs the data needed for visualizing attentions .
Shifts and pads with zero along an axis.
Given frame_logits from a per-pixel softmax , generate colors .
Common HParams for next_frame models.
Removes top level TimeLimit Wrapper.
see make_gym_env for details .
Create a gym env optionally with a time limit and maxskip wrapper .
returns the registered name and the env .
Repeat action, sum reward, and max over last observations.
Log out and possibly reraise errors during import .
kwargs provided .
Loading hparams from json ; can also start from hparams if specified .
Add problem hparams for the problems.
exampls from the tsv file .
extract CIFAR to directory unless it is there .
Image generator for CIFAR-10 and 100.
HParams for PPO base.
setting but quicker with only 2 epochs .
setting with a stochastic next-frame model .
setting with stochastic discrete model .
setting with stochastic discrete model & deterministic sim starts .
setting with stochastic discrete model , changed ppo steps .
setting with sv2p as world model .
override for tiny setting excluding agent-related hparams .
Tiny set for testing.
setting with a recurrent next-frame model .
setting with a tiny sv2p model .
Grid over games and frames, and 5 runs each for variance.
Merge multiple HParams into one with scopes.
Split single HParams with scoped keys into multiple.
training loop from scoped HParams .
Get mapping from keyboard keys to actions .
perform special action .
Expand observation array with additional information header (top rows).
return usual step tuple .
simulated and real environments .
Perform step(action) on environments and update initial_frame_stack.
Compute time first and second-order derivative channels.
using tf ops .
Plays the env problem by randomly sampling actions for ` num_steps ` .
provided vocabulary .
Encrypt plain text with a single shift layer.
given key .
A stack of super_lm layers.
Add mixture of experts with ~1B params.
Series of architectural experiments on Translation.
Mixture of experts (16 experts).
Series of architectural experiments on cheap language models.
Two-dimensional hierarchical mixture of 16 experts.
Series of architectural experiments on language modeling.
incorporating mixture-of-experts and local-attention .
128 experts, ~25B params - Train for 131072 steps on 8x8.
Test on local cpu.
With sequence length 4096.
With compressed attention.
Set of architectural experiments - language model on wikipedia on a 2x2.
tokens instead of masking .
extracts the dataset .
Get a Counter with the ngrams of the given ID list .
Compute Fbeta score.
Compute the addition score ( Equation 4 in the paper ) .
Compute the keep score ( Equation 5 in the paper ) .
Compute the deletion score ( Equation 6 in the paper ) .
Compute the SARI score for a single prediction and one or more targets .
Computes the SARI scores from the given source , prediction and targets .
directory unless they are there .
Extract images from an MNIST file into a numpy array.
Extract labels from an MNIST file into integers.
Image generator for MNIST.
Image generator for FashionMNIST.
using input parameters .
Basic 2-frame conv model with stochastic tower.
Basic 2-frame conv model with stochastic discrete latent.
Map the function f to the nested structure x (dicts, tuples, lists).
Get a structure of shapes for a structure of nested arrays .
Get a structure of sizes for a structure of nested arrays .
Find the frame with the caller on the stack.
Shorten file path in error lines for more readable tracebacks.
Cleaned-up form of traceback.
Create a layer class from a function .
Initialize the layer given an input shape and rng .
Returns dict<str ref_url, str ref_content>.
Urls for chunk: dict<str wiki_url, list<str> ref_urls>.
are part of shard shard_id .
return reference paragraphs by tf-idf score on title tokens .
Produce examples from shard_ids to out_filepaths.
Encodes sections with vocab.
sharded output files .
Extract pages from an xml dump.
Extract the title from a page .
Extract the text from a page .
found between instances of start_string and end_string .
leave the viewable text .
A stack of self attention layers.
Prepare question encoder.
Attention on image feature with question as query.
Multi layer perceptron with dropout and relu activation.
Prepare encoder.
A stack of transformer layers.
Iterative encoder decoder.
VQA attention baseline hparams.
A default set of length-bucket boundaries.
batching scheme based on model hyperparameters .
Wrapper around _batching_scheme with hparams.
Pads unknown features' dimensions for TPU.
Set the right shapes for the features .
Return the number of TFRecords in a file .
Pad batch dim of features to nearest multiple of batch_multiple.
Generate start and end indices per outfile.
Generate example dicts.
Convert single h5 record to an example dict.
Linearly interpolate between two tensors at coeff.
Linearly interpolate channel at "rank" between two tensors.
x from [ -0.5 , 0.5 ] , to [ 0 , 255 ] .
Returns a single or list of conditional latents at level 'level'.
checking for cond_latents .
Wrapper for data-dependent initialization.
Dropout x with dropout_rate = rate.
actnorm to each time-step independently .
x_{ij} = s x x_{ij} + b. Per-channel scaling and bias.
Add a bias to x .
Per-channel scaling of x.
1X1 convolution on x.
concatenates an edge bias across the depth of x .
left across time and pad valid across the spatial components .
Convolutional layer with edge bias padding and optional actnorm.
used in the affine coupling layer .
Dilated convolutional stack .
3-layer convolutional stack.
Reversible additive coupling layer.
coupling layer .
increase the number of channels .
Get a list of valid dilation rates .
maps a time-indexed list of 3-D latents to a gaussian .
mapping x to a standard normal distribution at init .
Map latent to the mean and log-scale of a Gaussian.
isotropic gaussian-noise to each latent .
Merge level_dist and latent_dist.
Returns a conditional prior for each level.
conditioned on z_ { t-1 } and latent .
/ concatenates x into x1 and x2 across number of channels .
One step of glow generative flow.
hparams.depth' steps of generative flow.
are pre-component .
Unconditional prior distribution.
x^i with q^i ( x ) = U ( x , x + 1.0 / 256.0 ) .
Glow encoder-decoder.
A custom getter function for float32 parameters and bfloat16 activations.
A custom getter function for float32 parameters and float16 activations.
Simulate quantization to num_bits bits, with externally-stored scale.
phi * ( step_num + 1 ) ) mod 1.0 .
cand1 or to cand2 in an unbiased way .
Convert a float32 to a bfloat16 using randomized roundoff .
uses the encoding for bfloat16 and float32 vars .
videos from files .
Compute the PSNR and SSIM .
dataset from in-memory predictions .
Computes the average of all the metric for one decoding .
according to reduce_func .
Computes statistics of metrics across multiple decodings.
Computes metrics from predictions.
saves the video metrics .
Swaps time and batch axis (the first two axis).
given tensor to given image shape .
Full LSTM cell.
2D Convolutional LSTM.
generated data points .
Injects the additional input into the layer .
based scheduled sampling .
Apply dynamic neural advection to previous image.
Apply convolutional dynamic neural advection to previous image.
A layer of VGG network with batch norm.
Tile latent and concatenate to image across depth.
Encodes numpy images into gif string.
encode images with ffmpeg to check if it works .
Outputs a ` Summary ` protocol buffer with gif animations .
Builds convolutional latent tower for stochastic model.
based on the schedule .
extract a random consecutive patch of num_frames .
Writes multiple video frames .
write frames .
Starts a thread for reading output from FFMPEG .
transconding and returns the video .
are set to acceptable values .
Returns a request function.
encodes inputs into mean and std of a gaussian .
expected fully connected shape after a series of convolutions .
3-D SNGAN discriminator.
computing the GAN loss .
Get the discriminator + generator loss at every step .
Gets extra loss from VAE and GAN.
apply 3-D convolution and leaky relu .
Weight-level magnitude pruning.
Unit-level magnitude pruning.
Prune the weights of a model and evaluate.
Loads the configuration .
Pong base parameters.
based on the original PPO paper .
Atari parameters with world model as policy.
Atari parameters with stochastic discrete world model as policy.
creating a simulated env , in or out of graph .
simulated env kwargs from real_env and loop hparams .
Get a policy network .
Base set of hparams for model-free PPO.
Tiny set of hparams for model-free PPO.
Tiny DQN params.
Eval set of hparams for model-free PPO.
Curvature range.
Estimate of gradient Variance.
Distance to optimum.
Prepare Variables for YellowFin.
Get the cubic root .
minimizing the surrogate .
Get the min mu which minimize the surrogate .
based on momentum SGD .
Applying gradients and tune hyperparams with YellowFin .
Compute gradients through momentum optimizer.
Adapted from TensorFlow Optimizer base class member function .
A stack of convolution blocks with residual connections.
used for training .
prepairs the dataset to be parsed by the data_generator .
Parse str to tokens and pos tags.
Convert the dataset in to a simpler format.
create vocabulary .
Split items into num_shards groups.
An initializer function for random normal coefficients.
An initializer function for random Glorot-scaled coefficients.
An initializer function for random uniform Glorot-scaled coefficients.
Make a n+1 dim one-hot array from n dim int-categorical array .
log softmax to x : log-normalize along the given axis .
x : exponentiate and normalize along the given axis .
padding string to list of pairs of pad values .
Output shape of a flatten layer.
initialize batch norm params .
Layer construction function for a batch normalization layer.
Helper: compute the output shape for the pooling layer.
used in pooling layers later .
given rate .
calculate the kernel shape .
Compute the shape of a conv given input shapes in canonical order .
relative to Conv HLO .
Generalized computation of conv shape .
Factory for dopamine agent initialization.
Factory for dopamine environment initialization function.
based on key prefixes .
Build WrappedReplayBuffer with custom OutOfGraphReplayBuffer.
* args and run parent method .
Hparams for next_frame_glow.
reproduce bits-per-pixel results on BAIR action-free dataset .
Hparams for qualitative video generation results.
Hparams for qualitative and quantitative results on shapes dataset.
given z^ { 1 .. t-1 } .
connected model .
A stack of layers.
downsampled images .
Preprocessing used for Imagenet and similar problems .
given image using the provided offsets and sizes .
cropped_image using a one of the bboxes randomly distorted .
Make a random crop of ( ` size ` x ` size ` ) .
are true .
scaling the smaller spatial dimension to ` size ` .
Crops to center of image with specified `size`.
Normalize the image to zero mean and unit variance .
Preprocesses the given image for evaluation .
learning rate schedule .
Learning rate that decreases when eval metric stalls .
using projection tensors .
Slice encoder hidden state under num_blocks.
Find the nearest element in means to elements in x.
training the embeddings via DVQ .
representing numbers bitwise ( lower-endian ) to int tensor .
Turn x_int into a bitwise ( lower-endian ) tensor and embed densly .
Embedding function that takes discrete latent and returns embedding .
Simple variational autoencoder without discretization.
protect from overflows .
Gumbel softmax discretization bottleneck.
Discretization bottleneck.
Predict a sequence of bits (a latent) with LSTM, both training and infer.
Get lookup table for VQ bottleneck.
quantized discrete bottleneck .
Discretize each x into one of codebook_size codes.
Compute the loss of large vocab tensors using a VQAE codebook .
quantized representation .
Sample from Gumbel-Softmax and compute neighbors and losses.
using Gumbel-Softmax .
flip bottleneck_noise many bits .
Simple un-discretization from tanh.
Improved semantic hashing bottleneck .
Improved semantic hashing un-bottleneck .
calling all the above bottlenecks with hparams .
calling all the above un-bottlenecks with hparams .
Create hyperpameters for inverse autoregressive flows.
containing the original vocabulary .
Replace out-of-vocab words with "UNK".
unpack the corpus .
Hparams for decoding.
Preprocess frame.
Encode frames to latents.
Decodes latents to frames.
Interpolate between the first input frame and last target frame.
nested summaries_log_dir based on decode_hp .
interpolated frames into tf summaries .
Create slot variables for Adam with accumulated gradients.
is zero .
beta_power variables every n batches and incrs counter .
Base hparams for TransformerRevnet.
do we split each training batch .
concatenated lines from file upto up_threshold characters .
Given python generators , generate from one , then from another , etc .
using the decoder output .
Preprocessing to strip tags in SGM files .
filename ` .
Get vocab for distill problems.
overrides from unparsed args list .
Create a run config .
Saves FLAGS and hparams to output_dir.
A stack of convolution blocks with residual connection.
Xception entry flow.
Xception exit flow.
Returns a plaintext representation of HTML content.
Return text strings in soup.
using cross entropy .
using DMOL .
Transformer base params for cifar-10.
separate rgb embeddings.
big 1d model for conditional image generation.2.99 on cifar10.
big 1d model for unconditional generation on imagenet.
Gets to 2.92 in just under 4 days on 8 p100s.
big 1d model for conditional image generation.
fo 12 layer big 1d model for imagenet 64x64 .
Set of hyperparameters for a very small imagetransformer with MoE.
training imagetransformer on tpu .
Small model for tpu cifar 10.
Moe tpu params.
related small model .
works very well on 4x4.
Range of hyperparameters for vizier.
TPU related imagenet model.
TPU config for cifar 10.
related 12 layer 8 heads model .
wrapping the training loop , updates step counters .
Reads words from a file.
Reads a file to build a vocabulary of ` vocab_size ` most common words .
returns a ` TokenTextEncoder ` for the vocabulary .
Normalize attention matrices and reshape as necessary.
Compute representation of the attention ready for the d3 visualization.
Decode a list of tokens to a unicode string.
files matching a wildcard pattern , yielding the contents .
Read the corpus and compute a dictionary of token counts .
Read a vocab file and return a dictionary of token counts .
Make a tf.train.Example for the problem .
make grpc requests with runtime args .
make CloudML Engine requests with runtime args .
makes request to deployed TF model , and decodes outputs .
Basic 2-frame recurrent model with stochastic tower.
Creates experiment function.
Generate source and target data from a single file.
make predictions and targets lists , check they match on length .
counting only those where targets ! = mask_id .
Calculate negative log perplexity.
Save State and optionally gin config.
Evalaute on train and eval data, and log metrics.
Evaluate.
Log metrics to summary writer and history.
Get a JAX random number generator and set random seed everywhere .
is reached .
Use jit on model_predict if required.
Get jit-ed update function for loss , optimizer , learning rate function .
Reshape x into a shape [num_devices, ...].
nested x into a shape [ num_devices , ... ] .
Train the model on the inputs .
Computes the number of input and output units for a weight shape .
loading from strings ; returns value if ca n't load .
Creates a time-step and appends it to the list .
given kwargs .
Returns a tuple of sum of raw and processed rewards.
Completes the given trajectory at the given index .
given indices and populates observations .
do n't have observations .
obtained from taking a step in all envs .
Returns the number of time-steps in completed and incomplete trajectories .
Pads the observations in all the trajectories and returns them .
Generate squad examples.
based on hyperparameters .
Create a layer stack based on the hyperparameter values .
Hyperparameters for single-stack Transformer.
Machine translation base configuration.
Small encoder-decoder model for testing.
Test out all the layers on local CPU.
incorporating mixture-of-experts , local and global attention .
Series of machine translation models.
With local self-attention in the decoder.
Recurrent decoder function.
Block of batch norm and relu.
Bottleneck block variant for residual networks with BN after convolutions.
Creates one layer of blocks for the ResNet model .
Catch bugs locally...
Small single parameters.
Data parallel CIFAR parameters.
Universal Transformer encoder function.
applying the universal transformer layer .
is used in universal transforemr steps .
is parametrised for encoding .
multihead attention function which is parametrised for encoding .
multihead attention function which is parametrised for decoding .
Basic Universal Transformer.
Universal Transformer with highway connection.
universal_transformer with depth-wise attention.
uses a gru as transition function .
uses a lstm as transition function .
based models .
Implements a Feed-forward layer with multiple inputs, pad-removing, etc.
timing signal ) .
Preprocess the input at the beginning of each step .
timing signal .
Iterate through records in WET file object.
Generate WETRecords from filepath.
remove obviously bad paragraphs ( bad text extraction ) .
Log start, end, and duration.
end with length and then 1 blank line .
Read WETRecord from file .; end with 2 blank lines .
Multi-layer feed-forward neural network with non-linear activations.
have the same observation and action space .
Initializes the environments and trajectories .
rounds , and changes to integer type .
Returns the number of distinct rewards .
pre-process or record .
given indices .
Takes a step in all environments , should n't pre-process or record .
Takes a step in all environments .
store on disk and their decoders .
yield single time-steps from a list of trajectories .
Transformer preparations and encoder.
Original Transformer decoder.
Latent prediction and loss.
A policy net function.
A value net function.
A policy and value net function.
Dumps the params with `logging.error`.
given policy net and behaviour .
Returns the padding value given a dtype .
is a multiple of boundary .
Computes rewards to go .
Computes the value loss .
Computes the value loss given the prediction of the value function .
Computes TD-residuals from V ( s ) and rewards .
Computes the GAE advantages given the one step TD-residuals .
Picks out the probabilities of the actions along batch and time-steps.
Computes the probability ratios for each time-step in a trajectory .
given observations .
given predictions .
Computes the combined ( clipped loss + value loss ) given predictions .
Computes the combined ( clipped loss + value loss ) given observations .
PPO optimizer step.
Value optimizer step.
Policy and Value optimizer step.
Runs the training loop for PPO , with fixed policy and value nets .
Download corpora for multinli.
Generate mnli examples.
Adds a residual connection to the filter x for the shake-shake model .
Building a 2 branching convnet .
Builds a full shake-shake sub layer.
Builds many sub layers into one full layer.
@ 700K steps , 1 GPU .
Check if metric has plateaued .
SAVP model hparams.
SAVP - VAE only model.
SAVP - GAN only model.
Default hyperparameters for a DietAdamOptimizer.
A two-layer feed-forward network with relu activation on hidden layer.
Quantize x according to params , optionally randomizing the rounding .
according to params .
Create a custom variable getter for diet variables according to params .
use diet variables .
Create the factorized Adam accumulators for diet variables .
Update the variable and its slots .
Construct EstimatorSpec for EVAL mode.
Generator for the dataset samples.
Adds a stack of LSTM layers on top of input .
Run LSTM cell with attention on inputs of shape [batch x time x size].
seq2seq model with attention , main step used for training .
encoding inputs that are [ batch x time x size ] .
The basic LSTM seq2seq model with bidirectional encoder.
hparams for LSTM.
Base attention params.
Basic LSTM Params.
Hparams for LSTM with area attention.
Construct input pipeline.
Transform a string with a filename into a list of float32 .
Transform a sequence of float32 into a waveform .
returns a new vertex .
Creates a Vertex mapped by key .
connecting source and target vertices .
representing the Graph .
Self-attention layer with source as memory antecedent.
sum to 1 .
Sample from the latent space in the autoencoder.
Hyperparameters for CIFAR-10 experiments.
For 64x64 ImageNet.
Basic transformer_sketch hparams.
Get the layers module good for TF 1 and TF 2 work for now .
takes broadcast_dims instead of noise_shape .
Saturating sigmoid : 1.2 * sigmoid ( x ) - 0.1 cut to [ 0 , 1 ] .
reached at max_step .
The shake-shake sum of 2 tensors, python version.
Overriding gradient for shake-shake of 2 tensors .
approximated by sums of 2 .
Conversion of pixel values to real numbers.
Make x n-d with squeeze and expand_dims.
Image standardization on batches and videos.
Flatten a 4d-tensor into a 3d-tensor by joining width and height .
works faster on tpu .
TPU hack for tf.cumsum.
does not scale up .
reducing to max 4 dimensions .
Shift the second dimension of x right by one .
Use a strided convolution to downsample x by 2 , ` nbr_steps ` times .
Use a deconvolution to upsample x by 2 * * ` nbr_steps ` .
making kernel 1d or 2d depending on inputs shape .
== 0 it 's a separable_conv .
works on TPU ( as of 11/2017 ) .
Create Variables for layer norm.
Layer norm raw computation.
normalize the tensor x , averaging over the last dimension .
Group normalization as in https://arxiv.org/abs/1803.08494.
One version of layer normalization.
Layer normalization with l2 norm.
using the spectral norm .
Resnet connection with zero initialization.
Apply a sequence of functions to the input or output of a layer .
Apply layer preprocessing.
Apply layer postprocessing.
A block of convolutions.
A block of standard 2d convolutions.
A block of standard 1d convolutions.
A block of separable convolutions.
Pooling ( supports `` LEFT '' ) .
Implements a downwards-striding conv block, like Xception exit flow.
Create Tensor of sinusoids of different frequencies.
Adds a bunch of sinusoids of different frequencies to a Tensor .
padding mask .
Compute the length of each sequence in the batch .
logit(density(x)).
padding positions .
followed by linear projection .
Dense layer with dropconnect.
Convolutional GRU in 1 dimension.
gates following the MPNN .
Convolutional LSTM in 1 dimension.
Diagonal Convolutional GRU as in https://arxiv.org/abs/1702.08727.
y on axis 1 so that they have the same length .
match logits length .
weight 1.0 to only the `` targets '' portion of the labels .
Check that the value is nonnegative .
weight 1.0 to only examples from the given task .
weight 1.0 to only the inputs for the given task .
weight 1.0 to the `` target '' part of the concatenated labels .
assuming 0s are padding .
Discretized mixture of logistics loss.
Splits input tensor into parameters of discretized mixture logistic.
Computes negative log probability for the discretized mixture of logistics.
Sampling from a discretized mixture of logistics .
smoothing to limit over-confidence .
Pool elements across the last dimension.
Same global pool, but only for the elements up to the current element.
Gated linear unit layer .
SRU cell as in https://arxiv.org/abs/1709.02755.
doing funky things with sets .
Layer from Deep Sets paper: https://arxiv.org/abs/1611.04500 .
State container for fn_device_dependency.
deps for name and device .
underlying variable ref .
underlying tf.Variable object .
Split approximately equally into num_splits parts.
Gradient function for smoothing_cross_entropy_factored.
smoothing cross-entropy .
create a subgraph with a custom gradient function .
Create a subgraph with a custom gradient .
LayerNorm, Conv, ReLU, Conv.
Return list of dims, statically where possible.
Either argmax or random sampling.
band part of ones .
Reshapes a to match the shape of b .
recomputes the function on the backwards pass .
Identical to layers.dense.
Multiply a batch of input matrices by a batch of parameter matrices .
starting with x2 , mixing mixing , going towards x1 .
Bipolar ReLU as in https://arxiv.org/abs/1709.04054.
Bipolar ELU as in https://arxiv.org/abs/1709.04054.
Gaussian Error Linear Unit.
NAC as in https://arxiv.org/abs/1808.00508.
NALU as in https://arxiv.org/abs/1808.00508.
Argmax along with the value.
Compute the k-th top element of x on the last axis iteratively .
find max and argmax over the last dimension .
Use indices to index into the last axis of x.
Is this an appropriate context to generate summaries .
Reshapes a to match the shape of b in all but the last dimension .
Summarize the video using image summaries starting with prefix .
y 's dtype , if necessary .
be even-sized on axis 1 and 2 , but only if necessary .
inspired by the sliced WGAN paper : https : //arxiv.org/abs/1804.01947 .
based on InfoGAN .
Instance normalization layer.
Generalized convolution layer .
reduce spatial dimensions .
A simple single-layer convolutional discriminator.
concatenated output .
Upscaling the image by a factor of f .
given inputs .
targeted dropout to the weights of a convolution .
targeted dropout .
KL divergence of diagonal gaussian N(mu,exp(log_var)) and N(0,1).
Convert to Tensor.
Generate weights with normalization.
Set the norm of the weight vector.
Data dependent initialization for eager execution.
given epoch .
Evaluate the PPO agent in the real environment .
Evaluate the agent with multiple eval configurations .
Evaluate the world model ( reward accuracy ) .
Write metrics to summary.
CamelCase game name with mode suffix.
Copy a subset of hparams to target_hparams .
Chooses a random frame sequence of given length from a set of rollouts .
Make frame chooser.
Point-wise, hinge loss-like, difference between arrays.
Augments an observation with debug info.
Runs a batch of rollouts from given initial observations .
Sets the state that will be used on next reset .
Download corpora if necessary and unzip them.
Generate splits of the data.
Write text to files.
Infer highest epoch number from file names in data_dir.
Load T2TGymEnv with data from one epoch.
Infer name from filenames.
Wrap environment with gym.Monitor.
Create SimulatedEnv with minimal subset of hparams.
Infers standard paths to policy and model directories.
removes last one .
observation if needed .
frame stack and infer policy .
Infer policy from stack of observations.
using tokenizer.encode .
Parsing the bAbi dataset ( train and test ) .
instantiates a class for each babi subsets-tasks .
encoder for the given class labels .
generates samples that are encoded .
Return a dict for encoding and decoding inference input/output .
Returns problem_hparams.
produce and number the output shards for each .
collecting input and target files .
Adding to base hparams the attributes for for librispeech .
linearized trees and tokens from the wsj tree format .
parsing as a sequence-to-sequence task that uses tokens .
Aggregate stats in per-shard stats files.
created it assuming 1k tasks .
Validate presence and minimum size of files.
Downloading and preparing the dataset .
Gives the file paths with regards to the given split .
Determine the minimum sequence length given a dataset_split .
Determine the maximum sequence length given a dataset_split .
Determine the dataset sized given a dataset_split .
Yields successive checkpoints from model_dir.
The TensorFlow Session config to use.
Create RunConfig, TPUConfig, and Parallelism object.
Create a T2T Estimator .
Create train and eval hooks for Experiment.
See create_experiment .
Restore from a checkpoint.
training every eval_freq_in_steps .
being produced .
Starts a TensorFlow server and joins the serving thread .
Decodes from dataset or file.
dataset on new checkpoint .
Decode from file on new checkpoint.
Flatten dict of dicts into a single dict with appropriate prefixes.
match keys in the flat dict .
work when not using TPU codepath .
Create the metrics_fn that TPUEstimatorSpec expects .
Remove summaries from the default graph .
writing scalar summaries .
Average losses across datashards.
Generate summaries for features.
Compose two custom getters.
Set a custom getter in the current variable scope .
given directory .
is real-valued .
sharded along batch dimension .
feed into body .
given body output and features .
Return a training op minimizing loss .
given mode .
Autoregressive eval.
A inference method.
Beam search decoding.
Slow version of Beam search decoding.
A greedy inference method.
A slow greedy inference method on TPU.
Run the model and extract samples .
Model fn for Estimator.
training ) mode .
Constructs `tf.estimator.EstimatorSpec` for EVAL (evaluation) mode.
Constructs `tf.estimator.EstimatorSpec` for PREDICT (inference) mode.
Adds `tf.summary`s to all terms in the losses dictionary.
Scheduled sampling .
Prepare one shard of the model for the decoder.
Return a flat int32 tensor of shape [ 1 , batch_size * length , 1 ] .
Duplicate elements of bc by length_factor.
Remove padding by concatenating all dimension into one .
Base model with attention expert.
Experiment with the exp_factor params.
Cheap model for single-gpu training.
Cheap model for debugging.
Large model for distributed training.
Unnecessarily large model with 24B params - because we can.
use for seq2seq .
use with languagemodel_wiki_scramble1k50 .
model space .
Bottom transformation for target images.
Compresses channel-wise input pixels into whole pixel representions.
Bottom transformation for image targets.
shorten the stft with strided convs .
get concatenated embedding or softmax variable .
Bottom transformation for symbols.
Bottom transformation for target symbols.
embedding video bitwise .
Bottom transformation for video.
Convert prediction and target from rgb to real.
Compute the CTC loss .
Compute loss numerator and denominator for one shard of output.
Average loss over the labels.
softmax cross-entropy between outputs and targets .
Poisson loss for real.
Loss for class label.
target space .
Top transformation for images.
return logits .
Generate logits.
Top transformation for video.
default bottom transformation ; if none available , return value .
default loss transformation ; if none available , return value .
default name for transformations ; if none available , return value .
default bottom transformation for targets ; if none , return value .
default top transformation ; if none available , return value .
default weights function ; if none available , return value .
Generates all possible pair combinations for the input list of sentences .
fo 8 layer big 2d model for cifar 10 .
Base params for img2img 2d attention.
Current best hparams for local 2d.
Base params for local1d attention.
Current best hparams for local 1d.
training img2img_transformer on tpu .
Residual feed-forward layer with normalization at start.
Transformer encoder layer.
Transformer encoder.
Transformer decoder layer.
uses the decoder part of Transformer ) .
operating on chunks .
Transformer model.
Config for language-model experiments.
Config for translation experiments.
run on 1 TPU .
Multihead scaled-dot-product attention with input/output transformations.
graph attention.
computes transformation for keys and values .
Computes query, key and value for edge matrices.
sparse_ggnn except that each input has a batch dimension .
One message-passing step for a GNN with a sparse adjacency matrix.
Dot product attention with edge vectors.
ggnn version of the MPNN from Gilmer et al.
is just adjacency , we get ggnn .
Precompute the a_in and a_out tensors .
a_t from h_ { t-1 } , see bottom of page 3 in the paper .
string - > int/float/str list ) dictionary .
writing to shard task_id .
save as TFRecord files .
Report hook for download progress.
's already in directory .
Unzips from gz_path into new_path.
Inner implementation for vocab generators.
Generate a vocabulary from the datasets in sources .
Generate lines for vocabulary generation.
r"""Generate a vocabulary from a tabbed source file.
Generate a vocabulary from txt files with example-per-line .
Shuffle a single file of records .
Shuffles the dataset .
examples into longer examples .
packing a dataset which has already been batched .
Make a temporary directory .
Iterate over the records on disk for the Problem.
Yields records from TFRecord files.
Create a fill-in-the-blanks training example from text .
The core Neural GPU.
Improved Neural GPU as in https : //arxiv.org/abs/1702.08727 .
determine the shape of reorder output .
Reorder a tuple into another tuple .
nested arrays .
gating function on a ( memory , gate , candidate ) tuple .
determine the shape of Concatenate output .
Constructs a residual version of layers , summing input to layers output .
Adds default hparams for all of the variants of the Universal Transformer.
Base parameters for Universal Transformer.
Multi-layer config for adaptive Transformer on TPU.
Multi-layer config for adaptive Transformer with hard attention.
Range of hyperparameters.
Split channels in 3 parts.
Build convolutional GRU with diagonal gating as in ImprovedNGPU.
Implementation of Neural GPU: https://arxiv.org/abs/1702.08727.
Strip ids_to_strip from the end ids.
underscores and OOV characters and append ' _ ' .
Transform a human-readable string into a sequence of int ids .
Transform a sequence of int ids into a human-readable string .
Transform a sequence of int ids into a their string versions .
Converts a space-separated string of tokens to a list of ids.
Load vocab from a file.
Initialize tokens from a list of tokens.
Initialize vocabulary with tokens from token_generator.
Write vocab file to disk.
Converts a sequence of subtoken ids to a native string.
Converts a list of tokens to a list of subtoken ids.
token to a list of subtoken ids .
Converts a list of subtoken ids to a list of tokens.
Converts a subtoken integer ID to a subtoken string.
Converts an escaped token string to a list of subtoken strings.
Converts an escaped token string to a list of subtoken IDs.
generated text .
has ` vocab_size ` near ` target_size ` .
Train a SubwordTextEncoder based on a dictionary of word counts .
Debugging dump of the current subtoken vocabulary .
Initialize token information from a list of subtoken strings.
Load from a file object.
Load from a vocab file.
Transform a string with a filename into a list of RGB integers .
Transform a sequence of int ids into an image file .
Transform sequence of float values into string (float values).
make a tiled field of images from numpy arrays .
string to markdown format .
Close SummaryWriter.
Saves scalar value.
Saves RGB image summary from onp.ndarray [H,W], [H,W,1], or [H,W,3].
tiled images from onp.ndarray .
matplotlib plot output to summary image .
Saves audio.
Saves histogram of values.
Saves a text summary .
Import module at usr_dir, if provided.
A set of basic hyperparameters.
A basic range of hyperparameters.
Check if name is in orig_ctr or in one of the other type containers .
To list of dicts suitable for Cloud ML Engine hyperparameter tuning.
Create and register problems for the game.
Decodes a single observation from PNG .
Encodes observations as PNG.
Makes a step in all environments.
according to self.dataset_splits .
List of pairs (split, paths) for the current epoch.
Saves the current epoch rollouts to disk , split into train/dev sets .
Converts a NumPy image to a tf.Summary.Value object.
Optionally converts images from hooks_args to image summaries.
resize function used by quite a few image problems .
Returns list of scaled images, one for each resolution.
encoded as pngs .
takes image and labels lists and creates pngs .
Image augmentation: cropping, flipping, and color transforms.
Image augmentation suitable for CIFAR-10/100.
Apply random horizontal and vertical shift to images.
Get the common attention and feed-forward layers .
Adds the hparams used by get_standardized_layers .
encdec attention loss between expected and actual attentions .
Gets a bunch of sinusoids of different frequencies .
get n-dimensional embedding as the layer ( vertical ) timing signal .
Adds sinusoids of diff frequencies to a Tensor, with timing position given.
Adds positional embedding.
Adds n-dimensional positional embedding .
edge vectors for the edge types in the adjacency matrix .
Calculate the length of mask based on padding .
be added to attention logits .
attention_bias_ignore_padding ( ) .
Create a bias tensor for prepend_mode= '' prepend_inputs_full_attention '' .
encourage attention to close positions .
Generate a mask to prevent the batch to attend to each others .
becomes two dimensions .
Reshape x so that the last two dimension become one.
Reshape x so that the first two dimension become one.
Compute color image summary.
Multi-head dot-product attention with sparsity.
Make attention weights non-0 only on the top-hard_attention_k ones.
Dot-product attention.
Generates matrix of relative positions between inputs.
Generates tensor of size [1 if cache else length_q, length_k, depth].
Relative position-aware dot-product attention inner calculation.
Calculate relative position-aware dot-product self-attention.
Helper to dot_product_self_attention_relative_v2.
Helper function for dot_product_unmasked_self_attention_relative_v2.
retrieve relative embeddings , sliced according to length .
Helper function for dot_product_unmasked_self_attention_relative_2d.
unmasked dot-product self-attention 2d .
Helper function for local 2d attention.
memory_flange is half of query sizes .
Stitches together the local 2d memory blocks.
is half of query .; Gathering memory blocks around query blocks .
unmasked dot-product local self-attention 2d on tpu .
Calculate simple unmasked dot-product local self-attention 2d on tpu.
Attention to the source and a neighborhood to the left within a block.
aboslute indexing for local attention .
Attention to the source position and a neighborhood to the left of it.
create a local version of the keys or values for 1d .
Masked local 1d attention with relative positions.
Strided block local self-attention .
input by splitting its length over blocks of memory_block_size .
Dilated self-attention .
Gathers blocks with gaps in between.
Try it and write a paper on it .; Dilated self-attention .
Making sure x is a multiple of shape .
Reshapes a tensor between dimensions i and j .
flattened blocks from x .
scatters blocks from x into shape with indices.
Getting gather indices .
Creates a mask for 2d block raster scan .
Get the memory regions that surround a 2d query .
Get right shifted blocks for masked local attention 2d .
Right shifts once in every block.
Computes attention compoenent (query, key or value).
Computes query, key and value.
2d Multihead scaled-dot-product attention with inp/output transformations.
Self-attention feedforward layer.
Attention over parameters.
Return a tensor with given shape containing coordinate along given axis .
Implementing attention that runs inside each expert .
using a mixture of experts .
Perform dot product on a subset of the sequence.
Perform a dot product attention on a single sequence on a single head .
Construct the graph with either tf.map_fn or a python for loop .
multihead self attention .
Increase the length and change the dimensionality .
change the dimensionality .
Reduce the length dimension using self attention .
Reduce the length dimension by compressing with conv .
Scaled dot-product attention .
Multihead scaled-dot-product self-attention.
Convert an group index to its bit representation.
Return the bucket id of the given tensor .
The image encoder for the VAN.
The higher level structure encoder for the VAN.
The VAN decoder.
Implements the deep analogy computation .
Implements a VAN.
use as encoder without the top few layers .
LSTM predictor network.
Constructs the tensorflow graph of the hierarchical model .
based on maximal signal power vs. power of the noise .
L2 distance between tensors true and pred.
L1 distance between tensors true and pred.
Calculates loss and psnr for predictions over multiple timesteps.
SV2P model hparams.
SV2P discrete model hparams.
SV2P model for atari.
SV2P model for atari with softmax.
Tiny SV2P model.
SV2P model with additional cutoff in L2 loss for environments like pong.
directory unless it is there .
captioning problem with token-wise captions .
passing on cmd line .
Returns master_type for trainingInput.
Construct jobSpec for ML Engine job.
Launch job on ML Engine.
Tar and gzip src_dir and copy to GCS target_dir.
Tar Tensor2Tensor and cp to train_dir.
Package, tar, and copy usr_dir to GCS train_dir.
are set to acceptable values for CloudML Engine runs .
Launch t2t_trainer on Cloud ML Engine.
overriding add_weight for trainable initializers .
Get the KL multiplier , either dynamically or schedule based .
Get KL loss for all the predicted Gaussians.
Create the latent tower .
Encode transformer inputs.
Decode Transformer outputs from encoder representation.
Create the initial cache for Transformer fast decoding .
Given encoder output and a symbols to logits function , does fast decoding .
packed following tpu params .
Base parameters for Transformer model.
HParams for transformer big model on WMT.
Hparams for transformer on LM for pretraining/finetuning/mixing.
Tied means fine-tune CNN/DM summarization as LM .
Fine-tune CNN/DM with a unidirectional encoder and decoder.
Train CNN/DM with a unidirectional encoder and decoder.
finetuning on text class problems .
Hparams for transformer on LM pretraining (with 64k vocab).
Hparams for transformer on LM pretraining (with 64k vocab) on TPU.
pretraining on TPU , large model .
pretraining on TPU with AdamW .
HParams for transformer base model for single GPU.
parsing on WSJ only .
parsing on WSJ semi-supervised .
Small range of hyperparameters.
Use relative position embeddings instead of absolute position encodings.
HParams for Transformer model on TPU for MLPerf on TPU 2x2.
hparams to be compatible with TPU training .
No dropout, label smoothing, max_length.
training languagemodel_lm1b8k on tpu .
training ASR model on LibriSpeech V1 .
training ASR model on LibriSpeech V2 .
training ASR model on Librispeech on TPU v1 .
training ASR model on Librispeech on TPU v2 .
Hparams for machine translation with ~1.1B parameters.
training languagemodel_wikitext103_l4k .
training languagemodel_wikitext103_l4k with memory .
training languagemodel_wikitext103_l16k with memory .
training image_cifar10_plain_gen_flat_rev with memory .
training image_imagenet64_gen_flat_rev with memory .
Reshape input from 4D to 3D if necessary.
Local 2d, self attention layer.
Local within block self attention.
Local 1d self attention.
Dilated attention with a masking strategy .
Dilated 1d self attention .
Local and global 1d self attention.
Full self-attention layer.
Multi layer transformer.
Multi layer transformer encoder.
ffn layer transformer.
masked self attention bias .
Postprocessing after decoding .
Prepare encoder for images.
Prepare decoder for images.
Creates output from decoder output and vars.
Get separate embedding for each of the channels.
Step the batch of environments .
Reset the batch of environments .
include a revision .
Read wikipedia pages from a history dump.
Extract the id from a page .
Extract the revisions of a page .
Create a dictionary with title , id , and list of revisions .
Copy a file to a directory if it is not already there .
encoded history dumps .
Extract the text from a revision .
Remove everything in curly braces .
do not start with a letter or a quote .
generate the vocabulary .
Get encoder from vocab file .
exceed max_edit_ratio between source and target .
add spelling errors and infill markers .
Compute diffs between two sequences.
Load variables from checkpoint.
Creates a TimeStep with both rewards and actions as optional .
Complete attention layer with preprocessing.
A stack of separable convolution blocks with residual connections.
kkurach @ for the code .
telling to be more similar to your own targets than to others .
connecting encoder and decoder .
Input embeddings -> is_padding.
Version with Noam's decay scheme.
Version for fast local runs.
Converts a space-separated string of tokens to lists of ids.
considering OOVs temporary IDs .
Computes new shape with the smallest side equal to ` smallest_side ` .
preserving the original aspect ratio .
Distort the color of a Tensor image.
func ( x , sel ) , with sel sampled from [ 0 ... num_cases-1 ] .
Subtracts the given means from each image channel .
vqa v2 preprocess image.
Prepare one shard of the model for the encoder.
Feed-forward layer in the transformer.
Transformer on languagemodel_lm1b32k_packed.
HParams for training languagemodel_lm1b32k_packed.
using relative attention .
Transformer with mixture of experts.
based on logits .
Set the random seed from flag everywhere.
Generate data for a problem in _SUPPORTED_PROBLEM_GENERATORS.
Generate data for `EnvProblem`s.
Generate data for a registered problem.
exists under the path .
is relative , not absolute .
Define ppo step.
Generalized advantage estimator .
Returns a reading spec of a gym space.
be represented by the space .
argmax if last dim is not 1 .
Explained variance , also known as R^2 .
Percentage of times that top-k predictions matches labels on non-0s.
round down the predictions to ints .
Percentage of times that predictions matches labels everywhere (non-0).
ignoring padding 0s .
exluding padding 0s .
Average log-perplexity with custom targets_mask.
excluding padding 0s .
Rounding accuracy for L1/L2 losses : round down the predictions to ints .
Percentage of times that predictions matches labels on non-0s.
Used to evaluate the VQA accuracy .
Precision of set predictions.
passes it to tensorboard .
given one-hot labels and logits .
Calculate sigmoid cross entropy for one-hot lanels and logits.
Calculate ROC AUC.
Creates the evaluation metrics for the model .
Create metrics accumulators and averager for Eager mode.
Calculate word error rate.
Calculate pearson correlation coefficient.
A stack of attention_lm layers.
Cheap model.
n-grams up to a given maximum order from an input segment .
BLEU score computation between labels and predictions.
Tokenize a string following the official BLEU implementation .
Compute BLEU for two files (reference and hypothesis translation).
catching ` NotFoundError ` .
sorted by step from files at path_prefix .
Continuously yield new files with steps in filename as they appear.
Extract the VQA V2 annotation files to directory unless it 's there .
Extract the VQA V2 image data set to directory unless it 's there .
Extract the VQA V2 feature data set to directory unless it 's there .
raising a value error for bad assignment .
Update results_dictionary with a scalar value.
Update results_dictionary from a list of values.
provided type , if compatible .
hyperparameter values from a string into a python map .
Adds {name, value} pair to hyperparameters.
existing hyperparameter .
Removes the hyperparameter with key 'name ' .
existing hyperparameter values , parsing new values from a string .
existing hyperparameter values , parsing new values from a dictionary .
Serializes the hyperparameters into JSON .
existing hyperparameter values , parsing new values from a json object .
Return the hyperparameter values as a Python dictionary .
Returns the value of ` key ` if it exists , else ` default ` .
Returns the field name given parameter type and is_list .
Returns the visualizations for query .
Default output directory.
Setup gin configuration.
supervised keys .
given some shapes and vocab size .
Select a subset of features from the example dict .
batch the given dataset .
Compile the model in Keras.
Train the given model on the given dataset .
train the given model on the given dataset .
Decode from estimator.
return the scores .
Put time dimension on channels in an embedded video .
Basic autoencoder model.
Autoregressive autoencoder model.
Residual autoencoder model.
Residual autoencoder model for text.
Residual discrete autoencoder model.
Residual discrete autoencoder model, big version.
Ordered discrete autoencoder model.
Ordered discrete autoencoder model for text.
Ordered discrete autoencoder model for text, small version.
compressing pong frames .
compressing pong frames for testing .
compressing cifar .
Tuning grid of the main autoencoder params .
run LSTM encoder and get the last output as encoding .
given mode and metric .
Get the history for the given metric and mode .
followed by a ReLU .
Strided 2-D convolution with explicit padding .
Standard building block for residual networks with BN before convolutions.
Resnet model.
Returns the length of the Longest Common Subsequence between two seqs .
Computes the length of the LCS between two seqs .
Computes ROUGE-L (sentence level) of two collections of sentences.
scores computation between labels and predictions .
Calculates n-grams.
ROUGE-2 F1 score computation between labels and predictions.
Normalize the examples from different tasks so they can be merged .
containing mixed examples .
Multiproblem loss function.
LM loss for multiproblems.
Generate task_ids for each problem.
Compute the maximum number of classes any subtask has .
Called prior to self-attention , to incorporate memory items .
Called after self-attention .; be updated here .
Compute the safe norm .
based on content similarity .
Read from the memory .
based on a combination of similarity and least used .
Reset the entries in the memory .
Define the training setup .
Metadata for rollouts.
Collect trajectories.
Basic parameters for a vanilla_gan.
outputting image in [ 0 , 1 ] .
Body of the model.
Make Inputs for built-in datasets.
Make random Inputs for debugging.
Takes a tf.Dataset and creates a numpy stream of ready batches .
Preprocessing for LM1B : filter out targets exceeding maximum length .
Return train and eval batches with input name and shape.
samples records from one or more Datasets .
Computes the pmf of a schedule given the global_step .
Returns the outputs of fns [ i ] with probability pmf [ i ] .
Multi-dimensional linear interpolation.
Multi-dimensional step interpolation.
Create a probability-mass-function based on relative epoch rates .
Encodes a schedule tuple into a string.
Decodes a string into a schedule tuple .
converts iterables into tuples .
Returns a list of filepatterns, one for each problem.
Generates data for each problem.
containing examples from multiple problems .
contains both inputs and targets .
Generates TF-Records for problems using a global vocabulary file .
Generates a non-padding mask for areas based on lengths .
Pools for an area in features_2d.
based on a given pooling function ( fn ) .
Computes area sums for features.
Computes features for each area.
Computes the key for each area .
Dot-product area attention.
Make a function that logs the duration since it was made .
Train the PPO agent in the simulated environment .
Train the PPO agent in the real environment .
Train the world model on problem_name .
have already been written .
Run the main training loop .
Single conv layer with relu, optional pooling, and dropout.
Hparams for GeneExpressionConv model.
is an upper bound on the negative likelihood .
Multinomial sampling from a n-dimensional tensor.
Samples from the latent space in the autoencoder.
Residual block over inputs.
compresses 2-D inputs by 2 * * num_compress_steps .
compresses 1-D inputs by 2 * * num_compress_steps .
decompresses 2-D inputs by 2 * * num_compress_steps .
decompresses 1-D inputs by 2 * * num_compress_steps .
Transformer text encoder over inputs with unmasked full attention.
Transformer image decoder over targets with local attention.
using latent_attention_type .
given inputs ( typically , compressed targets ) .
Transformer-based latent prediction model.
using a Transformer decoder and a prior over latent sequences .
using scale and normalization transformations .
be the same as in common_attention , avoiding import .
weight decay and weight noise .
weight noise to vars in var_list .
weight decay to vars in var_list .
Log the sizes and shapes of variables, and the total size.
Summarize the variables .
Get variable initializer from hparams.
Summarize the tensors .
Extract image features from pretrained resnet model.
datasets to directory unless directory/timit exists .
Data generator for TIMIT transcription problem.
Reads a file to build a vocabulary .
version for languagemodel_wiki_scramble8k50.
Reshapes first two dimensions in to single dimension.
beam_size ] .
Tiles a given tensor by beam_size .
Returns the shape of the tensor but sets middle dims to None .
Computes the i'th coordinate that contains the batch index for gathers .
running on TPU .
Replaces the lower bits of each element with iota.
Creates the top k values in sorted order with indices .
Finds the values and indices of the k largests entries .
Given sequences and scores , will gather the top k=beam size sequences .
Beam search with length penalties.
Augments video with optional hue, saturation and constrast.
Creates a border around each frame to differentiate input and target .
Converts input, output and target videos into video summaries.
display videos at decode time .
video metrics summaries using the decoder output .
Creates a VideoWriter for debug videos .
resize example [ `` frame '' ] .
assume that only video frames are provided .
Generate samples of the encoded frames with possible extra data.
generating the data .
Return a decorator which add a TF name/variable scope to a function .
underlying variable .
UnsortedSegmentSum on each row.
Helper function to NoisyTopKGating.
The squared coefficient of variation of a sample.
VQ Gating hparams.
works for very small constant k .
VQ gating.
Noisy top-k gating.
Apply a function to each coordinate ids of a multidimensional tensor .
creates a feed-forward network .
Flatten all dimensions of a except the last.
Call a local mixture of experts .
works well on TPU .
Reduces data per device.
Opposite of reduce_by_device().
Compute the sum of all Tensors and put the result everywhere .
processing arguments that are singletons or lists .
Remove padding from the given tensor .
padding back to the given tensor .
Create one input Tensor for each expert.
weighted by the gates .
corresponding to the examples in the per-expert ` Tensor ` s .
multiplied by the corresponding gates .
Send the inputs to the experts .
Return the output from the experts .
Factory function for envs.
Factory function for Agents.
Collects frames from real env for random starts of simulated env.
Creates an Agent from hparams .
using the Agent API .
Evaluates the world model .
given worker ( directory ) id .
Given a representation of the board , returns a list of open spaces .
Given a representation of the board , returns reward and done .
Hyperparameters for decoding.
Log inference results.
Perform decoding from dataset .
Decodes once.
write them out .
decode filename .
yield elements from the given generator .
produce batches of inputs .
reads from the terminal and yields `` interactive inputs '' .
Save frames of the videos into files.
Shows an image using matplotlib and saves it .
Read a file of partial texts to continue .
Returning inputs sorted according to decreasing length .
is normally 1 .
see above ) to a dictionary .
hooks after decodes have run .
produce and number of output shards for each .
Image Transformer decoder with local1D spatial layers.
Image Transformer decoder with local2D spatial layers.
masked layers .
Model parallel ImageNet parameters.
Returns a list of degree vectors, one for each input and hidden layer.
respecting autoregressive ordering .
Performs incomplete Sinkhorn normalization to inputs.
is reversible .
* sum log | scale | .
Slice encoder hidden state into block_dim.
training the embeddings .
representing numbers into a bitwise ( lower-endian ) tensor .
Discretization bottleneck for latent variables.
approximating the behavior of Adam .
Old version - Adam.
Small transformer model with small batch size for fast step times.
Emily's model hparams.
Convert a file to examples .
Return a mix of env and video data fields and decoders .
Transforms time step observations to frames of a video.
Iterate through lines of file.
Yield dicts for Text2TextProblem.generate_samples from lines of files.
Yield dicts for Text2ClassProblem.generate_samples from lines of files.
Yield dicts for Text2TextProblem.generate_samples from lines of txt_path.
Encode Text2Text samples from the generator with the vocab.
returns a function to pack examples .
Wraps generator with packer if self.packed_length.
dev shard .
text out of an input file .
Read complete text of input files and yield unicode strings.
Generator for examples.
is prepared and the vocab is generated .
training/dev data .
ResNet convolutional striding block.
ResNet identical size block.
ResNet.
WideResnet convolutational block.
WideResnet from https://arxiv.org/pdf/1605.07146.pdf.
Builds a traditional GRU cell with dense internal transformations.
Builds a convolutional GRU.
r"""Parametrized Gated Recurrent Unit (GRU) cell construction.
hide padding and future words .
Build masks for this batch.
Helper: create layer norm parameters.
encoding parameters .
Implements bare positional encoding.
Core dot product self-attention.
Pure single-headed self-attention.
Pure transformer-style multi-headed attention.
Transformer-style multi-headed attention.
see below ) .
chunks to attend to in chunked attention .
shift the tensor to the right by padding on axis 1 .
Create a Zipf distribution .
Generate a random Zipf sample of given length .
reversing nlp-like task on sequences of symbols .
convert a list of digits in the given base to a number .
convert a number to a list of digits in the given base .
generate a random number as a lower-endian digits list .
Run command on GCS instance, optionally detached.
be available at given IP address .
Launch a GCE instance .
Evolved Transformer encoder .; See arxiv.org/abs/1901.11117 for more details .
Evolved Transformer decoder .; See arxiv.org/abs/1901.11117 for more details .
Add attend-to-encoder layers to cache.
Create the initial cache for Evolved Transformer fast decoding .
Add Evolved Transformer hparams.
Base parameters for Evolved Transformer model on TPU.
Big parameters for Evolved Transformer model on TPU.
2-level mixture of experts.
gating for mixture-of-experts in TensorFlow .
Add necessary hyperparameters for mixture-of-experts.
figuring out how to split a dimensino into groups .
subsuming the correction factor .
based on model hparams .
ensure it takes nargs args .
specifies a copy and/or reversal .
Construct a problem name from base and reversed/copy options.
Get pre-registered optimizer keyed by name .
copied/reversed problem in ` base_registry ` or ` env_registry ` .
initialize the ` EnvProblem ` with the given name and batch size .
Creates a help string for names_list grouped by prefix .
string with contents of registry .
function from __init__ .
function from __init__ .; called on successful set .
register a function , or registration itself .
Check the dynamic symbol versions .
Decorate an objective function.
Decorate an eval function.
Get parameters for this estimator.
Build a gradient boosting model from the training set ( X , y ) .
Return the predicted value for each sample .
Get feature importances.
Docstring is inherited from the LGBMModel .
Return the predicted probability for each class for each sample .
Parse config header file.
Get names of all parameters.
Get aliases of all parameters.
Construct code for auto config file for one param value.
Write descriptions of parameters to the documentation file.
Generate auto config file.
Load LightGBM library.
Convert data to 1-D numpy array.
Convert a ctypes float pointer array to a numpy array .
Convert a ctypes double pointer array to a numpy array .
Convert a ctypes int pointer array to a numpy array .
is passed to C API .
Fix the memory of multi-dimensional sliced object.
Get pointer of float numpy array / list.
Get pointer of int numpy array / list.
Predict logic.
Get size of prediction result.
Predict for a 2-D numpy matrix.
Predict for a CSR data.
Predict for a CSC data.
Initialize data from a list of 2-D numpy matrices.
Lazy init.
Create validation data align with current Dataset.
Get subset of current Dataset.
Save Dataset to a binary file.
Set property into the Dataset.
Get property from the Dataset.
Set categorical features.
Set predictor for continued training.
Set reference Dataset.
Set feature name.
Set label of Dataset.
Set weight of each instance.
start from .
Get the label of the Dataset .
Get the weight of the Dataset .
Get the feature penalty of the Dataset .
Get the monotone constraints of the Dataset .
Get the initial score of the Dataset .
Get the raw data of the Dataset .
Get the group of the Dataset .
Get the number of rows in the Dataset .
Get the number of columns ( features ) in the Dataset .
Get a chain of Dataset objects .
Add features from other Dataset to the current Dataset.
Save Dataset to a text file.
Free Booster's Datasets.
Set the network configuration.
Add validation data.
Reset parameters of Booster.
Update Booster for one iteration.
Boost Booster for one iteration with customized gradient statistics.
Rollback one iteration.
Get the index of the current iteration .
Get number of models per iteration.
Get number of weak sub-models.
Evaluate for data.
Evaluate for validation data.
Save Booster to file.
Shuffle models.
Load Booster from a string.
Save Booster to string.
Dump Booster to JSON format.
Make a prediction .
Refit the existing Booster by new data .
Get the output of a leaf .
Convert to predictor.
Get number of features.
Get names of features.
split value histogram for the specified feature .
Evaluate training or validation data.
Predict for training and validation dataset.
Get inner evaluation count and names.
attributes to the Booster .
Find the path to LightGBM library files.
Convert numpy classes to JSON serializable objects.
Format metric string.
Create a callback that prints the evaluation results .
Create a callback that records the evaluation history into `` eval_result `` .
Create a callback that resets the parameter after the first iteration .
Create a callback that activates early stopping .
Perform the training with given parameters .
Make a n-fold list of Booster from random indices .
Perform the cross-validation with given paramaters .
Logarithmic loss with non-necessarily-binary labels.
Measure performance of an objective.
is not tuple or does not have 2 elements .
Plot model's feature importances.
Plot one metric during training.
Create a digraph representation of specified tree .
specified tree .
Return the -std=c++ [ 0x/11/14 ] compiler flag .
is a 1d numpy array corresponding to the vector to which you want to
Train a supervised model and return a model object .
Get the vector representation of word .
Given a string , get a single vector represenation .
Given a word , get the subwords and their indicies .
Given an index , get the corresponding vector of the Input Matrix .
Given a string , get a list of labels and a list of
Get a copy of the full input matrix of a Model .
Get a copy of the full output matrix of a Model .
Get the entire list of words of the dictionary optionally
Get the entire list of labels of the dictionary optionally
Split a line of text into words and labels.
Quantize the model reducing the size of the model and
Converts a List of pairs (regex, params) into an RegularizerApplicator.
List default first if it exists
turns PyTorch and Numpy types into basic Python types so they
Takes a list and groups it into sublists of size `` count `` , using `` default_value `` to pad the
batches the individual instances into lists of the
Take a list of objects and pads it to the desired length , returning the padded list .
added to every key in `` dictionary `` .
tags `` matches
work as expected; random seeds for reproducible experiments .
configures 3 global logging attributes - streaming stdout and stderr
closes any open file handles and logs set up by ` prepare_global_logging ` .
avoid loading spacy models a whole bunch of times , we 'll save references to them ,
given package .
Get peak memory usage for this process, as measured by
Get the current GPU memory usage .
be a list or a generator .
updates checklist and returns an updated state .
Finds the production rule matching the filter function in the given type 's valid action
computes a linking between the two , and constructs an
returns the indices of each entity 's neighbors .
Produces a tensor with shape `` ( batch_size , num_entities ) `` that encodes each entity 's
Produces the probability of an entity given a question word and type .
track three metrics here :
creates the LambdaGrammarStatelet object that 's used for decoding .
computing logical form accuracy ( which is expensive
overrides `` Model.decode `` , which gets called after `` Model.forward `` , at test
Gets the logits of desired terminal actions yet to be produced by the decoder , and
Given a query ( which is typically the decoder hidden state ) , compute an attention over the
collect completed paths of at most `` self._max_path_length `` steps .
Check that all the instances have the same types .
Gets the maximum padding lengths from all `` Instances `` in this batch .
converts this `` Batch `` into a set of pytorch Tensors that can be passed
Based on the current utterance , return a dictionary where the keys are the strings in
create a new `` Grammar `` object from the one in `` AtisSqlTableContext `` , that also
add a new expression , there may be other expressions that refer to
is a helper method for generating sequences , since we often want a list of expressions
is a helper method for adding different types of numbers ( eg .; starting time ranges ) as entities .
gets entities from the current utterance finds which tokens they are linked to .
Return a sorted list of strings representing all possible actions
get the entities and the linking scores in `` _get_linked_entities ``
Creates a Flask app that serves up a simple configuration wizard .
string paths .
loads the params from a file .
Trains the model specified in the given : class : ` Params ` object , using the data and training
handles divide-by-zero .
One sentence per line, formatted like
max_vocab_size limits the size of the vocabulary , not including the @ @ UNKNOWN @ @ token .
be reloaded later .
Loads a `` Vocabulary `` that was serialized using `` save_to_files `` .
have a vocabulary file for a trained model somewhere , and you really want to
Constructs a vocabulary given a collection of ` Instances ` and some parameters .
are two possible ways to build a vocabulary ; from a
be used for extending already generated vocabulary .
generated vocabulary using a collection of instances .
are padding and OOV tokens added to the given namespace .
return the index of; Adds `` token `` to the index , if it is not already present .
Computes the regularization penalty for the model .
has raw text in it ,
Takes a list of : class : ` ~allennlp.data.instance.Instance ` s , converts that text into
runs inference / decoding / whatever
checks the device of the model parameters to determine the cuda_device
warns once if a user implements a model which returns a dictionary with
Instantiates an already-trained model , based on the experiment
embedding modules in the model and assures it can embed
be used guide search .
Takes a logical form , and the list of target values as strings from the original lisp
takes a list of rows and a column name and returns a list of strings as
takes a row ( as a list ) and a column name and returns the number in that
takes a row as a list and a column name and returns the date in that column .
Takes a row and a column and returns a list of rows from the full set of rows that contain
returns a `` Date `` object whose year , month , and day are the three
evaluates to a list of rows , and returns the first one in that
evaluates to a list of rows , and returns the last one in that
evaluates to a single row , and returns the row that occurs before
evaluates to a single row , and returns the row that occurs after
Takes a list of rows and a column and returns the most frequent values ( one or more ) under
Takes a list of rows and a column and returns the most frequent value under
Takes a list of rows and a column name and returns a list containing a single row ( dict from
Takes a list of rows and a column and returns a list containing a single row ( dict from
Takes a list of rows and a column and returns the max of the values under that column in
Takes a list of rows and a column and returns the mean of the values under that column in
returns the difference between the values under
does not occur in the; Takes a row and returns its index in the full list of rows .
be called on nodes of a logical form tree , which are either non-terminal
given action , returns at most `` max_num_paths `` paths to the root ( production with
Returns a mapping from each `MultiMatchNamedBasicType` to all the `NamedBasicTypes` that it
Takes a logical form as a string , maps its tokens using the mapping and returns a parsed expression .
Returns the sequence of actions ( as strings ) that resulted in the given expression .
is useful if you want; constructs a logical form from it .
Given a current node in the logical form tree , and a list of actions in an action sequence ,
Takes a type signature and infers the number of arguments the corresponding function takes .
is the result of parsing a logical form in Lisp format .
add a name and its translation to the local name mapping , and the corresponding
Creates a server running SEMPRE that we can send logical forms to for evaluation .
Averaged per-mention precision and recall .
Counts the mentions in each predicted cluster which need to be re-allocated in
Computes the mention F measure between gold and
Computes the Constrained EntityAlignment F-Measure ( CEAF ) for evaluating coreference .
returning a new grammar state with whatever
Clips gradient norm of an iterable of parameters.
Move the optimizer state to GPU , if necessary .
Assumes a well-formed batch ,; Returns the size of the batch dimension .
seconds past Epoch to human readable string .
Convert human readable string to datetime.datetime.
specified by the config .
creates the serialization directory if it does n't exist .
using multiple GPUs .; is a simplification
Is a no-op if gradient rescaling is not enabled .
Gets the metrics but sets `` `` loss '' `` to
Parse all dependencies out of the requirements.txt file.
Parse all dependencies out of the setup.py script.
Given a sentence , return all token spans within the sentence .; are ` inclusive ` .
Given a sequence corresponding to BIO tags , extracts spans .
Given a sequence corresponding to IOB1 tags , extracts spans .
Given a sequence corresponding to BIOUL tags , extracts spans .
Given a tag sequence encoded with IOB1 labels , recode to BIOUL .
Given a sequence corresponding to BMES tags , extracts spans .
Just converts from an ``argparse.Namespace`` object to params.
Check if a URL is reachable .
Check if a file in this repository exists .
be feeding params dicts to functions we do n't own ;
filter out non-encodable values .
Given a `` flattened '' dict with compound keys , e.g .
preferring values from ` preferred ` .
is required in order to deal with
request that some of its
associated with dict.pop ( key ) , along with checking for
Performs a pop and coerces to an int.
Performs a pop and coerces to a float.
Performs a pop and coerces to a bool.
associated with dict.get ( key ) but also checks for returned
Gets the value of `` key `` in the `` params `` dictionary , ensuring that the value is one of
need to just represent the parameters as a dict , for instance when we pass
Returns the parameters of a flat dictionary from keys to values .
Raises a `` ConfigurationError `` if `` self.params `` is not empty .; take `` class_name `` as
Load a ` Params ` object from a configuration file .
Returns Ordered Dict of Params from list of partial order preferences.
representing the current state of this `` Params `` object .
keeps the patience and should_decrease settings .
use this to serialize the state of the metric tracker .
update the various things that depend on it .
add multiple metrics at once .
has stopped for long enough .
Archive the model weights, its training configuration, and its
Instantiates an Archive from an archived ` tar.gz ` file .
be used to load a module from the pretrained model archive .
Takes a list of possible actions and indices of decoded actions into those possible actions
evaluates to the correct denotations over all
learning rate finder for given args
learning rate search for given ` num_batches ` and saves the results in `` serialization_dir ``
using : class : ` ~allennlp.training.trainer.Trainer `
Exponential smoothing of values
Compute a weighted average of the `` tensors `` .; be any shape
used when some of the arguments to the function are meant to be
Given an `` nltk.Tree `` representing the syntax tree that generates a logical form , this method
get from a type annotation ) into a
Executes a logical form , using whatever predicates you have defined .
Executes the program defined by an action sequence directly , without needing the overhead
Induces a grammar from the defined collection of predicates in this language and returns
Returns a sorted list of all production rules in the grammar induced by
Converts a logical form into a linearization of the production rules from its abstract
produced by : func : ` logical_form_to_action_sequence ` , which is a
do this with the `` @ predicate ``; Adds a predicate to this domain language .
Adds a constant to this domain language .
is a valid non-terminal in the grammar .
does the bulk of the work of executing a logical form , recursively executing a single
does the bulk of the work of : func : ` execute_action_sequence ` , recursively executing
is used when converting a logical form into an action sequence .
gets the transitions for the predicate
Chooses ``num_samples`` samples without replacement from [0, ..., num_words).
Takes a list of tokens and converts them to one or more sets of indices .
pads a list of tokens to `` desired_num_tokens `` and returns a padded copy of the
includes 2 annotated spans which are identical ,
Join multi-word predicates to a single
Converts a list of model outputs (i.e., a list of lists of bio tags, each
Return the word indices of a predicate in BIO tags .
Get the predicate in this prediction .
Tests whether the predicate in BIO tags1 overlap
Generate a coherent tag , given previous tag and current label .
Assumes the predicate in tags1 overlap with
are part of a multiword predicate
Sanitize a BIO label - this deals with OIE
representing the sentences with encoded characters
Compute context insensitive token embeddings for ELMo representations.
Given a list of tokens , this method precomputes word representations
is very similar to that done by the normalization functions in
Converts a character span from a passage into the corresponding token span in the tokenized
Finds a list of token spans in `` passage_tokens `` that match the given `` answer_texts `` .
Converts a question, a passage, and an optional answer (or answers) to an ``Instance`` for use
Process a list of reference answers.
acts the same as the static method `` BidirectionalAttentionFlow.get_best_span ( ) ``
do batch processing , or it can be really slow .; lets you take
Return a new BeamSearch instance that 's like this one but with the specified constraint .
remove punctuation , articles and extra whitespace .
predicted answer sets and first finds a greedy 1-1 alignment
Takes a predicted answer and a gold answer ( that are both either a string or a list of
converts it into strings used for
predicted answers and evaluates the predictions for each question
Takes a prediction file and a gold file and evaluates the predictions for each question in the
call this method , we will use this directory to store a cache of already-processed
containing all the instances
Restores a model from a serialization_dir to the last saved checkpoint .
predicate argument predictions and gold labels for a single verbal
formatted SRL tags to the format required for evaluation with the
Given a `` sentence `` , returns a list of actions the sentence triggers as an `` agenda `` .
lead to them .
is the most frequent
touch the given set of objects .
is done separately for each; Return the topmost objects ( i.e .
is done separately for; Return the bottom most objects ( i.e .
is , if; are above the given objects .
is , if; are below the given objects .
Returns true iff the objects touch each other.
Given a set of objects , separate them by the boxes they belong to and return a dict .
returns an attribute value that
Given a possibly complex data structure ,
Given a structure ( possibly ) containing Tensors on the CPU ,
Supports sparse and dense tensors.
Takes a list of tensor dictionaries , where each dictionary is assumed to have matching keys ,
Given a variable of shape `` ( batch_size , ) `` that represents the sequence lengths of each batch
Sort a batch first tensor by some specified lengths .
Given the output from a `` Seq2SeqEncoder `` , with shape `` ( batch_size , sequence_length ,
given tensor , where
does not work if some elements of `` vector `` should be
calculate max along certain dimensions on masked values
calculate mean along certain dimensions on masked values
affecting masked entries .
decoding in log space over a sequence given a transition matrix
produced by a `` TextField `` and returns a mask
Takes a matrix of vectors and a set of weights over the rows in the matrix ( which we call an
Computes the cross entropy loss of a sequence , weighted with respect to
masked values in `` tensor `` with `` replace_with `` .
make sure that the tensors have the same shape ,
torch.load ( ) ` a GPU-trained model onto a CPU ( or specific GPU ) ,
using element-wise operations and concatenation , specified by a
is equal to obj .
does a weighted ( linear ) multiplication while combining .
computes the resultant dimension when
is mathematically equivalent to
is a subroutine for : func : ` ~batched_index_select ` .; given `` indices `` of size
given `` indices `` of size `` ( batch_size , d_1 , ... , d_n ) `` indexes into the sequence
given `` indices `` of size `` ( set_size , subset_size ) `` specifies subsets of the `` target ``
starting at 0 .
given values ( designed for distances ) into `` num_total_buckets `` semi-logscale
Add begin/end of sentence tokens to the batch of sentences.
Remove begin/end of sentence embeddings from the batch of sentences.
Implements the frequency-based positional encoding described
Produce N identical layers.
Given a ( possibly higher order ) tensor of ids with shape
Given a tensor of embeddings with shape
occurs in the table , and if it does , returns the names of the columns
are the transformation rules used to normalize cell in column names in Sempre .
Takes a logical form as a lisp string and returns a nested list representation of the lisp .
Computes the ELMo embeddings for a single tokenized sentence .
Computes the ELMo embeddings for a batch of tokenized sentences .
Computes the ELMo embeddings for a iterable of sentences .
contains a sentence tokenized by whitespace .
existing fields mapping .
given `` counter `` for all of the vocabulary items in all of the
using the provided `` Vocabulary `` .
padding lengths , keyed by field name .
given in `` padding_lengths `` ( which is
Return the full name ( including module ) of the given class .
was registered under .
Inspect the docstring and get the comments for each parameter .
Create the `` Config `` for a class by reflecting on its `` __init__ ``
Pretty-print a config in sort-of-JSON+comments.
Render a single config item , with the provided indent
Return a mapping { registered_name - > subclass_name }
Convert `url` into a hashed filename in a repeatable way.
Return the url and etag ( which may be `` None `` ) stored for ` filename ` .
Given something that might be a URL ( or might be a local path ) ,
Split a full s3 path into the bucket name and path .
create more helpful error
Check ETag on S3 object.
Pull a file directly from S3 .
Given a URL , look for the corresponding dataset in the local cache .
Extract a de-duped collection ( set ) of text from a file .
Processes the text2sql data into the following directory structure :
Apply dropout to this layer , for this whole mini-batch .
returns a JSON representation containing sentences , labels , correct and
lets you take advantage of spacy 's batch processing .
yielding all sentences processed .
returning file_paths in a directory
formatted files which yields documents , regardless
formatted file .
given coref label , add it to a currently open span ( s ) , complete a span ( s ) or
Given a sequence of different label types for a single word and the current
Prints results from an ``argparse.Namespace`` object.
Apply dropout to input tensor .
return the metric .
passed gradient-tracking Tensors to a Metric , there will be
Replaces abstract variables in text with their concrete counterparts.
unifies a SQL query .; involves unifying quoted strings
use ID as a column reference to the
Reads a schema from the text2sql data , returning a dictionary
reading in text2sql data .
exists because Pytorch RNNs require that their inputs be sorted
Returns an initial state for use in an RNN.
has run forward , the states need to be updated .
Takes a list of valid target action sequences and creates a mapping from all possible
Convert the string to Value object.
Convert a list of strings to a list of Values
is correct .
Try to parse into a number .
Try to parse into a date .
Given a sequence tensor , extract spans and return representations of
serialization_directory : str, required.
transitioning from state to state , and a
Returns the state of the scheduler as a ``dict``.
Load the schedulers state.
Identifies the best prediction given the results from the submodels .
Load the pre-trained weights from the file .
Takes a type and a set of basic types , and substitutes all instances of ANY_TYPE with all
Takes a complex type ( without any placeholders ) , gets its return values , and returns productions
Generates all the valid actions starting from each non-terminal .
Gives the final return type for this function .; takes a single argument ,
Gives the types of all arguments to this function .; returning a basic type ,
replaces any instances of `` ANY_TYPE `` inside this
override this method to do just one thing on top of `` ApplicationExpression._set_type `` .
Send the mean and std of all parameters and gradients to tensorboard , as well
learning rates to tensorboard
Send histograms of parameters to tensorboard.
provided ) to tensorboard .
Create explanation (as a list of header/content entries) for an answer
stemming to attempt alignment between extracted world and given world literals .
matching between time-steps of vectors
matching between each time step of
Given the forward ( or backward ) representations of sentence1 and sentence2 , apply four bilateral
Training data in WikitableQuestions comes with examples in the form of lisp strings in the format :
returns answer index ( or; executing friction logical forms .
Given an utterance , we get the numbers that correspond to times and convert them to
is not explicitly mentioned in the utterance , the query assumes that
Given an utterance , this function finds all the numbers that are in the action space .
Given a digit in the utterance , return a list of the times that it corresponds to .
Given a list of times that follow a word such as `` about `` ,
evaluate here whether the predicted query and the query label evaluate to the
Formats a dictionary of production rules into the string format expected
initialize the valid actions with the global actions .
formats an action as it appears in models .
accumulate the rules that generated its children in a list .
changes the order in which; See the `` NodeVisitor `` visit method .
is a predominately variable free language in terms of simple usage , in the
be treated as numbers or strings if their type can be inferred -
do n't have vocabularies or weights of their own , so they override _load .
following original implementation .
~allennlp.run ` command only knows about the registered classes in the `` allennlp ``
has a list of `` Tokens `` , and each `` Token `` gets converted into arrays by
Creates ELMo word representations from a vocabulary file.
Sorts the instances by their padding lengths , using the keys in
Take the question and check if it is compatible with either of the answer choices .
Creates a Flask app that serves up the provided `` Predictor ``
serving up an input form with the
Returns the valid actions in the current grammar state .; See the class docstring for a
decodes the _maximum_
Applies the chu-liu-edmonds algorithm recursively
Replace all the parameter values with the averages.
Restore the backed-up ( non-average ) parameter values .
batch_size , length_1 , length_2 ,
be used to prune the set of unfinished states on a beam or finished states
Returns the best finished states for each batch instance based on model scores .
embedding matrix for the given vocabulary using the pretrained embeddings
Read pre-trained word vectors from an eventually compressed text file, possibly contained
embedding matrix is assumed to; formatted file .
takes in input a string and if it contains 1 or 2 integers , it assumes the
Gets the embeddings of desired terminal actions yet to be produced by the decoder , and
Pulls at most ``max_instances_in_memory`` from the input_queue,
puts them in the input_queue .
Returns a list of valid actions for each element of the group.
pulls filenames off the input queue , uses the dataset reader
Given labels and a constraint type , returns the allowed transitions .
Given a constraint type and strings `` from_tag `` and `` to_tag `` that
Computes the ( batch_size , ) denominator term for the log-likelihood , which is the
Computes the numerator term for the log-likelihood , which is just score ( inputs , tags )
Computes the log likelihood .
viterbi algorithm to find most likely tags for the given inputs .
Given a starting state and a step function , apply beam search to find the
takes a certain arg .
takes in any positional arguments .
are actually represented as Union [ X , NoneType ] .
Given some class , a ` Params ` object , and potentially other keyword arguments ,
Given a dictionary of extra arguments , returns a dictionary of
Does the work of actually constructing an individual argument for : func : ` create_kwargs ` .
is the automatic implementation of ` from_params ` .; subclasses ` FromParams `
defines the computation
was changed to Tensor.sparse_mask .
Parses a chunk of text in the SemEval SDP format .
Disambiguates single GPU and multiple GPU settings for cuda_device param.
loads the model archive from a file .
tunes the given model , using a set of parameters that is largely identical to those used
Extracts the top-k scoring items with respect to the scorer.
Add the epoch number to the batch instances as a MetadataField.
Take the next ` max_instances ` instances from the given dataset .
Breaks the dataset into "memory-sized" lists of instances,
is specified , then split the batch
Returns the number of batches that `` dataset `` will be split into ; if you want to track
return one epoch worth of batches .
use carriage returns to get the training line to update for each batch
captures the internal-module outputs of
objects into a list of : class : ` ~allennlp.data.instance.Instance ` s .
Instantiate a : class : ` Predictor ` from an archive path .
Instantiate a : class : ` Predictor ` from an : class : ` ~allennlp.models.archival.Archive ` ;
'Scaled Dot Product Attention
Mask out subsequent positions.
Helper: Construct a model from hyperparameters.
Pass the input (and mask) through each layer in turn.
Apply residual connection to any sublayer with the same size.
left ) for connections .
preserves output variance for approximately gaussian
allows initializing model parameters in `` blocks '' .
Initialize the biases of the forget gate to 1 , and all other gates to 0 ,
Converts a Params object into an InitializerApplicator.
assume the first line in the file is a tab; read tables formatted as TSV files here .
is useful when you are reading; read tables formatted as JSON objects ( dicts ) here .
returns them as strings .; do some simple heuristic
return a list of; Splits a cell into parts and returns the parts of the cell .
is any cell in this column that can be split .
're just doing the same thing that SEMPRE did; be split .
be linked to spans in the question , that should be in the agenda ,
inp_fn: str, required.
Return an Element from span (list of spacy toks)
Ensure single word predicate
Return a conll representation of a given input Extraction .
Return an integer tuple from
Construct an Element instance from regexp
Parse a raw element into text and indices ( integers ) .
Given a list of extractions for a single sentence -
conform to ontonotes representation .
Given a dictionary from sentence - > extractions ,
Given a Kinesis record data that is decoded , deaggregate if it was packed using the
Parses a S3 Uri into a dictionary of the Bucket , Key , and VersionId
Constructs a S3 URI string from given code dictionary
Constructs a Lambda ` Code ` or ` Content ` property , from the SAM ` CodeUri ` or ` ContentUri ` property .
knows how to interpret and handle
Is there policies data in this resource ?
Returns the type of the given policy
is a dictionary with one key which is the name; Is the given policy data a policy template ?
is a no-op if there are no global properties
Takes a SAM template as input and parses the Globals section
is used as part of the recursion .; perform the merge operation for the given inputs .
Merges the two dictionaries together
Returns the token type of the input .
Is this a valid SAM template dictionary
Generates a number within a reasonable range that might be expected for a flight .
Generates a number within a reasonable range that might be expected for a hotel .
dialog management and fulfillment for booking a hotel .
dialog management and fulfillment for booking a car .
Called when the user specifies an intent for this bot .
Adds the appropriate managed; Returns the Lambda EventSourceMapping to which this pull event corresponds .
triggers a Lambda function whose execution role is auto-generated by SAM , add the
read default values for template parameters and merge with user supplied values .
pseudo parameter values
Add this deployment preference to the collection
wanted to initialize the session to have some attributes we could
Sets the color in the session and prepares the speech to reply to the
Called when the user specifies an intent for this skill
Route the incoming request based on type ( LaunchRequest , IntentRequest ,
based on the prefix and given data .; ensures that the logicalId is
return a hash of data that can be used as suffix of logicalId
Stable, platform & language-independent stringification of a data with basic Python type.
Add the information that resource with given ` logical_id ` supports the given ` property ` , and that a reference
Returns the value of the reference for given logical_id at given property .
leverages KMS encrypt and base64-encode encrypted blob
Transforms the SAM defined Tags into the form CloudFormation is expecting .
is not provided , this method will; Gets the name of the partition given the region name .
gets called before the SAM template is processed .
True if this Swagger has the given path and optional method
given method contains a valid method definition .
Returns the swagger contents of the given method .; checks to see if a conditional block
is already present at the given path/method
Adds the path/method combination to the Swagger , if not already present
given path+method .
Wrap entire API path definition in a CloudFormation if condition.
add a OPTIONS response config to the Swagger that
containing configuration for OPTIONS HTTP Method to configure CORS .
defined for this; Creates the value for Access-Control-Allow-Methods header for given path .
Add Authorizer definitions to the securityDefinitions part of Swagger.
be set if an Authorizer; Sets the DefaultAuthorizer for each method on this path .
consist solely of Authorizers
Add Gateway Response definitions to Swagger.
Returns a **copy** of the Swagger document as a dictionary.
is a Swagger document
know how to handle API Gateway specific methods; normalized version of HTTP Method .
runs before a template gets transformed .; parse and process Globals section
succeeds only for inputs of the provided valid_type .
succeeds only if the input is a list , and each item in the list passes as input
succeeds only if the input is a dict , and each key and value in the dict passes
succeeds only if the input passes at least one of the provided validators .
given values for each parameter , this method will return a policy statement that can be used
given input contains values for all parameters used by this template
Parses the input and returns an instance of this class .
are added to the end of the plugins list .
Retrieves the plugin with given name
is to invoke the hook function on all registered plugins .
leverages KMS decrypt and base64-encode decrypted blob
Demonstrates S3 trigger that uses
Prepends the first argument ( i.e. , the exception message ) of the a BaseException with the provided message .
Validate the incoming token and produce the principal user identifier
loops over an array of objects containing a resourceArn and
gets called before `` each '' SAM resource gets processed
process aggregated records sent to KinesisAnalytics .
Translates the SAM manifest provided in the and returns the translation to CloudFormation .
return a dictionary of API Events on the function
process given API events .; adds the APIs to Swagger JSON in the respective Serverless : :Api
Adds the API path/method from the given event to the Swagger JSON of Serverless : :Api resource this event
Get API logical id from API event properties.
Decides whether to add a condition to the implicit api resource .
combines the given list of conditions .
implicit API paths if necessary .
given API logical id and swagger resource path .
are tentatively added to the template for uniform handling of both Implicit & Explicit
Sets up the resource such that it will triggers a re-deployment when Swagger changes
is seperate from the main , public invoke method so that other code within this SDK can
Read at most amt bytes from the stream .
specified : code : ` function_arn ` .
Retrieve the next work item for specified : code : ` function_arn ` .
Post the result of processing work item by : code : ` function_arn ` .
Post the error message from executing the function handler for : code : ` function_arn `
Retrieve the result of the work processed by : code : ` function_arn `
process this application
handles the get_application API call to the serverless application repo
handles the create_cloud_formation_template API call to the serverless application repo
does not , an; make sure it has a specific key .
gets called after the template is processed
Handles the response from the SAR service call
Handles service calls and exception management for service calls
Validates the template and parameter values and raises exceptions if there 's an issue
filtering by type
overwrite , if the logicalId is already used .; Adds the resource to dictionary with given logical Id .
Gets the resource at the given logicalId if present
given plugins ,; returns a plugins object with the given list of plugins installed .
Loads the SAM resources from the given SAM manifest , replaces them with their corresponding
iterate , order them based on the following order :
Constructs a Resource object with the given logical id , based on the given resource dict .
is an alphanumeric string .
provided resource dict contains the correct Type string , and the required Properties dict .
have been provided , then returns a dict
Generates the resource dict for this Resource , the value associated with the logical id in a CloudFormation
have been populated , and that all properties have
are top-level entries of a CloudFormation resource; attributes on resource .
Gets the resource attribute if available
provides value for this attribute .; does not provide
Constructs the list of supported resource references by going through the list of CFN resources generated
Returns the Resource class corresponding to the 'Type ' key in the given resource dict .
Build a responseCard with a title , subtitle , and an optional set of options which should be displayed as buttons .
included ) and max ( excluded )
feed into a backend API to provide query schedule availability .
check if the given time and duration fits within a known set of availability windows .
return the windows of availability of the given duration , when provided a set of 30 minute windows .
Build a string eliciting for a possible time slot among at least two availabilities .
Build a list of potential options for a given slot , to be used in responseCard generation .
dialog management and fulfillment for booking a dentists appointment .
Demonstrates a simple HTTP endpoint using API Gateway .
using Fn : :Or .; accepts up to 10 conditions ,
given input is an intrinsic function dictionary .; is a dictionary with single
contains only one key and is of the given intrinsic_name
Splits a resource reference of structure `` LogicalId.Property '' and returns the `` LogicalId '' and `` Property ''
are present in the parameters and returns the value .; is not in parameters ,
are runtime properties which ca n't be converted
generated ) logical id .
found within the string of ` Fn : :Sub ` intrinsic function
string to be substituted , there could be either a
resolving replacements in the Sub action based on the handler that is passed as an input .
are interested in parsing the $ { } syntaxes inside; handle value to Fn : :Sub key .
is using $ { key } syntax by calling the ` handler_method ` on every
Resolve resource references within a GetAtt dict.
Resolves the function and returns the updated dictionary
resolves `` Fn : :FindInMap '' references that are present in the mappings and returns the value .
enhenced monitoring DATA_MESSAGE ,
returns the ApiGateway RestApi .
Constructs the RestApi 's ` BodyS3Location property ` _ , from the SAM Api 's DefinitionUri property .
returns the ApiGateway Deployment .
returns the ApiGateway Stage .
Generates CloudFormation resources from a SAM API resource
Add CORS configuration to the Swagger file, if necessary
Add Auth configuration to the Swagger file, if necessary
Add Gateway Response configuration to the Swagger file, if necessary
returns the Lambda Permission resource allowing the Authorizer to invoke the function .
endpoint configuration property of AWS : :ApiGateway : :RestApi resource
keep retrying ` task_to_try ` until either :
Returns the Lambda function , role , and event resources to which this SAM Function corresponds .
tries to extract alias name from a reference; be supplied as an intrinsic function .
returns the Lambda function .
Constructs a Lambda execution role based on this SAM function 's Policies property .
Validates whether the DeadLetterQueue LogicalId is validation
returns the resources associated with this function 's events .
Constructs a Lambda Version resource that will be auto-published when CodeUri of the function changes .
Constructs a Lambda Alias for the given function and pointing to the given version
Returns the API Gateway RestApi , Deployment , and Stage to which this SAM Api corresponds .
Constructs a AWS : :CloudFormation : :Stack resource
is using the serverless app repo
Returns the Lambda layer to which this SAM Layer corresponds .
is 'Retain ' .; Sets the deletion policy on this resource .
dialog management and fulfillment for ordering flowers .
Constructs the Lambda Permission resource allowing the source service to invoke the function this event
Returns the CloudWatch Events Rule and Lambda Permission to which this Schedule event source corresponds .
Constructs the Target property for the CloudWatch Events Rule .
Returns the Lambda Permission resource allowing S3 to invoke the function this event source triggers .
Make the S3 bucket depends on Lambda Permissions resource because when S3 adds a Notification Configuration ,
is not supported this undocumented way of
Returns the Lambda Permission resource allowing SNS to invoke the function this event source triggers .
resolve the reference and grab
has a RestApi property , then simply return the Lambda Permission resource allowing
Adds the path and method for this Api event source to the Swagger body for the provided RestApi .
given dictionary recursively .
provide a reference to a `` derived '' SAM resource such as Alias of a Function or Stage of an API
have their logical ids mutated from the original id that the customer writes in the
performs the actual traversal of input and calls the appropriate ` resolver_method ` when
Traverse a dictionary to resolve intrinsic functions on every value
Traverse a list to resolve intrinsic functions on every element
Try to resolve parameter references on the given input object .; be of any type .
given object looks like one of the; Try to resolve SAM resource references on the given template .
given object looks like one of the; Try to resolve SAM resource id references on the given template .
represent an intrinsic function in it ?
Returns the CloudWatch Logs Subscription Filter and Lambda Permission to which this CloudWatch Logs event source
Converts the given template to IAM-ready policy statement by substituting template parameters with the given
Is this a valid policy template dictionary
Render a chart or page to local html files .
padding being optional .

Parses a string and returns a pin-num .
Returns the numbered function ( i.e .
Prints the C representation of this AF.
Start the loop .
matches the provided object_file .
Find any MP_REGISTER_MODULE definitions in the provided c file.
Generate header with module table entries for builtin modules.
Reads test files
converts CPython module names into MicroPython equivalents
indents paragraphs of text for rst formatting
creates a table given any set of columns
restructured text documents to display tests
Initializes the found DFU device so that we can program it .
erases the entire device .
Erases a single page.
Sets the address for the next operation .
assumes that memory has; Writes a buffer into memory .
assumes that memory has already; Writes a single page .
start running the program .
Parses the struct defined by ` fmt ` from ` data ` , stores the parsed fields
Reads a DFU file , and parses the individual elements from the file .
are currently in DFU mode .
identifies the memory layout .
detected in DFU mode .
Writes the indicated elements into the target memory ,
Prints a progress report suitable for use on the command line.
verifying this files functionality .
Parses a string and returns a ( port-num , pin-num ) tuple .
Parses a string and returns a ( port , gpio_bit ) tuple .
run one operator and return the results .
Load data from an external file for tensor.
Loads external tensors into model
set all tensors as external data .; saves all the tensors data as external data after calling this function .
saves all the tensors data as embedded data after calling this function .; set all tensors data as embedded data .
according to information in the ` external_data ` field .
Create an iterator of tensors from node attributes of an ONNX model.
Remove a field from a Tensor 's external_data key-value store .
Write external data of all tensors to files on disk.
returns a handle to it
return a handle
Construct a NodeProto.
Construct an OperatorSetIdProto.
convert the input to a bytes or to False .
based on the value type .
based on the data type and shape .
doc_string ` field on any nested protobuf messages
object to a numpy array .
Converts a numpy array to a tensor def.
Serialize a in-memory proto to bytes
bytes into a in-memory proto
Loads a serialized ModelProto into memory
Loads a serialized TensorProto into memory
Saves the ModelProto to the specified path .
combines several useful utility functions together .
Unrolls an RNN cell across time steps.
Change attribute names as per values in change_map dictionary.
attributes in the remove list from the input attribute dict
Changing onnx 's pads sequence to match with mxnet 's pad_width
pooling operator supports asymmetrical padding
do n't provide this attribute ,
reshape bias term to ( 1 , num_channel ) .
getting 'channels ' or 'units ' since onnx do n't provide
Using FullyConnected operator in place of linalg_gemm to perform same operation
obtain the shape of an array
r"""Resize image with OpenCV.
Decode an image to an NDArray.
's larger than image size .
Pad image border with OpenCV.
Get the interpolation method for resize functions .
Resizes shorter edge to size.
resize it to size .
given ` size ` by trimming on all four
Normalize src with mean and std.
Randomly crop src with size.
Creates an augmenter list .
Saves the Augmenter to string
avoid duplicate dump .
Resets the iterator to the beginning of the data .
Resets the iterator and ignore roll over data
reading in next sample .
Helper function for batchifying data
Decodes a string or byte string to an NDArray .
returns the decoded raw bytes .
integer vectors .
character vectors .
character vectors for batch .
Add a pooling layer to the model .
contain complete frames
Write data to csv file
crop center and resize
construct and return generator
construct and return descriptor
Compute the length of the output sequence after 1D convolution along
Compute the spectrogram for a real signal .
Calculate the log of linear spectrogram from FFT energy
cropping boxes according to parameters
is larger than threshold
padding boxes according to parameters
running a function
Print information about the annotation file.
filtering parameters .
Load anns with the specified ids.
Load cats with the specified ids.
Display the specified annotations .
Download COCO images from mscoco.org server.
contains { imageID , x1 , y1 , w , h , score , class }
be polygons , uncompressed RLE to RLE .
Save cnn model
Construct highway net
Train cnn model
Collate data into batch.
shared memory for stacking .
Move data into new context.
multiprocessing DataLoader .
fetching data from queue and put in reorder dict .
processing data in worker process .
Assign next batch workload to workers.
pushing terminate signals .
are used .
converted to strings ) in a dictionary
A wrapper for the user-defined handle.
Creates a new KVStore .
Initializes a single or a sequence of key-value pairs into the store .
Pushes a single or a sequence of key-value pairs into the store.
Pulls a single value or a sequence of values from the store .
Pulls a single RowSparseNDArray value or a sequence of RowSparseNDArray values \
Specifies type of low-bit quantization for gradient compression \
Registers an optimizer with the kvstore.
Returns the type of this kvstore .
Returns the rank of this worker node .
Returns the number of worker nodes .
is often used when checkpointing; Saves the optimizer ( updater ) state to a file .
Loads the optimizer ( updater ) state from the file .
Sets a push updater into the store.
Sends a command to all server nodes .
Add a module to the chain .
Gets current parameters.
Initializes parameters.
is necessary before one; Binds the symbols to construct executors .
initializes optimizers .
Forward computation.
according to installed optimizer and the gradient computed
outputs from a previous forward computation .
Gets the gradients with respect to the inputs of the module .
accumulates evaluation metric on outputs of the last forward computation .
Installs monitor on all executors.
Generate the iterator of mnist dataset
is used to run predictions on the audio files in the directory ` pred_directory ` .
Thread loop for generating data
processes if not already started
Resets the generator by stopping all processes
Create a base class with a metaclass .
searching possible path .
array from a Python array .
const void * * from a list of MXNet objects with handles .
Convert a ctypes pointer to a numpy array .
Build argument docs in python style.
Append the definition position to each function contained in module .
op functions created by ` make_op_func ` under
created by ` op_code_gen_func ` and write to the source file
is turned off by default in backend .
is currently turned on .
Wraps a function with an activated NumPy-compatibility scope.
computes the root relative squared error ( condensed using standard deviation formula )
computes the relative absolute error ( condensed using standard deviation formula )
computes the empirical correlation coefficient
Get input size
Convert convolution layer parameter from Caffe to MXNet
pooling layer parameter
Parse Caffe prototxt into symbol string
Convert caffe model definition into Symbol
Complete an episode's worth of training for each environment.
parses the trained .caffemodel file
given audio clip , calculate the log of its Fourier Transform
Read metadata from the description file
Featurize a minibatch of audio , zero pad them and return a dictionary
Estimate the mean and std of the features from the training set
GRU Cell symbol
Traverses the root of directory that contains images and
write image list into the file .
.lst file .
Reads the .lst file and generates corresponding iterator .
preprocesses , packs the image and put it back in output queue .
be spawned to fetch the image
be spawned to fetch processed image
Defines all arguments.
normnalize an image nd array .
testing iterator for the CUB200-2011 dataset .
transform an image .
Sample a training batch ( data and label ) .
Return a batch .
Load mnist dataset
Check the library for compile-time features .; are maintained in libinfo.h and libinfo.cc
Check for a particular feature by name
make a directory to store all caches
find out which indexes correspond to given image set ( train or val )
given image index , find out full path
given image index , find out annotation path
preprocess all ground-truths
top level evaluations
is a template
write results files in pascal devkit path
python evaluation wrapper
get image size info
parser : argparse.ArgumentParser
train a model
create multiple random crop augmenters .
Create augmenters for detection.
Calculate areas for multiple labels
Calculate intersect areas, normalized.
Check if constrains are satisfied
according to crop box
Propose cropping areas
according to padding region
Generate random padding region
Validate label and its shape.
estimate label shape
parse object detection label .
Reshape iterator for data_shape or label_shape.
Override the helper function for batchifying data
returning next batch .
Override Transforms input data with specified augmentations.
Checks if the new label shape is valid
bounding boxes drawn .
is useful when
enumerating aspect ratios X
Return width, height, x center, and y center for an anchor (window).
Given a vector of widths ( ws ) and heights ( hs ) around a center
Enumerate a set of anchors for each aspect ratio wrt an anchor .
Enumerate a set of anchors for each scale wrt an anchor .
set atual shape of data
define deep speech 2 network
run lipnet training code using argument info
visualize [cls, conf, x1, y1, x2, y2]
Check the difference between predictions from MXNet and CoreML .
set gpu module
apply beam search for prediction result
Description : build network
Description : save parameter of network weight
Description : Setup the dataloader
Description : training for LipNet
Description : Print sentence for prediction result
Description : inference for LipNet
training for LipNet
Sample from independent categorical distributions
Sample from independent normal distributions
Sample from independent mixture of gaussian (MoG) distributions
NCE-Loss layer under subword-units input.
Download the BSDS500 dataset and return train and test iters.
Run evaluation on cpu.
get two list , each list contains two elements : name and nd.array value
return one dict which contains `` data '' and `` label ''
mxnet operator .
Construct symbol from onnx graph.
Get the model metadata from a given onnx graph .
Construct SymbolBlock from onnx graph.
convert to numpy array .
Convert a list of AttributeProto to a dict , with names as keys .
Reshapes both modules for new input shapes.
is a wrapper class for a regular optimizer that is
encapsulates two optimizers and; create a svrg optimizer .
is necessary before one; Binds the symbols to construct executors for both two modules .
supports data batches with different shapes , such as
Backward computation.
Computes the gradients over all data w.r.t weights of past
sees a portion of
accumulated in the KVStore to each device .
Calculates the gradient based on the SVRG update rule .
based on the SVRG update rule .
Trains the module parameters .
processing a data batch .
Get registrator function.
Get registrator function that allow aliases.
Get creator function
Tokenization/string cleaning for all datasets except for SST .
splits the data into words and generates labels .
is defined by the longest sentence .
based on a vocabulary .
based on a pretrained word2vec
preprocessed data for the MR dataset .
Generates a batch iterator for a dataset .
Load the pre-trained word2vec from file .
VGG 16 layers network
Get multi-layer perceptron
LeCun, Yann, Leon Bottou, Yoshua Bengio, and Patrick
Parse the arguments
Implements forward computation.
Implements backward computation
reset binding .
required by this module .
A list of names for the outputs of this module.
Gets states from all devices.
be specified .
Binding for a ` BucketingModule ` means setting up the buckets and binding the
change `` self.curr_module `` .
Prepares the module for processing a data batch .
Installs monitor on all executors
recording/not recording .
affects ctx.is_train in operator
Get status on recording/not recording.
Get status on training/predicting.
compute gradient for autograd .
parse head gradient for backward and grad.
Compute the gradients of heads w.r.t previously marked variables .
Compute the gradients of heads w.r.t variables .
recorded computation history as ` Symbol ` .
parse the text file and load it into three NDArray 's
MXNET_DLL int MXSymbolListAtomicSymbolCreators(mx_uint *out_size,
Read .caffemodel path and .params path as input from command line
Add a param to the .params file
.params file .; connected layer .
.params file .
is no such param in .caffemodel fie , silently ignore it .
Convert a Caffe .caffemodel file to MXNet .params file
comprising foreground and background examples
Register a subclass of CustomOpProp to the registry with name reg_name.
Declare dependencies of this operator for backward pass.
assigning into dst depending on requirements .
create new operators
Used to infer storage type of
Used to infer storage
Get index for new entry.
Closes the record and index files .
Returns the current position of read head .
Creates the index file from open record file
raise exception if failed
Run the doxygen make commands
Build mxnet .so lib
build r pdf
build scala for scala docs , java docs , and clojure docs to use
build scala doc and then move the outdir
build java docs and then move the outdir
move the outdir
Convert a markdown table to rst format
convert them into the rst format
returns if a line is within a code block
split lines into code and non-code blocks
Evaluate python source codes
needed for website presentation
given meta info
save a caffe model
_func with multi-process using params .
Split the number(sam_num) into numbers by n_tile
create a namedtuple with default values
overriding keys in a
return a dict of zipped fields
convert raw configuration to unified dictionary
Imports the ONNX model file , passed as a parameter , into MXNet symbol and parameters .
Returns the name and shape information of input and output tensors of the given ONNX model file .
wrapper for a small Convolution group
extract features from base network , attaching extra
the basic aggregation module for SSD detection.
weighting to loss .
Reshapes x to the same shape as y.
binded on img
Train a neural style network .
Load data/label from dataset
resize , sub mean , swap channels ...
Gets MNIST dataset
Get input slice from the input shape.
Check the argument names of symbol .
Load a list of arrays into a list of arrays specified by slices .
sharing data with an existing executor .
Load data and labels into arrays.
Perform a forward pass on each executor .
Update evaluation metric with label and current outputs.
Install monitor on all executors.
Set parameter and aux values.
Update metric with the current executor.
Clear all contents in the relay memory
Get Header Guard Convention for DMLC Projects.
Process a file.
Main entry function.
Print summary of certain result map.
Process a cpp file.
Process a python file.
Print summary of lint.
Return the server controller .
Run the server , whose behavior is like .
Generate function for ndarray op by handle and function name.
Create a NDArray function from the FunctionHandle .
Counts tokens in the specified string.
Return a new array of given shape and type , filled with zeros .
given shape and type , without initializing entries .
Creates an array from any object exposing the array interface .
Loads an array from file.
Loads an array dictionary or list from a buffer
Saves a list of arrays or a dict of str- > array to file .
Get the common prefix for all names
helps in inferring DType of args and auxs params
Creates prefix and params for new `Block`.
ParameterDict ` containing this : py : class : ` Block ` and all of its
[Deprecated] Please use save_parameters.
saved by ` save_parameters ` .
[Deprecated] Please use load_parameters.
block as a child of self .; Block ` s assigned to self as
Registers a forward pre-hook on the block .
Registers a forward hook on the block .
Applies `` fn `` recursively to every child block as well as self .
Block ` and its children .
Has no effect on
use another data type .
Print the summary of the model 's output and parameters .
Generic infer attributes.
json format that can be loaded by
Defines the forward computation .
saved by ` HybridBlock.export ` or
Calculates the expectation of the gradients per epoch for each parameter w.r.t number of batches
Calculates the variance of the gradients per epoch for each parameter w.r.t number of batches
do n't exist .
r"""AlexNet model from the `"One weird trick..." <https://arxiv.org/abs/1404.5997>`_ paper.
f1 , precision and recall on the entity class
Construct data iter
Generate network symbol
convert the caltech101 mat file to images
Build using CMake
Create a linear regression network for performing SVRG optimization .
r"""SqueezeNet model from the `"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters
parse operator attributes in required format .
convert padding format for pad operator .
convert string to list .
Helper function to get inputs
create a basic operator
convert weights and inputs .
attributes to onnx 's Conv operator
attributes to onnx 's ConvTranspose operator
attributes to onnx 's Crop operator
attributes to onnx 's Gemm operator
attributes to onnx 's BatchNormalization operator
attributes to onnx 's Tanh/Relu operator
attributes to onnx 's Pad operator
create extra transpose node for dot operator
attributes to onnx 's
attributes to onnx 's InstanceNormalization operator
attributes to onnx 's Elu/LeakyRelu/PRelu operators
attributes to onnx 's Softmax operator
attributes to onnx 's Concat operator
attributes to onnx 's Transpose operator
attributes to onnx 's LRN operator
attributes to onnx 's LpNormalization operator
attributes to onnx 's Dropout operator
attributes to onnx 's Clip operator
Helper function for scalar arithmetic operations
attributes to onnx 's ArgMax operator
attributes to onnx 's Reshape operator .
attributes to onnx 's Cast operator
attributes to onnx 's Slice operator
attributes to onnx 's Squeeze or Split
operator attributes to onnx 's Unsqueeze operator
attributes to onnx 's squeeze operator
attributes to onnx 's Pow operator
attributes to onnx 's ReduceSum operator
attributes to onnx 's HardSigmoid operator
attributes to onnx 's LogSoftMax operator
attributes to onnx 's ReduceL1 and ReduceL2 operators
attributes to onnx 's RandomUniform
attributes to onnx 's RandomNormal
attributes to onnx 's MaxRoiPool
attributes to onnx 's Tile
attributes to onnx 's Expand
Get the current executor
using the forward function .
tends to oputut 0 ,
Return ResNet Unit symbol for building ResNet
Return ResNeXt symbol of
Adapted from https : //github.com/tornadomeet/ResNet/blob/master/train_resnet.py
Creates a symbolic variable with specified name .
Creates a symbol that contains a collection of other symbols , grouped together .
Loads symbol from a JSON file.
Loads symbol from json string.
raised to powers from exp element .
Returns element-wise maximum of the input elements.
Returns element-wise minimum of the input elements.
Given the `` legs '' of a right triangle , returns its hypotenuse .
filled with ones on the diagonal and zeros elsewhere .
given shape and type , filled with zeros .
given shape and type , filled with ones .
given shape and type , filled with the given value ` val ` .
spaced values within a given interval .
Compute the histogram of the input data .
Split an array into multiple sub-arrays.
name string from the symbol , this function only works for non-grouped symbol .
corresponding input key from the symbol .
Gets all attributes from the symbol.
gets all attributes from the symbol and its children .
Sets an attribute of the symbol.
is a list of; Gets a new grouped symbol ` sgroup ` .
Gets a new grouped symbol whose output contains
Lists all the arguments in the symbol.
Lists all the outputs in the symbol.
Lists all the auxiliary states in the symbol.
Lists all arguments and auxiliary states of this Symbol.
given the known types
calling type inference API .
given the known shapes of
calling shape inference API .
symbol to a file .
symbol to a JSON string .
get NDArray lists handles from various inputs .
get an executor , allocate all the arguments needed .
returns it .
Gets the autodiff of current symbol .
Evaluates a symbol given arguments .
Return symbol for target backend.
Perform pixel-shuffling on the input .
calling the decorated function using an exponential backoff .
loaded with the provided model .
Creates a new MXNet module .
given validation record file
Initializes the parameters and auxiliary states .
do nothing but to keep a reference to
Actual implementation of the backward computation.
build a mapping
Returns the next batch of data .
Returns the singleton instance .
Get the variable given a name if one exists or create a new one if missing .
Reset before re-using the cell for another graph .
Initial state for this cell.
fused weight matrices into separate
Pack separate weight matrices into a single packed
Unroll an RNN cell across time steps.
Get activation function.
fused rnn weights
Unfuse the fused RNN in to a stack of rnn cells.
Append a cell into the stack .
resizing to given image dimensions and
given mxnet arguments
Run the layer comparison on a caffe model , given its prototxt , weights and mean .
Implementation of Breadth-first search (BFS) on caffe network DAG
Compare layer by layer of a caffe network with mxnet network
Entrypoint for compare_layers
Get executor to Stochastic Gradient Langevin Dynamics and/or Bayesian Dark Knowledge
Create copy of parameters
Parse command line arguments
Program entry point
Gatys et al.
Get symbol of mnist
Get synthetic gradient value
Get toy symbol
Run DistilledSGLD on mnist dataset
Run SGLD on toy dataset
Run DistilledSGLD on toy dataset
Run HMC on toy dataset
Run synthetic SGLD
loading pascal voc dataset
loading ms coco dataset
Converts a reshape layer from mxnet to coreml.
Convert a transpose layer from mxnet to coreml .
Convert a flatten layer from mxnet to coreml .
Convert a softmax layer from mxnet to coreml .
Convert an activation layer from mxnet to coreml.
Convert a leakyrelu layer from mxnet to coreml .
Convert an elementwise add layer from mxnet to coreml.
Convert a convolution layer from mxnet to coreml .
Convert a pooling layer from mxnet to coreml .
Convert a batchnorm layer from mxnet to coreml .
Convert concat layer from mxnet to coreml.
dmlc 's opts
Unfuses the fused RNN in to a stack of rnn cells .
using CUDNN or CPU kenrel
Wait for network service to appear
Convert symbol for detail information.
Creates a visualization ( Graphviz digraph object ) of the given computation graph .
Measure the accuracy of ResNet
Training with multiple GPUs
Take an executor 's underlying symbol graph and return its generated optimized version .
get an optimized trt executor .
detect all images in iterator
Return detections for batch
detecting multiple images
visualize detections in one image
is -1 for negative detections
wrapper for im_detect and visualize_detection
Runs the caffe upgrade tool on the prototxt to create a prototxt in the latest format .
prototxt the network structure
formatted mean file
Return a distance matrix given a matrix .
cross entropy loss with a mask
embedding + LSTM Projected
Sampled softmax via importance sampling .
Split labels into `num_splits` and
generate training examples from multivariate time series data
Returns a pre-defined model by name
Return a new handle with specified storage type , shape , dtype and context .
be used to construct NDArray .
Prepare the value of dtype if ` dtype ` is None .; is an NDArray , numpy.ndarray
are not None
Creates a ` CSRNDArray ` , an 2D array with compressed sparse row ( CSR ) format .
Create a ` CSRNDArray ` based on data , indices and indptr
Creates a ` RowSparseNDArray ` , a multidimensional row sparse array with a set of \
Create a ` RowSparseNDArray ` based on data and indices
arrays with broadcasting .
Creates a sparse array from any object exposing the array interface .
Data-type of the array's ith aux data.
The data types of the aux data for the BaseSparseNDArray.
Return a copy of the array after casting to a specified type .
Check whether the NDArray format is valid .
associated with the BaseSparseNDArray .
Get a deep copy NDArray of the i-th aux data array associated with the
Returns a `` scipy.sparse.csr.csr_matrix `` object with value copied from this array
Return a copy of the array with chosen storage type .
Copies the value of this array to another array.
passed as a parameter , into ONNX model .
Benchmarking both storage and dot
Convert caffe mean
r"""Densenet-BC model from the
Loads the MXNet model file and
Helper function to import module
Build network symbol for training SSD
Build network for testing SSD
is an internal helper function that can be used for either of these
Get the output and gradients of output of a convolutional layer .
Get the gradients of the image .
obtained using ` get_image_grad `
Compute CAM.
Draw a heatmap on top of the original image using intensities from activation_map
gives a saliency map .
checking shape of label and prediction
evaluation metric from metric names or instances of EvalMetric
Creates a custom evaluation metric that receives its inputs as numpy arrays .
Save configurations of metric.
Update the internal evaluation with named label and pred
Resets the internal evaluation result to initial state.
Gets the current evaluation result .
Gets the current global evaluation result .
zipped name and value pairs .
zipped name and value pairs for global results .
Update various binary classification counts for a single (label, pred)
Calculate the Matthew 's Correlation Coefficent
transformed by the
Returns a new dataset with the first element of each sample
Forward the image through the LSTM network model
Return a caffe_pb2.NetParameter object that defined in a prototxt file
Returns layers in a caffe_pb2.NetParameter object
Return a caffe_pb2.NetParameter object that defined in a binary
Iterate over all layers
accepts keyword arguments ) .
Deprecated ) .
'run ' or 'stop ' .
Use this to save profile; stop profiler .
Return a printable string of aggregate profile stats .
Pause profiling.
paused profiling .
Set counter value.
Increment counter value.
Decrement counter value.
Set up the profiler state to record operator.
compiled module .
Launch cuda kernel.
Clear the internal statistics to initial state.
Update internal records.
update num_inst and sum_metric
get recall and precision from internal records
calculate average precision
according to key
calculate average precision, override the default one,
symbol: the pre-trained network symbol
Description : generate list for lip images
Description : Align to lip position
Switch on/off verbose mode
Internal verbose print function
Legacy initialization method.
save imglist to disk
load class names from text file
read data into numpy
create data iterator with NDArrayIter
Function factory for file extension argparse assertion
generates the colormap for visualizing the segmentation mask
get the ( 1 , 3 , h , w ) np.array data for the supplied image
Module main execution
make sure they have same classes
get total number of images , init indices
given index , find out sub-db and sub-index
checkpoint Module to prefix every epoch .
saves a model checkpoint every few epochs .
log the training evaluation result every period .
install callback to executor .
collecting stats for current batch .
collecting for current batch and return results .
End collecting and print results.
make a random data iteration plan
Expand the pending files in the current stage .
Dataset loader with preprocessing.
Creates an instance of token embedding .
embedding names and their pre-trained file names .
embedding vectors from the pre-trained token embedding file .
Sets the mapping between token indices and token embedding vectors .
Look up embedding vectors of tokens .
embedding vectors for tokens .
embedding file name is valid .
Generate the implementation of step HMC
Generate the implementation of HMC
Generate the implementation of SGD
Generate the implementation of SGLD
Generate the implementation of DistilledSGLD
Get a list of architectures given our dockerfiles
Build a container for the given platform
Get the image id of the local docker layer with the passed tag
Run command in a container
tagged container from the given docker registry
Load data into sliced arrays.
outputs that lives on multiple context into one , so that they look
Prepare the group2contexts , will duplicate the context
according to the workload .
Collect internal arrays from executors.
Bind executors on their respective devices.
Reshape executors.
Assign, i.e.
arg_params ` and ` aux_params ` .
according to workload and run forward on each devices .
Get the shapes of the outputs .
Get outputs of the previous forward computation.
Get the gradients with respect to the inputs of the module .
be called after
Accumulate the performance according to ` eval_metric ` on all devices
bind the i-th executor .
Get the sliced shapes for the i-th executor .
parse # classes and class_names if applicable
has instance of `` dtype `` .
Shuffle the data.
r"""MobileNet model from the
Get the canonical name for a symbol .
sampled candidates ,
Inception_score function.
do not load symnet and will convert context
use cell.unroll instead
using RNN cells .
Load model checkpoint from file.
Make a callback to checkpoint Module to prefix every epoch .
specified by path into numpy.ndarray
Returns a tuple of names and zero arrays for LSTM init states
Loads the model from checkpoint specified by prefix and epoch , binds it
note index 0 and 2
determine overlaps between boxes and query_boxes
Clip boxes to image boundaries.
bounding box regression targets from ex_rois to gt_rois
Transform the set of class-agnostic boxes into class-specific boxes
greedily select boxes with high confidence and overlap with current maximum <= thresh
rois (nroi, 4), scores (nrois, nclasses), bbox_deltas (nrois, 4 * nclasses), im_info (3)
Convert a python string to C string .
Find mxnet library.
Load ndarray file and return as list of numpy array.
get the output .
Change the input shape of the predictor .
Get the index-th output .
play the game for a maximum of; Begin an episode of a game instance .
Unrolls the recurrent cell for one time step .
Check that all input names are in symbol 's arguments .
Check that input names matches input data descriptors .
check that names match
calls both `` forward `` and `` backward `` .
evaluates the performance according to
Iterates over predictions.
collects the outputs .
Assigns parameter and aux state values.
model parameters to file .
Loads model parameters from file.
Find MXNet dynamic library files.
included header files .
Generate a greyscale captcha image representing number string
Generates a character string of digits .
Generate a random captcha image sample
Registers a new optimizer.
Instantiates an optimizer with a given name and kwargs .
given weight , including FP32 high
Updates the given parameter using the corresponding gradient and state .
Sets an individual learning rate multiplier for each parameter.
Sets an individual weight decay multiplier for each parameter.
Sets the number of the currently handled device .
Updates num_update.
Gets the learning rates given the indices of the weights .
weight decays for indices .
sync state context.
Sets updater states.
Gets updater states.
Convert a video into the mouth images
Read from frames
Read from videos
Config video types
using face detector
using mouth detector
using mouth crop
Get video frames
Prepare the input of model
Subtract ImageNet mean pixel-wise from a BGR image.
Not necessary in practice
evaluate accuracy of any data iterator passed to it as an argument
running the training the model .
Set size limit on bulk execution.
taking score from parent beam and bigram probability of last two chars
add beam if it does not yet exist
described by the paper of Hwang et al .
length-normalise LM score
sorted by probability
increase acc about more than 1 % ,
initialize a detector
tuple or int
takes difference of each frame as input .
Custom evaluation metric on CRPS.
encoding to encode the label into the CDF target .
coco ann: [u'segmentation', u'area', u'iscrowd', u'image_id', u'bbox', u'category_id', u'id']
example results
Draw random samples from an approximately log-uniform or Zipfian distribution.
Run a for loop with user-defined computation over NDArrays on dimension 0 .
Run a while loop with user-defined computation and loop condition .
Run an if-then-else using user-defined condition and computation
determine if the NDArray contains an infinite element
LSTM Cell symbol
resize , transform image , return im_tensor , im_info , gt_boxes
read by opencv
resize input image to target size and return scale
transform into mxnet tensor,
transform from mxnet im_tensor to ordinary RGB image
adding a new axis
Used in testing .; given a matrix .
based on Recall @ k .
learning rate based on schedule .
Returns symbol for LSTM model up to loss/softmax
returns the resulting symbol
Creates an unrolled LSTM symbol for inference if loss_type is not specified , and for training
Returns name and shape of init states of LSTM network
ctypes implementation of imperative invoke wrapper
training/not training .
Compute the gradients of outputs w.r.t variables .
computes both gradient of arguments and loss value .
computes gradient of arguments .
Splits an NDArray into `num_slice` slices along `batch_axis`.
Splits an NDArray into `len(ctx_list)` slices along `batch_axis` and loads
is smaller than ` max_norm ` .
Check whether the sha1 hash of the file content matches the expected hash .
Download an given URL
Return the base URL for Gluon dataset and model repository .
Return the URL for hosted file in Gluon repository .
Print at most `limit` elements of list.
Create a symbol function by handle and function name .
based on the current mini-batch
Generate row ids for all rows
Convert caffe model
r"""VGG model from the `"Very Deep Convolutional Networks for Large-Scale Image Recognition"
check function consistency with uniform random numbers
Remove images without usable rois
be flipped when loading into network
pretrained on local file system .
pretrained model files in local file store .
initialize all entries given annotation json file
according to the installed optimizer and the gradients computed
Clips gradient norm.
Image visualization and preservation
Get the translation of images
Load the dataset and split it to train/valid data
Get net G
Get the netD
Get configurations for net
Entry point to dcgan
Gets a customized logger .
helper function to get dataloader
r"""ResNet V1 model from `"Deep Residual Learning for Image Recognition"
Helper function for random generators.
Draw random samples from a Poisson distribution.
Draw random samples from a generalized negative binomial distribution.
sampling from multiple multinomial distributions .
Single-shot multi-box detection with VGG 16 layers ConvNet
Creates a model from previously saved checkpoint .
Saves current progress to checkpoint.
reset binded state .
Reshapes the module for new input shapes .
optimizer from a shared module .; Used in bucketing , where exactly the same
Gets outputs of the previous forward computation.
optimizer ( updater ) state to a file .
Loads optimizer (updater) state from a file.
Draw random samples from a uniform distribution.
Draw random samples from a normal (Gaussian) distribution.
r"""Draw samples from an exponential distribution.
Draw random samples from a gamma distribution.
Draw random samples from a negative binomial distribution.
Draw random samples from a discrete uniform distribution.
Some tricks of feature engineering are adapted
Initialize parameters in the KVStore.
learning rate of the optimizer .
is set to True ,; invoke pull operations on KVStore .
reduce the gradients from different contexts .
Makes one step of parameter update.
trainer states ( e.g .
Loads trainer states (e.g.
estimating the density of the sparse dataset
Execute the command line command .
Submit function of local jobs.
identifying non-zero and non-repeating values , and returns them in a list Parameters
trailing zeros in the list of integers and returns a new list of integers
Calculates the Longest Common Subsequence between p and l ( both list of int ) and returns its length
divided by total number
Longest Common Subsequence accuracy measure: calculate accuracy of each prediction as LCS/length
parse the text file and load into NDArrays .
Decode image from str buffer.
Pad image border
resize it to size
is smaller than size
Move iterator position forward
see if the two arrays are the same size .
Imports the ONNX model files , passed as a parameter , into Gluon SymbolBlock object .
get dataset iterators
decayed by ratio every N epochs .
Seeds the random number generators in MXNet.
Draw random samples from a uniform distribtuion.
Draw random samples from a Gaussian distribution.
Adding two tensors
Mean of all the input tensors.
Returns indices of the maximum values along an axis
Returns indices of the minimum values along an axis.
Elementwise maximum of arrays.
Elementwise minimum of arrays.
input arrays along a given axis .
padding to input tensor
Leaky Relu function
Applies the sofplus activation function element-wise to the input.
Compute N-D convolution on (N+2)-D input.
transposed convolution of the input tensor .
Applies a linear transformation: Y=XWT+b.
max pooling on the input .
avg pooling on the input .
pooling on the input .
Performs general matrix multiplication and accumulation
Local Response Normalization.
Reshape the given array by the shape attribute .
Cast input to a given dtype
Splits an array along a particular axis into multiple sub-arrays.
Returns a slice of the input tensor along multiple axes.
Transpose the input array .
Remove single-dimensional entries from the shape of a tensor .
Inserts a new axis of size 1 into the array shape
collapsing the higher dimensions .
Clips (limits) the values in an array.
Reduce the array along a given axis by maximum value
Reduce the array along a given axis by mean value
Reduce the array along a given axis by minimum value
Reduce the array along a given axis by sum value
Reduce the array along a given axis by product value
Reduce the array along a given axis by log sum value
Reduce the array along a given axis by log sum exp value
Reduce the array along a given axis by sum square value
Reduce input tensor by l1 normalization.
Reduce input tensor by l2 normalization.
Max ROI Pooling.
Rearranges data from depth into blocks of spatial data.
Rearranges blocks of spatial data into depth.
batched one-hot vectors .
does not have eps attribute , so can not map it to L2normalization in MXNet
copy the results back to the host through the mounted
runs inside the vm
given a word/token .
Train the model using Caffe operator in MXNet
Preprocess a 210x160x3 uint8 frame into a 6400 ( 80x80 ) ( 1 x input_size )
Returns a new empty handle.
Return a new handle with specified shape and context .
calling basic or advanced indexing functions .
Given start , stop , step and array length , return
Given data and index shapes , get the output ` NDArray ` shape .
Given start , stop , and stop , calculate the number of elements
Given two shapes that are not identical , find the shape
filled with all ones , with the given shape and type .
Moves the `source` axis into the `destination` position
Helper function for element-wise operation.
raised to powers from second array , element-wise
Returns the result of element-wise * * equal to * * ( == ) comparison operation with
Returns the result of element-wise * * not equal to * * ( ! = ) comparison operation
Returns the result of element-wise * * greater than * * ( > ) comparison operation
Returns the result of element-wise * * greater than or equal to * * ( > = ) comparison
Returns the result of element-wise * * lesser than * * ( < ) comparison operation
Returns the result of element-wise * * lesser than or equal to * * ( < = ) comparison
Returns the result of element-wise * * logical and * * comparison
Returns the result of element-wise * * logical or * * comparison
Returns the result of element-wise * * logical xor * * comparison
DEPRECATED, use ``concat`` instead
use mx.img instead
filled with all zeros , with the given shape and type .
Return a 2-D array with ones on the diagonal and zeros elsewhere .
represents as DLManagedTensor until
backed by a dlpack tensor .
backed by Numpy 's ndarray .
Returns an index array for use in scatter_nd and gather_nd.
Given value and vshape , create an ` NDArray ` from value with the same
is called by __setitem__ when key is a basic index , i.e .
is called by __setitem__ when key is an advanced index .
is called when key is a slice , or an integer ,
Performs a synchronized copy from the `source_array` to the current array.
Returns a sliced NDArray that shares memory with the current one.
sliced at ` idx ` in the first dim .
altering any data .
Broadcasts the input array to a new shape .
Tuple of array dimensions.
Device context of the array.
Data-type of the array's elements.
Whether this array's corresponding gradient array
Returns a `` numpy.ndarray `` object with value copied from this array .
casting to a specified type .
Returns an array on the target device with the same value as this array.
Attach a gradient buffer to this NDArray , so that ` backward `
attached to this NDArray .
detached from the current graph .
Compute the gradients of this NDArray w.r.t variables .
Build the align array
Get the position of words
Prepares the module for processing a data batch by pulling row_sparse
Rescale the gradient of provided parameters by a certain scale
builds factorization machine network with proper formulation:
Reshape data into (num_example, batch_size)
Tokenizes a text file .
docstring for symbolic functions .
Get user friendly information of the output shapes.
Query CUDA for the number of GPUs present.
Query CUDA for the free and total bytes of GPU global memory.
Returns the current context .
label for the data items .
Try to configure cython and return cython configuration
Compose symbol on inputs.
Set the attribute of the symbol.
Configuration factory for various networks
Wrapper for get symbol for train
is associated with .
based on row_id .
initializes by loading from data .
deferred initialization .
Sets data and grad.
Initialize grad buffers.
Reduce data from multiple context to cpu.
used for : py : class : ` NDArray ` API .
Re-assign Parameter to other contexts.
Sets this parameter's value on all contexts.
Returns a copy of the 'row_sparse' parameter on the same context as row_id's.
Returns copies of the 'row_sparse' parameter on all contexts, in the same order
Returns a copy of this parameter on one context.
Returns copies of this parameter on all contexts, in the same order
Returns a gradient buffer for this parameter on one context.
Returns gradient buffers on all contexts, in the same order
is initialized on .
is taken if; gradient buffer on all contexts to 0 .
representing this parameter .
Cast data and gradient of this Parameter to a new data type.
Retrieves a : py : class : ` Parameter ` with name `` self.prefix+name `` .
Retrieves a : py : class : ` .Constant ` with name `` self.prefix+name `` .
Copies all Parameters in ``other`` to self.
managed by this dictionary to be used for : py : class : ` NDArray `
Set an attribute to a new value for all Parameters.
Save parameters to file.
Load parameters from file.
Create a Torch function from the FunctionHandle .
add all the torch backed ndarray functions to current module .
r"""Inception v3 model from
Pack a string into MXImageRecord .
Unpack a MXImageRecord to string .
Unpack a MXImageRecord to image .
Pack an image into ``MXImageRecord``.
Opens the record file .
ensure integrity , reset if in new process .
Closes the record file .
Inserts a string buffer as a record .
Returns record as a string.
Sets the current read pointer position .
Returns the current position of write head .
given index .
selected pandas dataframe .
callback arguments for model.fit ( )
selected dataframes .
Callback funtion for training.
selected dataframe after a completed batch
records each epoch time
Render the plot with bokeh.io and push to notebook .
Reads a csv of sentences/tag sequences into a pandas dataframe .
depending on the length of the input sequence
Run a for loop with user-defined computation over Symbols on dimension 0 .
reserved tokens .
Indexes keys of `counter`.
according to the vocabulary .
token indices to tokens according to the vocabulary .
Create an io iterator by handle.
add all the data iterators to current module .
Get DataDesc list from attribute lists.
Get next data batch from iterator.
Ignore roll over data and set to start.
Increments the coursor by batch_size for next batch
Load data from underlying arrays.
concat two NDArrays .
underlying arrays , internal use only .
Get pad value of DataBatch.
Given a quantized symbol and a dict of params that have not been quantized ,
Given a symbol object representing a neural network of data type FP32 ,
Given a dictionary containing the thresholds for quantizing the layers ,
save them in
save them in a dictionary mapped by layer names .
Given a discrete distribution ( may have not been normalized to 1 ) ,
Given a dataset , find the optimal threshold for quantizing it .
Given a ndarray dict , find the optimal threshold for quantizing each value of the key .
Given a str as a path the symbol .json file or a symbol , returns a Symbol object .
Given a str as a path to the .params file or a pair of params ,
generating a quantized model from a FP32 model w/ or w/o calibration .
collecting layer output NDArrays .
collecting min and max values from an NDArray .
is a CNN which takes 32x32 image as input
is a CNN which takes 100 dimensional embedding as input
takes a 32x32 image as input
takes a 256x8x8 feature map as input
GaussianLogDensity loss calculation for layer wise loss
Calculate the discriminator layer loss
Get the dataset
fill the ith grid of the buffer matrix with the values from the img
create a grid of images and save it as a final image
adversarial training of the VAE
root mse between the logarithms of the prediction and the truth .
are obtained with modifications .; Gets a neural network .
Trains the model .
Conducts k-fold cross validation for the model.
Trains the model and predicts on the test data set .
Perform CapsNet training
Update the hyper-parameters and loss of CapsNet
Reset class MNISTCustomIter(mx.io.NDArrayIter):
Generate next of iterator
Get the attribute dict given the attribute set by the symbol .
kvstore assuming some parameters ' storage types are row_sparse .
Create kvstore
Perform update of param_arrays from grad_arrays on NCCL kvstore.
Perform update of param_arrays from grad_arrays on kvstore.
Perform update of param_arrays from grad_arrays not on kvstore.
kwargs to any configured callbacks .
Internal training function on multiple devices.
Checkpoint the model data into file .
verify the argument of the default symbol and user provided parameters
Initialize weight parameters and auxiliary states.
Initialize the predictor module for running prediction .
Initialize the iterator given input .
Initialize the iterator given eval_data .
Run the prediction , always only use one device .
Run the model given an input and calculate the score
Fit the model.
Checkpoint the model checkpoint into file .
create a model .
build and upload all built dockerimages in parallel
passed platform and upload the cache to the specified S3 bucket
passed image by id , tag it with docker tag and upload to S3 bucket
Login to the Docker Hub account
Load the precompiled docker cache from the registry
Delete the local docker cache for the entire docker image chain
create and publish the Docker cache to Docker Hub
Download the chinese_text dataset and unzip it
override reset behavior
Implementation of updating metrics
Get the current evaluation result .
Structure of the Deep Q Network in the NIPS 2013 workshop paper:
Get the dictionary given name and ndarray pairs .
List all the output NDArray.
Calculate the outputs specified by the bound symbol .
pass to get the gradient of arguments .
Install callback for monitor.
Get dictionary representation of argument arrrays.
Get dictionary representation of gradient arrays.
Get dictionary representation of auxiliary states arrays.
Get dictionary representation of output arrays.
Copy parameters from arg_params, aux_params into executor's internal array.
Return a new executor with the same symbol and shared memory ,
Get a debug string about internal execution plan .
parse pascal voc record into a dictionary
pascal voc evaluation
Convert MXNet layer to ONNX
split params dictionary into args and aux params
return dictionary of output name to shape
Convert weights to numpy
Convert MXNet graph to ONNX graph
learning rate and refactor scheduler
training phase .
is a set of 50 images representative of ImageNet images .
Return the boston housing data in a nice package .
Return the clssic IMDB sentiment analysis training data in a nice package .
Predict total number of non-violent crimes per 100K popuation.
Return the diabetes data in a nice package .
Return the classic iris data in a nice package .
Return the Adult census data in a nice package .
packaged version of NHANES I data with surivival times as labels .
packaged version of CRIC data with progression to ESRD within 4 years as the label .
Correlated Groups 60
A simulated dataset with tight correlations among distinct groups of features.
Ranking datasets from lightgbm repository .
retraines the model once .
is retrained for each test sample with the non-important features set to a constant .
is revaluated for each test sample with the non-important features set to their mean .
is revaluated for each test sample with the non-important features set to an imputed value .
is revaluated for each test sample with the non-important features set to resample background values .
do the features plus a constant base rate sum up to the model output .
Generate a random array with a fixed seed .
Shuffle an array in-place with a fixed seed.
Estimate the SHAP values for a set of samples .
Plots SHAP values for image inputs.
is under-defined , this picks the ordering that keeps nearby samples similar .
seem to have with the feature at the given index .
Converts human agreement differences to numerical scores for coloring.
Draw the bars and separators .
Draw additive plot.
do n't work .
computes the deeplift
used to save interim tensors , detached
saves the tensor - attached to its graph .
handles to all non-container layers in the model .
Removes the x and y attributes which were added by the forward handles
gets a JSON dump of an XGBoost model while ensuring the features names are their indexes .
computes the expected value conditioned on the given label value .
Estimate the SHAP interaction values for a set of samples .
make predictions from this model .
Return the values for the model applied to X .
Visualize the given SHAP values with an additive force layout .
plots to an output file .
assuming their value is False and find blocked Switch paths .
decompose softmax into its components and recurse , we can handle all of them : )
inputs of this operation are variable ( i.e .
Get the SHAP value computation graph for a given model output .
Runs the model while also setting the learning phase flags to False .
Passes a gradient op creation request to the correct handler .
run the experiments on remote machines in parallel .
Create a SHAP monitoring plot .
Summarize a dataset with k mean samples weighted by the number of data points they
Use the SHAP values as an embedding which we project to 2D for visualization .
Create a SHAP dependence plot , colored by an interaction feature .
Runtime
Local Accuracy
Keep Negative ( mask )
Keep Absolute ( mask )
Remove Positive (mask)
Remove Absolute (mask)
Keep Negative ( resample )
Keep Absolute ( resample )
Remove Positive (resample)
Remove Absolute (resample)
Keep Negative ( impute )
Keep Absolute ( impute )
Remove Positive (impute)
Remove Absolute (impute)
Keep Negative ( retrain )
Remove Positive (retrain)
Batch Remove Absolute (retrain)
Batch Keep Absolute (retrain)
Test an explanation method.
AND (false/false)
AND (false/true)
AND (true/true)
OR (false/false)
OR (false/true)
OR (true/true)
XOR (false/false)
XOR (false/true)
XOR (true/true)
SUM (false/false)
SUM (false/true)
SUM (true/true)
block matrix inversion identities to quickly estimate transforms .
4-Layer Neural Network
Gradient Boosted Trees
Create a SHAP summary plot , colored by feature values when they are provided .
Kernel SHAP 1000 mean ref.
IME 1000
TreeExplainer (independent)
mean(|TreeExplainer|)
Saabas
LIME Tabular 1000
Deep SHAP (DeepLIFT)
applied to the data given by X .
is not installed .
Executes training.
Runs and blocks until all trials finish.
remaining output records in the output queues to plasma .
given space .
shapes to spaces that do n't have shapes .
Downsamples images from (210, 160, 3) by the configured factor.
Get a new batch from the internal ring buffer .
Runs one logical iteration of training.
Removes subdirectory within checkpoint_folder
Saves the current model state to a checkpoint .
Saves the current model state to a Python object .
training state from a given model checkpoint .
training state from a checkpoint object .
based on export_formats .
Dump a whole json record into the given file .
Parse a whole json record from the given file .
given file .
str recursively .
Computes the loss of the network .
Computes the gradients of the network .
Creates the queue and preprocessing operations for the dataset .
Build CIFAR image and labels.
update a Ray cluster .
Tear down the Ray cluster .
testing purposes only .; Kills a random Ray node .
runs a script on the specified cluster .
Build a whole graph for the model .
Build the core model within the graph .
Build training specific ops for the graph.
weight decay loss .
FullyConnected layer for final output .
Forward pass of the multi-agent controller.
Forward pass of the loss.
Unpacks the action mask / tuple obs from agent grouping.
Get a named actor which was previously created .
named actor under a string key .
are in schema
Required Dicts indicate that no extra fields can be introduced .
Update the settings according to the keyword arguments .
Update the settings when the target fields are None .
compute an actor handle ID .
compute an actor handle ID in the non-forked case .
Annotate an actor method.
exit the current actor .
Get the available checkpoints for the given actor ID , return a list
Create an actor.
Method execution stub for an actor handle.
is defined in order to make pickling work .
loads the specified inputs into device memory .
Run a single step of SGD .
Generate genes (encodings) for the next generation.
Perform selection action to candidates.
Perform crossover action to candidates .
Perform mutation action to candidates.
starting at the given path .
Lists experiments in the directory subtree.
save remote id .
Starts trial and restores last result if trial was paused .
Stops this trial.
Starts the trial .
returns resources if resources allocated .
Pauses the trial .
invoke ` Trainable.reset_config ( ) ` to reset trial .
Fetches one result of the running trials.
has at least the specified resources .
printing to the console .
describing the total resources available .
Saves the trial 's state to a checkpoint .
erases old checkpoints
based on trial.export_formats .
Generates an actor that will execute a particular instance of
Generates one actor for each instance of the given logical
Generates all output data channels
executes the physical dataflow .
given logical operator to the environment and
Sets the number of instances for the source operator of the stream .
Applies a map operator to the stream.
Applies a flatmap operator to the stream.
Applies a key_by operator to the stream.
rolling sum operator to the stream .
Applies a system time window to the stream.
Applies a filter to the stream.
Inspects the content of the stream .
Closes the stream with a sink operator .
open more ) .
Update the list of log files to monitor .
Open some closed files if they may have new lines .
Get any changes to the log files and push updates to Redis .
Run the log monitor .
given experiment specifications .
be queued into the TrialRunner .
Generates trials with configurations from `_suggest`.
Generates variants from a spec (dict) with unresolved values.
joining keys into tuple of paths .
Run main entry for AutoMLBoard.
Initialize configs of the service.
Get the IDs of the GPUs that are available to the worker .
Return information about failed tasks.
Initialize the serialization library .
existing Ray cluster or start one and connect to it .
Disconnect the worker , and terminate processes started by ray.init ( ) .
log messages from workers on all of the nodes .
received in the given output queue .
error messages in the background on the driver .
Connect this worker to the raylet , to Plasma , and to Redis .
Disconnect this worker from the raylet and object store .
produce a deterministic class ID for a given class .
Enable serialization and deserialization for a particular class.
Get a remote object or a list of remote objects from the object store .
Store an object in the object store.
Return a list of IDs that are ready and a list of IDs that are not .
Define a remote function or an actor class .
contains the following attributes .
Get the SerializationContext of the driver that this worker is processing .
register its class if needed .
Put value in the local object store with object id objectid .
Get the value or values in the object store associated with the IDs .
Submit a remote task to the scheduler .
Run arbitrary code on all of the workers.
Retrieve the arguments for the remote function .
Store the outputs of a remote function in the local object store.
Execute a task assigned to this worker .
be ready and process the task .
Get the next task from the raylet .
runs to receive and execute tasks .
reshapes all values in a dictionary .
Get a dictionary of addresses .
Create a redis client .
is not created .; Return a incremental temporary file name .
randomized filenames for log files .
Prepare the socket file for raylet and plasma .
Start the Redis servers .
Start the log monitor .
Start the reporter .
Start the dashboard .
Start the plasma store .
Start the raylet .
logging files for workers to redirect its output .
Start the monitor .
Start the raylet monitor .
processes on the node .
Start all of the processes on the node.
Kill a process of a given type .
Kill the Redis servers.
Kill the plasma store.
Kill the raylet.
Kill the log monitor.
Kill the reporter.
Kill the dashboard.
Kill the monitor.
Kill the raylet monitor.
Kill all of the processes.
Return a list of the live processes .
Create a large array of noise to be shared by all workers .
model network configuration .
be obtained with ` get_model_config ` .
Do a rollout .
Generates Trial objects with the variant generation process .
applying ` self.operation `
Serialize this policy for Monitor to pick up .
connecting to cluster workers .
Passes the result to HyperOpt unless early terminated or errored .
plasma to prefetch the given object_id .
Get an object directly from plasma without going through object table .
Restores the state of the batched queue for writing .
Checks for backpressure by the downstream reader.
discarding any .
Collects at least train_batch_size samples.
Improve the formatting of an exception thrown by a remote function .
be printed in the background .
Check if an object is a Cython function or method
Check if an object is a function or method .
Generate a random string to use as an ID .
Make this unicode in Python 3 , otherwise leave it as bytes .
s * to ` str ` .
Get the device IDs in the CUDA_VISIBLE_DEVICES environment variable .
Determine a task 's resource requirements .
logging for ray .
get a particular statistic .
Run a sysctl command and parse the output .
Return the total amount of system memory in bytes .
Get the size of the shared memory file system .
Send a warning message if the pickled object is too large .
Create a thread-safe proxy which locks every method call
create a directory that is globally readable/writable .
produces a distributed array from a subset of the blocks in
Assemble an array from a distributed array of object IDs.
Computes action log-probs from policy logits and actions.
wrapper used only for tests
r"""V-trace for softmax policies.
r"""V-trace from log importance weights.
selected log_probs for multi-discrete actions of behaviour
generates a weight variable of a given shape .
generates a bias variable of a given shape .
given dataframe to fit into terminal .
Opens a txt file at the given path where user can add and save notes .
query the job info , with the given job_id .
query the trial info , with the given trial_id .
Callback for early stopping.
completed if it is paused and has previously ran .
Build a Job instance from a json string .
Build a Trial instance from a json string .
Build a Result instance from a json string .
Given a rollout , compute its value targets and the advantage .
Handle an xray heartbeat batch message from Redis.
Remove this driver 's object/task entries from redis .
Handle a notification that a driver has been removed .
ready in the subscription channels .
Experimental: issue a flush request to the GCS.
Run the monitor .
View for the home page.
View for a single job.
View for a single trial.
Get job information for current job.
Get job information for current trial.
Get winner trial of a job.
Returns a base argument parser for the ray.tune tool.
Converts configuration to a command line argument format.
Creates a Trial object from parsing the spec .
Poll for compute zone operation until finished.
Return the task id associated to the generic source of the signal .
Send signal.
Get all outstanding signals from sources .
Reset the worker state associated with any signals that this worker
True if this is the `` first '' call for a given key .
Get a single or a collection of remote objects from the object store .
User notification for deprecated parameter.
Produces a list of Experiment objects .
Generates an Experiment object from JSON .
Registers Trainable or Function at runtime.
Perform a QR decomposition of a tall-skinny matrix .
Perform a modified LU decomposition of a matrix .
string for nice sorting .
path to most recently modified checkpoint .
self._metadata_checkpoint_dir ` .
checkpointed trials from previous run .
have finished running .
Runs one step of the trial event loop.
Adds a new trial to this TrialRunner .
Replenishes queue.
based off trial.last_result .
recover trial .
Notification to TrialScheduler and requeue trial.
queue if possible .
Stops trial.
Helper function for running examples
Cython simple class
Cython with blas.
Rewrites the given trajectory fragments to encode n-step rewards .
ignores -inf values .
Minimized ` objective ` using ` optimizer ` w.r.t .
Get variables inside a scope
a common dense layer: y = w^{T}x + b
Returns a custom getter that this class's methods must be called
Context that construct cnn in the auxiliary arm.
Construct a conv2d layer on top of cnn.
Construct a pooling layer.
pooling layer .
Construct an average pooling layer.
Batch normalization on `input_layer` without tf.layers.
Adds a Batch Normalization layer .
Adds a local response normalization layer .
Fetch the value of a binary key.
associates a value with a given binary key .
pass in previously created workers .
Free a list of IDs from object stores.
Start the collector worker thread .
Initialize logger settings.
Run the main event loop for collector thread .
be checked first .
given job name .
given experiment directory .
given job .
given trial .
Build meta file for job.
Build meta file for trial.
Add a list of results into db .
Adds a time dimension to padded inputs .
Truncate and pad experiences into fixed-length sequences.
Return a config perturbed as specified .
perturbed params to the trial name to show in the console .
Logs transition during exploit/exploit step.
perturbed state from trial_to_clone - > trial .
Returns trials in the lower and upper `quantile` of the population.
get fair share of time ( as defined by time_attr ) .
Returns the ith default ( aws_key_pair_name , key_pair_path ) .
flattened inputs .
Returns the given config dict merged with a base agent conf .
Returns the class of a known agent given its name .
Return the first IP address for an ethernet interface on the system .
Run the reporter .
Throws an exception if Ray can not serialize this class efficiently .
is a namedtuple and False otherwise .
Register a trainable function or class.
Register a custom environment for use with RLlib.
reported from the policy graph .
episode metrics from PolicyEvaluator instances .
given evaluators .
Summarizes a set of episode metrics tuples .
vs off-policy estimates .
metadata if needed .
Checkpoints metadata.
pending to allow scheduler to start .
is a blocking call .
Passes the result to Nevergrad unless early terminated or errored .
Start the import thread .
given export key from redis .
Run on arbitrary function on the worker.
Called to clip actions to the specified range of this policy .
Passes the result to skopt unless early terminated or errored .
Convert a hostname to a numerical IP addresses in an address .
Determine the IP address of the local node .
Create a Redis client .
Start one of the Ray processes.
be available .
detect the number of GPUs on this machine .
Compute the versions of Python , pyarrow , and Ray .
Check if various version info of this process is correct .
Start the Redis global state store .
Start a single Redis server .
Start a log monitor process .
Start a reporter process .
Start a dashboard process .
check a resource dictionary and add sensible defaults .
Start a raylet , which is a combined local scheduler and object manager .
assembles the command used to start a Java worker .
configure the plasma object store .
Start a plasma store process .
starts an object store process .
starts a worker process .
Run a process to monitor the other processes .
Unpacks Dict and Tuple space observations into their original form.
Unpack a flattened Dict or Tuple observation array/tensor .
Convert the Ray node name tag to the AWS-specific 'Name' tag.
Update the AWS tags for a cluster periodically .
get info for this node , updating the cache .
Validates export_formats.
Updates the resource requirements .
given result meets this trial 's stopping criteria .
is due for checkpointing .
printing out to the console .
Returns whether the trial qualifies for restoring.
Compares two checkpoints based on the attribute attr_mean param .
Preprocess 210x160x3 uint8 frame into 6400 (80x80) 1D float vector.
take 1D float array of rewards and compute discounted reward
is array of intermediate hidden states )
Load a class at runtime given a full path .
be overridden with a batch method .; Terminates a set of nodes .
Passes the result to BayesOpt unless early terminated or errored
return the result .
dispatch a batch of input to self.serve_method .
Returns the gym env wrapper of the given class , or None .
Configure environment for DeepMind-style Atari.
is added to match TF conv2d ` same ` padding .
Call ray.get and then queue the object ids for deletion .
given size that is 64-byte aligned .
ensuring the output is 64-byte aligned .
Adds an item to the queue .
Gets an item from the queue.
documenting method overrides .
Adds new trial.
is filled .
is finished , all trials will be stopped .
is called whenever a trial makes progress .
Notification when trial terminates.
scheduling within iteration by completion percentage .
provides a progress notification for the algorithm .
bracket assuming bracket is not filled .
have completed .
Called after trial has finished
Cleans up bracket after bracket is completely finished .
Read the client table .
Initialize the GlobalState object by connecting to Redis .
Execute a Redis command on the appropriate Redis shard based on key .
Execute the KEYS command on all Redis shards .
parse the object table information for a single object ID .
parse the object table info for one or more object IDs .
parse the task table information for a single task ID .
parse the task table information for one or more task IDs .
parse the function table .
Get the profile events for a given batch of profile events .
Return a list of profiling events that can viewed as a timeline .
Return a list of transfer events that can viewed as a timeline .
Get a dictionary mapping worker ID to worker information .
Get the current total cluster resources .
Get the current available cluster resources .
Get the error messages for a specific driver .
Get the error messages for all drivers or a specific driver .
given actor id .
Returns the total length of all of the flattened variables .
Gets the weights and returns them as a flat array .
Sets the weights to new_weights , converting from a flat array .
containing the weights of the network .
Sets the weights to new_weights .
serialized ErrorTableData object .
shutdown the async API .
removes some non-critical state from the primary Redis shard .
removes some critical state from the Redis shards .
Creates a copy of self using existing input placeholders .
builds the graph for a deep net for classifying digits .
Get signature parameters
Check if we support the signature of this function .
Extract the function signature from the function .
Extend the arguments that were passed into a function .
Poll for cloud resource manager operation until finished.
Poll for global compute operation until finished.
Returns the ith default gcp_key_pair_name .
given key_name .
Create public and private ssh-keys.
Setup a Google Cloud Platform Project .
Setup a gcp service account with IAM roles .
using an existing key pair if possible .
Pick a reasonable subnet if not specified by the config .
Add new IAM roles for the service account.
Inserts an ssh-key into project commonInstanceMetadata
submit remote functions .
linked list .
Remove an object from the linked list .
cancel all tasks assigned to this event loop .
Complete all tasks.
Turn an object_id into a Future object.
Returns a list of all trials' information.
Returns trial information by trial_id.
Adds a trial by name and specification ( dict ) .
stop trial by trial_id .
Apply the given function to each remote worker .
Apply the given function to each model replica in each worker .
Apply the given function to a single model replica .
Run a single SGD step .
starting a router and register it .
Returns a list of one-hot encodings for all parameters.
Apply one hot encoding to generate a specific config .
Pin an object in the object store.
Retrieve a pinned object from the object store .
is d1 and d2 deep merged .
Updates original dict with values from new_dict recursively.
completed but only returns once the object is local .
be removed .
Iterate over train batches.
updates an autoscaling Ray cluster from a config json .
described by a config json .
Kills a random Raylet worker .
Create the cluster head node , which in turn creates the workers .
Attaches to a screen for the specified cluster.
Runs a command on the specified cluster .
Rsyncs files.
head node IP for given configuration file if exists .
given configuration file .
train ( ) for a Function API .
Returns logits and aux_logits from images.
renaming Agent = > Trainer with a warning .
Profile a span of time so that it appears in the timeline visualization .
run this as a thread to flush profile data in the
Push the logged profiling data to the global control store .
Add a key-value pair to the extra_data dict .
worker if possible .
Forward pass for the mixer.
Passes the result to SigOpt unless early terminated or errored .
len ( x ) )
Bottleneck block with identity short-cut for ResNet v1.
Bottleneck block with identity short-cut.
Residual block with identity short-cut.
updates from the buffer of another filter .
Returns a copy of Filter.
Syncs all fields together from other filter.
Returns non-concurrent version of current class
= -sum { sin ( xi ) * [ sin ( i * xi^2 / pi ) ] ^ ( 2m ) }
Parse integer with power-of-2 suffix eg.
Parse all_reduce_spec.
Build list of device prefix names for all_reduce.
Group device names into groups of group_size.
according to tensor size .
Calculate the average gradient for a shared variable across all towers .
controlling device for the aggregation .
Apply all-reduce algorithm over specified gradient tensors.
Extract consecutive ranges and singles from index_list.
Form the concatenation of a specified range of gradient tensors.
Unpack a previously packed collection of gradient tensors .
Concatenate gradients together more intelligently.
tower_grads done by pack_small_tensors .
outputted with Headers as first set of results .
Sends the current log directory to the remote node .
generating intermediate dictionaries .
Create a FunctionDescriptor instance from list of bytes .
Create a FunctionDescriptor from a function instance .
Create a FunctionDescriptor from a class .
See whether this function descriptor is for a driver or not .
Calculate the function id of current function descriptor .
Return a list of bytes representing the function descriptor .
cached remote functions
Export a remote function.
Pickle a remote function and export it to redis .
Import a remote function.
Get the FunctionExecutionInfo of a remote function .
be executed is present on this worker .
Push an actor class definition to Redis.
Load the actor class.
Load actor class from local code.
Load actor class from GCS.
wraps a user-defined actor method .
Save an actor checkpoint if necessary and log any errors .
log any errors .
implements the common experience collection logic .
prepare for policy evaluation .
Call compute actions on observation batches to get next actions .
Process the output of policy neural network evaluation.
have multiple logical episodes , one per life .
Compare two version number strings of the form W.X.Y.Z.
Create CMake instance and execute configure step
Copy Flatbuffers' artifacts to package folder
built libraries names and solve flatc path .
provides access into the Table 's vtable .
retrieves the relative offset stored at ` offset ` .
String gets a string from data stored inside the flatbuffer .
retrieves the length of the vector whose offset is stored
retrieves the start of data of the vector whose offset is
initializes any Table-derived type to point to the union at
retrieves a value of the type specified by ` flags ` at the
returns the vector that starts at ` Vector ( off ) `
retrieves the VOffsetT that the given vtable location
decodes values starting at buf [ head ] as
using ` packer_type ` .
finds and runs flatc built from source .
Returns the numpy module if it exists on the system ,
compares an unwritten vtable to a written vtable .
initializes bookkeeping for writing a new object .
serializes the vtable for the current object , if needed .
Doubles the size of the byteslice , and copies the old data towards
zeros at the current offset .
prepares to write an element of ` size ` after ` additional_bytes `
prepends an SOffsetT , relative to where it
Prepends an unsigned offset into vector data, relative to where it
initializes bookkeeping for writing a new vector .
writes data necessary to finish vector construction .
CreateString writes a null-terminated byte string as a vector .
CreateString writes a byte vector .
writes a numpy array into the buffer .
are always stored inline , so need to be created right
sets the vtable key ` voffset ` to the current location in the
finalizes a buffer , pointing to the given ` rootTable ` .
prepends an UOffsetT onto the object at
prepends a struct onto the object at vtable slot ` o ` .
prepends a value specified by ` flags ` to the Builder ,
prepends a VOffsetT to the Builder , without checking
prepends a SOffsetT to the Builder , without checking
prepends a UOffsetT to the Builder , without checking
r"""Return full path to the user-shared data dir for this application.
r"""Return full path to the user-specific config dir for this application.
sends a : class : ` Request < Request > ` .
Sends a GET request .
Writes out dict as toml to a file
input dict as toml
Preserve inline table in its compact syntax instead of expanding
is truthy if it exists and is n't one of ( 0 , false , no , off )
Check virtualenv membership dynamically
Unpack an object from `packed`.
Gets rid of the used parts of the buffer.
Establish a socket connection and set nodelay settings on it .
Alternative to the common request method, which sends the
be called once , before the connection is used .
known errors and prettify them instead of showing the
Get the OS appropriate handle for the corresponding output stream .
Hide the console cursor on the given stream
Returns the completion results for click.core.Choice
Internal handler for the bash completion support.
expr ::= seq ( '|' seq )* ;
seq ::= ( atom [ '...' ] )* ;
Parse command-line argument vector.
Flatten an arbitrarily nested iterable
get the output of a command and decode it .
given python executable 's environment as json
Force a value to bytes.
Force a value to a text-type.
split an iterable into n groups, per https://more-itertools.readthedocs.io/en/latest/api.html#grouping
Determine the proper output encoding for terminal rendering
Given a string , decode it for output to a terminal
Given an encoding name , get the canonical name from a codec lookup .
Given a stream , wrap it in a ` StreamWrapper ` instance and return the wrapped stream .
True if the connection is dropped and should be closed .
* address * and return the socket object .
True if the system can bind an IPv6 address .
Takes a string like abc.1.twelve and turns it into ( `` abc '' , 1 , `` twelve '' ) .
Determine if unicode string only contains ASCII characters .
Raise an option parsing error using parser.error ( ) .
Return an OptionGroup object
are set .
determining if custom platform options are allowed .
provided for the -- no-cache-dir option .
provided for the -- no-use-pep517 option .
Given a value spelled `` algo : digest '' , append the digest to a list
missing values of source from the existing fields .
Get the pinned version of an InstallRequirement .
Returns a new requirement object with extras removed.
replace tokens like { some.thing } with the
Return the path of the Makefile .
Initialize the module as appropriate for POSIX systems .
Initialize the module as appropriate for NT
Parse a config.h-style file .
Return the path of pyconfig.h .
Return a mapping containing an install scheme .
Return a path corresponding to the scheme .
Display all information sysconfig detains.
has not been reached .
Increments the parser by n characters
is satisfied is valid .
Creates a generic `` parse error '' at the current position .
sorted ( command name , command summary ) tuples .
Command name auto-correct.
Create a copy of this extension bound to another environment .
Return an attribute node for the current extension.
Call a method of the extension .; is a shortcut for
Parse a translatable tag .
given name .
Generates a useful node from the data provided .
running as a shell script .
Properly merges both requests and session hooks.
Receives a Response .
be removed when redirecting
being redirected we may want to strip authentication from the
method re-evaluates the proxy configuration by considering the
Constructs a : class : ` PreparedRequest < PreparedRequest > ` for
Constructs a : class : ` Request < Request > ` , prepares it and sends it .
Sends a OPTIONS request .
Sends a HEAD request .
Send a given PreparedRequest .
Check the environment and merge it with some settings .
Returns the appropriate connection adapter for the given URL .
Registers a connection adapter to a prefix.
Return a string , safe for output , of subprocess output .
Return path's uid.
Expand ~ and ~user constructions.
Provide an alternative for os.path.samefile on Windows/Python2
ensure that we are making the smallest amount of
aggregated from the pipfile , resolver ,
Given a resolved entry with multiple parent dependencies with different
Retrieves the full set of available constraints and iterate over them , validating
Monkey-patch urllib3 with SecureTransport-backed SSL-support.
Undo monkey-patching by :func:`inject_into_urllib3`.
is called by ST to request that data; read callback .
is called by ST to request that data
be used to wrap calls that do I/O from
matches the set in; Sets up the allowed ciphers .
do this in two cases :; Called when we have set custom validation .
is run automatically by; performs the TLS handshake .
Dump the bytecode into the file or file like object passed .
Returns the unique hash key for this template name .
Return a cache bucket for the given template .
Look for an encoding by its label.
Accept either an encoding object or label.
Decode a single string.
bom_encoding , input ) , with any BOM removed from the input .
Encode a single string .
-based decoder .
Return a generator that first yields the : obj : ` Encoding ` ,
-based encoder .
Decode one chunk of the input.
Given a list of Python tuples , create an associated CFDictionary .
Used entirely for error; Creates a Unicode string from a CFString object .
throws an exception if there is an error to
Given a bundle of certs in PEM format , turns them into a CFArray of certs
creates a temporary Mac keychain that we can use to work with
Given a single file , loads all the trust objects from it into arrays and
Has the end goal
redirect and where to ?
Set initial length value for Response content if available.
Set-up the _decoder attribute if necessary.
be called if the decoder is actually
Catch low-level python exceptions , instead re-raising urllib3
Similar to :meth:`httplib.HTTPResponse.read`, but with two additional
Given an : class : ` httplib.HTTPResponse ` instance `` r `` , return a
Similar to :meth:`HTTPResponse.read`, but with an additional
Returns the URL that was the source of this response .
Set Ok (success) finalizer to a spinner.
fail finalizer to a spinner .
breaking the spinner .
compose last frame and 'freeze ' it .
Like `describe_token` but for token expressions.
Return a lexer which is probably cached .
wraps it in a token stream .
is called with the stream as returned by ` tokenize ` and wraps
Load the pyproject.toml file.
Iterate through CPython versions available for Pipenv to install.
Find a version in pyenv from the version supplied.
given version with pyenv .
Parses an editable requirement into:
does not exist ,
Creates an InstallRequirement from a name , which might be a
Return the file cache path based on the URL .
Write the PID in the named PID file.
Remove the named PID file if it exists .
Release the lock .
Export the Bazaar repository at the url to the destination location
Gather details from installed distributions.
Print the informations from installed distributions found .
raises ` exc ` with the message , passed
Called if the parser encounters an unknown tag .
Like fail_unknown_tag but for end of template situations.
Are we at the end of a tuple?
Return a new free identifier as : class : ` ~jinja2.nodes.InternalName ` .
Parse a single statement .
Parse multiple statements into a list until one of the end tokens
Parse an assign statement.
Parse a for loop .
Parse an if construct .
allows assignments to
Parse the whole template into a ` Template ` node .
returns an abbreviated stack trace with lines that only concern
Parses a URI using the regex given in Appendix B of RFC 3986 .
Normalize the URL to create a safe key for the cache
Return a cached response if it exists in the cache , otherwise
caching requests .
get a new set of headers that we want to
Close the file descriptor .
checks if the file descriptor is still valid .
Read from the file descriptor and return the result as a string .
keeps internal caches for environments and lexers .
is useful if you want to; based on a string .
use the ` pretty ` library or the
escapes a single bytestring or unicode string with the
sets the initial value of autoescaping based on the
Works exactly like : func : ` dumps ` but is safe for use in `` < script > ``
Return a shallow copy of the instance .
is not in the cache otherwise
Clear the cache.
Return a list of items .
Check when updating if those are still needed; Hardcoded license URLs .
Reconstruct the library name without it 's version
Given the ( reconstructed ) library name , find appropriate destination
Convert Pipfile.lock hash lines into InstallRequirement option format.
hide noise generated by ` setup.py develop ` .
Find this package's .egg-info directory.
Encodes a string into the proper filesystem encoding
Fetch the string value from a path-like object
Encode a filesystem path to the proper filesystem encoding
using the proper filesystem encoding
Build a collection of `` traces '' for each package .
Check that a timeout attribute is valid .
Create a copy of the timeout object
Start the timeout clock , used during a connect ( ) attempt
Get the value to use when setting a connection timeout .
Get the value for the read timeout .
Establish a new connection via the SOCKS proxy .
Compute and return values (req, editable, comments) for use in
Detect /proc filesystem style.
Try to look up the process tree via the /proc interface .
return a temporary file .
speeds up exit by about 10ms for
attaches extra information to exceptions that
Given a sequence of parameters in the order as should be considered
be used with the context object to promote
is used for the `` usage ``
Finds the outermost context .
Finds the closest object of a given type .
sets the innermost object to a
Looks up the default for a parameter name .
Invokes a command callback in exactly the way it expects .
Similar to :meth:`invoke` but fills in default keyword
is the way to invoke a script with all the bells and
Writes the usage line into the formatter .
go into the usage line and returns
Returns the names for the help option .
Returns the help option object.
Creates the underlying option parser for this command .
makes it by shortening the long help string .
Writes the help into the formatter if it exists .
Writes the help text to the formatter if it exists .
Writes all the options into the formatter if they exist .
Writes the epilog into the formatter if it exists .
Given a context , this invokes the attached callback ( if it exists )
Adds a result callback to the chain command .
adds all the commands
Command ` with this group .
declaring and attaching a command to
declaring and attaching a group to
Given a context variable this calculates the default value .
Given a value this runs it properly through the type system .
Get a stringified version of the param for use in error messages to
is an alternative flow that can be activated in the full
using the supplied finder and the
Get all abstract dependencies for a given list of requirements .
Get all dependencies for a given install requirement .
given install requirement from the wheel cache .
given install requirement from the json api .
given install requirement from the dependency cache .
given install requirement from the pip resolver .
Build a pip command from a list of sources
Get a package finder for looking up candidates to install
produce a resolver .
is a little weird in order to allow backwards; Given a requirement , return its cache key .
Reads the cached contents into memory .
Writes the cache to disk as JSON .
given ireqs .
given cache keys .
Given a requirement , return its cache key .
enables locks for decorated function .
Get a TreeWalker class for various types of tree with built-in support
Pretty printer for tree walkers
allow for custom writing to the terminal .
Show the hidden spinner .
Return the VCS-specific command arguments .
Make a copy of the current instance , but with a new rev .
Return the type of the version control backend if found at given
start with os.path.sep ,
Parse the repository URL to use , and return the URL , revision ,
Return the URL and RevOptions object to use in obtain ( ) and in
ignoring incidental differences .
represented by this
Run a VCS subcommand
is a repository directory .
Create the fully qualified name of the files created by
Compact a path set to contain the minimal number of paths
containing the paths that need to be renamed .
display to user
Stashes a directory.
Stashes a file.
Stashes the directory or file and returns its new location .
Commits the uninstall by removing stashed files .
moving stashed files back .
Remove paths in ``self.paths`` with confirmation (unless
files would be deleted and prompt for confirmation
Rollback the changes previously made by remove ( ) .
>>> package = yarg.get('yarg')
Returns a list of Python version strings that
A list of :class:`yarg.release.Release` objects for each file in a
is an executable regular file , or a symlink towards one .
takes a given filename ; tries to find it in the environment path ;
splits a command line into a list of arguments .
is a wrapper around select.select ( ) that ignores signals .
register file descriptors and
Try to suggest a semantic form for a version for which
Suggest a normalized version close to the given version string .
Check if the provided version matches the constraints .
Updates a key/value to the given .env
Removes a given key from the given .env
starting from the given directory up to the root
Run command in sub process.
Return dotenv as dict
Load the current dotenv as system environemt variable.
Get an all-lowercase version of the requirement 's name .
Monkey-patch urllib3 with PyOpenSSL-backed SSL-support.
have been met .
used by the
Given an PyOpenSSL certificate , provides all the subject alternative names .
feed a character with known length
based on existing data
Load requests lazily.
outline tables to inline tables .
Take an input command and run it , handling exceptions and error codes and returning
Parse a Python version output returned by ` python -- version ` .
Prepares a string for the shell ( on Windows too ! )
acts as a portal to the target environment .
Given a list of dependencies , return a resolved list of dependencies ,
Converts a Pipfile-formatted dependency to a pip-formatted one.
see if there 's a hard requirement for version
Determine if a path can potentially be installed
is for a File dependency .
PEP 423 style standard .
Properly case project name from pypi.org.
Given an executable name , search the given location for an executable
Canonicalize a list of packages and return a set of canonical names
Returns the path of a Pipfile in parent directories .
set os.environ temporarily
given string is an url
file from url to a path with filename
stay consistent .
Check if a provided path exists and is readonly .
Error handler for shutil.rmtree.
Call os.path.expandvars if value is a string , otherwise do nothing .
Take a pipfile entry and normalize its markers
Check if a given path is a virtual environment 's root .
Set a temporary sys.version_info tuple
Given a set and some arbitrary element , add the element ( s ) to the set
Compare two urls by scheme, host, and path, ignoring auth
Convert a path with possible windows-style separators to a posix-style path
Given a ` pythonfinder.Finder ` instance and an optional line , find a corresponding python
Given an input , checks whether the input is a request for python or notself .
Retrieve hashes for a specific ``InstallRequirement`` instance.
Select a way to obtain process information from the system .
traverse up the tree , yielding ` argv [ 0 ] ` of each process .
Form shell information from the SHELL environment variable if possible.
Get the shell that the supplied pid or os.getpid ( ) is running in .
Sends PreparedRequest object.
Initializes a urllib3 PoolManager .
given proxy .
Response < requests.Response > ` object from a urllib3
given URL .
Disposes of any internal state.
use when making the final request .
formatted URI to git+git @ format
VCS uris from pipenv.patched.notpip format
Determine if dictionary entry from Pipfile is for a vcs dependency .
given separators .
Convert a pipfile entry to a string
Retrieve a value from a nested object via a tuple representing the
Return a comparison of actual and expected hash values .
Reset the UniversalDetector and all of its probers back to their
Takes a chunk of a document and feeds it through all of the relevant
analyzing the current document and come up with a final
Start a bash shell and return a : class : ` REPLWrapper ` object .
Send a command to the REPL , wait for and return output .
set them on the current object .
set them on the current object
self.name `` if given a * * PEP-508 * * style URL
Generates a 3-tuple of the requisite * name * , * extras * and * url * to generate a
is a safeguard against decoy requirements when a user installs a package
implements a substitute for the forkpty system call .
makes the pseudo-terminal the controlling tty .
swallows exceptions .
Converts a value into a valid string.
Return a condensed version of help string .
Returns a system stream for byte processing.
Returns a system stream for text processing.
is similar to how the : class : ` File ` works but for manual
Formats a filename for user display.
Returns the config folder for the application .
Opens the file if it 's not yet open .
accept incomplete strings .
Prompts for confirmation (yes/no question).
takes a text and shows it via an environment specific
creates an iterable context manager that can be used
have the effect of clearing; Clears the terminal screen .
Edits the given text in the defined editor .
launches the given URL ( or filename ) in the default
stops execution and waits for the user to press any
makes an attribute optional .
Return a shallow copy of this graph .
Add a new vertex to the graph .
Remove a vertex from the graph , disconnecting all edges from/to it .
existing vertices .
Compare two digests of equal length in constant time.
given fingerprint matches the supplied certificate .
Resolves the argument to a numeric constant , which can be passed to
All arguments except for server_hostname, ssl_context, and ca_cert_dir have
Detects whether the hostname given is an IP address .
computing the current backoff
Get the value of Retry-After in seconds .
Sleep between retry attempts.
given HTTP method should be retried upon , depending if
Is this method/status code retryable ?; Based on whitelists and control
Are we out of retries?
Return a new Retry object with incremented retry counters .
Build a response by making a request or using the cache .
Try to look up the process tree via Linux 's /proc
using hashlib.sha256 ( )
Replace the Python tag in a wheel file name with a new value.
Determine if any scripts are not on PATH and format a warning .
Return the given rows of a RECORD file in sorted order .
path.
Install a wheel
Return the Wheel-Version of an extracted wheel , if possible .
looks like an egg_info .
build an InstallRequirement object using the
Format command information for logging.
Return the path to the wheel in the temporary build directory .
Return the lowest index that one of the wheel 's file_tag combinations
Is this wheel supported on this system ?
Build one wheel.
using the PEP 517 build process .
using the `` legacy '' build process .
Build wheels.
Get the distutils command for interacting with PyPI configurations .
Read the PyPI access configuration as supported by distutils , getting
have set `` username `` and; Save the PyPI access configuration .
Check that `` username `` and `` password `` have been set , and raise an
using the provided metadata .
reading lines of from a subprocess into a buffer .
Return a suitable command for signing a file .
Run a command in a child process , passing it any input data specified .
Sign a file .
Upload documentation to the index.
Return a suitable command for verifying a file .
Verify a signature for a file .
is a convenience method for downloading a file from an URL .
Send a standard library : class : ` Request ` to PyPI and return its
posting to an HTTP server .
Do the completion for bash
Do the fish completion
Do the powershell completion
Returns the completion code to be evaluated by the shell
Install the completion
Get a TreeBuilder class for various types of trees with built-in support
Our embarrassingly-simple replacement for mimetools.choose_boundary.
Iterate over fields.
deprecated : : 1.6
Encode a dictionary of `` fields `` using the multipart/form-data MIME format .
Return a resource finder for a package .
Return a resource finder for a path , which should represent a container .
Get a resource into the cache ,
Call SAX-like content handler based on treewalker walker
instantiates the Retrying object
Sleep a random amount of time between wait_random_min and wait_random_max
Sleep an incremental amount of time after each attempt, starting at
Return the return value of this Attempt instance or raise an Exception .
generate ranges with a length of more than
given is an internal python attribute .
checks if an attribute on a builtin mutable object
call this method to check if the
calls ( : meth : ` intercepted_binops ` )
Subscribe an object from sandboxed code and prefer the
Return an undefined object for unsafe attributes.
is detected , then this is routed through this
Call an object from sandboxed code .
Create a new attribute on a class .
Create a tuple subclass to hold ` Attribute ` s for an ` attrs ` class .
Get annotations for *cls*.
Transform all ` _CountingAttr ` s on a class into ` Attribute ` s .
Create a tuple of all values of * obj * 's * attrs * .
Add a hash method to * cls * .
* cls * .
Make a repr method for * attr_names * adding * ns * to the full name .
Add a repr method to * cls * .
Return the tuple of `` attrs `` attributes for a class .
Return an ordered dictionary of ``attrs`` attributes for a class, whose
have a validator .
Return a script of an initializer for * attrs * and a dict of globals .
create a new class called * name * with * attrs * .
composes multiple validators into one .
accumulated methods and return the class .
return a new class with a ` __slots__ ` attribute .
Add __module__ and __qualname__ to a *method* if possible.
apply * changes * .
adds * meth * to the list of validators .
allows to set the default for an attribute .
command strings and returns a Popen-ready list .
Std/out output (cached)
given pattern to appear in std_out
is complete .
Runs the current command and passes its output to the next
Backport of ``socket.makefile`` from Python 3.5.
Creates a processed traceback object from the exc_info .
Rewrites a syntax error to please traceback systems .
Return a string with the traceback .
Return a unicode string with the traceback as rendered HTML .
Standard python exc_info for re-raising
resolve_command `` method
Encode into a cmd-executable string.
make an abstract dist object .
Prepare a requirement that would be obtained from req.link
Prepare an editable requirement
Prepare an already-installed requirement
given list .
Print colorize text.
named module or requirement
Return this platform 's string for platform-specific distributions
code for the ` provided ` platform run on the ` required ` platform ?
run its ` script_name ` script
Return a current distribution object for a Requirement or string
Convert an arbitrary string to a standard version string
return an exception
Evaluate a PEP 508 environment marker .
Yield distributions accessible via `path_item`
multiple nested eggs .
Given a list of filenames , return them in descending order
accessible on a sys.path directory
Return a dist_factory for a path_item and entry
Attempt to list contents of path, but suppress some exceptions.
Yield non-empty lines from file at path
Given a path to an .egg-link , resolve distributions
named package includes a subpath of path_item ( if needed )
ensuring that all entries are ordered
is a namespace package
Ensure that previously-declared namespace packages include path_item
Compute an ns-package subpath for a filesystem or zipfile importer
Normalize a file/dir name for comparison purposes
given path appears to be an unpacked egg .
Given an iterable of lines from a Metadata file , return
Yield ``Requirement`` objects for each specification in `strs`
Return an adapter factory for `ob` from `registry`
Ensure that the parent directory of `path` exists
Split a string or iterable thereof into ( section , content ) pairs
required_by is non-empty , return a version of self that is a
Prepare the master working set .
Build a working set from a requirement spec .
Add a path item to `` .entries `` , finding any distributions on it
matching ` name `
requires ` and run ` script_name ` script
matching ` requirements ` are activated
Invoke `callback` for all distributions
Evaluate markers for req against each extra that
Scan `search_path` for distributions usable in this environment
can_add ( ) `` it and it has not already been added
matching ` req ` and usable on ` working_set `
Give an error message for problems extracting file ( s )
is overridden and set to an insecure
Perform any platform-specific postprocessing of ` tempname `
Build a dictionary similar to the zipimport directory
Load a manifest at path or return a suitable manifest already loaded .
is current for this zip_path
resolve it .
Resolve the entry point from its module and attrs .
Parse a single entry point from string ` src `
Parse an entry point group
Parse a map of entry point groups
A map of extra to its list of (direct) requirements
Given a mapping of extras to dependencies , strip off
needed for this distro if ` extras ` are used
is importable on ` path ` ( default=sys.path )
Return what this distribution 's standard .egg filename should be
Return a `` Requirement `` that matches this distribution exactly
Return the ` name ` entry point of ` group ` or raise ImportError
Return the entry point map for ` group ` , or the full entry map
substituting in any changed keyword args
installed by distutils ( e.g .
Recompute this distribution 's dependencies .
agglutinated rules in a definition-schema .
Validates a schema that defines rules against supported rules .
'allowed ' : ( 'allof ' , 'anyof ' , 'noneof ' , 'oneof ' ) }
Existing definitions are
Export the svn repository at the url to the destination location
allows the auth information to be passed to svn via the
change working directories
Get a spinner object or a dummy spinner to wrap a context .
Atomically open `target` for writing.
Open local or remote file for reading.
swap out * stream_name * with a stream wrapper .
reads data from the file descriptor .
compiles a pattern-string or a list of pattern-strings .
seeks through the stream until a pattern is matched .
takes a list of compiled regular expressions and returns the
is similar to expect ( ) , but uses plain string matching instead
is the common loop used inside expect .
reads at most `` size '' bytes from the file ( less if the read hits
reads and returns one entire line .
reads until EOF using readline ( ) and returns a list containing
Returns the length hint of an object .
invoking a program on a temporary file .
unformatted text .; is the ultimate fallback .
yields the items added to the bar during
Wrap your iterables with it .
Progress iterator.
Parse version to major, minor, patch, pre-release, build parts.
string to a VersionInfo instance .
Compare two versions
Compare two versions through a comparison
Returns the greater version of two versions
Returns the smaller version of two versions
according to the Semantic Versioning specification
Set constants _EOF and _INTR.
Start the given command in a child process in a pseudo terminal .
closes the connection with the child application .
returns True if echo is; returns the terminal echo mode .
sets the terminal echo mode on or off .
return at most `` size `` bytes from the pty .
Read one line from the pseudoterminal , and return it as unicode .
bytes to the pseudoterminal .
wraps send ( ) with mnemonic access for sending control
starts nicely with; forces a child process to terminate .
waits until the child exits .; is a blocking call .
tests if the child process is running or not .
Send the given signal to the child application .
Read at most `` size `` bytes from the pty , return them as unicode .
Write the unicode string ``s`` to the pseudoterminal.
Build an instance from a requirement.
Build a new instance from this and a new requirement .
Push a new state into history .
Take a collection of constraints , spit out the resolution result .
Remove the `` extra == ... '' operands from the list .
Build a new marker without the ` extra == ... ` part .
operands from a marker .
whehter a marker contains an `` extra == ... '' operand .
have occurred .
given project , get a dictionary mapping available versions to Distribution
Give an url a score which can be used to choose preferred URLs
are candidates for distribution
Get a digest from a dictionary by looking at keys of the form
Return the URLs of all the links on a page together with information
are created only when get_project is called , and terminate
Tell all the threads to terminate ( by sending a sentinel value ) and
See if an URL is a suitable download for a project .
referring page and with a
Get a URL to fetch from the work queue , get the HTML page , examine its
known to this locator .
Add a distribution to the finder .; update internal information
Remove a distribution from the finder .
Get a version matcher for a requirement .
fulfill a requirement .
is typically used; replace one provider with another .
depends on .
move output from a pipe to a queue .
Send data to the subprocess' stdin.
send ( ) , sending string `` s `` to child process , with os.linesep
Wait for the subprocess to finish.
Sends a Unix signal to the subprocess .
does not raise an exception if the
Attach a regular expression pattern matcher to a custom type converter
Convert a string to an integer .
incoming string containing some date / time info into a
apart the format [ [ fill ] align ] [ 0 ] [ width ] [ .precision ] [ type ]
Using `` format '' attempt to pull values from `` string '' .
Search "string" for the first occurrence of "format".
Match my format to the string exactly.
Search the string for my format.
Search "string" for all occurrences of "format".
Generate a Result instance for the given regex match object
provided packages and adds them to Pipfile , or ( if no packages are given ) , installs all packages from Pipfile .
removes it from Pipfile .
Spawns a shell within the virtualenv.
installed into the virtualenv .
provided in Pipfile .
Runs lock, then sync.
Displays currently-installed dependency graph information.
given module in your editor .
specified in Pipfile.lock .
returns either U+FFFD or the character based on the
is a generic handler for emitting the tags .
Converts a list of distributions into a PackageSet.
Check if a package set is consistent
checking if the dependency graph would be consistent after \
Computes the version of packages after installing to_install .
define three types of bytes :
retains only the sequences of English
Join the two paths represented by the respective
matched by this
Return the string representation of the path with forward ( / )
The final path component, if any.
The final component's last suffix, if any.
A list of the final component's suffixes, if any.
The final path component, minus its last suffix.
Return a new path with the file name changed .
Return a new path with the file suffix changed ( or added , if
providing sequence-like access to the
The logical parent of the path.
is absolute ( has both a root and , if applicable ,
matches the given pattern .
Open the file pointed by this path and return a file descriptor ,
is the same or not as this file
Does not yield any
Create this file with the given access mode , if it does n't exist .
Create a new directory at this given path .
Change the permissions of the path , like os.chmod ( ) .
Like chmod(), except if the path points to a symlink, the symlink's
Remove this file or link .
Remove this directory .; be empty .
Like stat(), except if the path points to a symlink, the symlink's
Rename this path to the given path .
Rename this path to the given path , clobbering the existing
Make this path a symlink pointing to the given path .
Whether this path exists.
is a directory .
is a regular file ( also True for symlinks pointing
is a FIFO .
is a socket .
Return a new path with expanded ~ and ~user constructs
Return the python codec name corresponding to an encoding or None if the
Produces a file object from source .
Returns (line, col) of the current position in the stream.
Read one character from the stream or queue if available .
Returns a string of characters from the stream up to but not
encoding declared by the meta element
past a list of characters
Look for a sequence of bytes at the start of a string.
matching a given sequence .
Return a name , value pair for the next attribute in the stream ,
load the build backend
Invoke the optional get_requires_for_build_wheel hook
Invoke optional prepare_metadata_for_build_wheel
Identify the .dist-info folder inside a wheel ZipFile .
Build a wheel and extract the metadata from it .
built during the get_wheel_metadata hook .
Invoke the mandatory build_wheel hook.
Appends a ( key , item ) to the table .
Patch two bugs in functools.update_wrapper .
Least-recently-used cache decorator.
evaluates true , else return None
Try to look up the process tree via the output of ` ps ` .
go through this function .
Visit assignments in the correct order.
Moved into a function for testability .
starts the given command in a child process .; does all the
waits until the terminal ECHO flag is set False .
reads at most size characters from the child application .
string `` s `` to the child process , returning the number of
send ( ) , sending string `` s `` to child process , with
Write control characters to the appropriate log files
sends a character which causes; sends an EOF to the child .
does not require; sends a SIGINT to the child .
gives control of the child process to the interactive user ( the
is used by the interact ( ) method .
Turn a list of extras into a string
Turn a string of extras into a parsed extras list
Turn a list of specifier tuples into a string
Given a direct url as defined by * PEP 508 * , convert to a : class : ` ~pip_shims.shims.Link `
Given a : class : ` ~pip_shims.shims.Link ` compatible URL , convert to a direct url as
Given a : class : ` ~packaging.requirements.Requirement ` instance with markers defining
Given a base path , look for the corresponding `` pyproject.toml `` file and return its
Split markers from a dependency
Split a vcs+uri formatted uri into ( vcs , uri )
Given a path or URI , check for a ref and split it from the path if it is present ,
Get a standardized key for an InstallRequirement .
Formats a packaging.requirements.Requirement with a lowercase name.
Generic formatter for pretty printing InstallRequirements to the terminal
printing the specifier part of
Pulls out the (name: str, version:str, extras:(str)) tuple from the pinned InstallRequirement.
sorts the input on the group key first .
Builds a dict-based lookup table (index) elegantly.
Generates an : class : ` ~pip._internal.req.req_install.InstallRequirement ` .
Get a cleaned list of all the candidates with valid specifiers in the ` requires_python ` attributes .
Given a packager name , get the variants of its name for both the canonicalized
Detect the best version depending on the fields used .
Return the distribution name with version .
Read the metadata values from a file path .
Write the metadata fields to filepath.
Write the PKG-INFO format data to a file object .
given iterable ` other ` and kwargs .
set a metadata field .
Get a metadata field .
Return fields as a dict.
get dependencies , given a set of extras
Remove item from six.moves.
s * * to six.binary_type .
s * to six.text_type .
defines __unicode__ and __str__ methods under Python 2 .
Parse a requirements file and yield InstallRequirement instances .
return a line iterator
result in creating/yielding
want to shlex
Return a parser for parsing requirement lines
Joins a line ending in '\ ' with the previous line ( except when following
Strips comments and filter empty lines.
match ' -- skip-requirements-regex ' pattern
be retrieved via ` os.getenv ` .
Restore the original SIGINT handler after finishing.
Call self.finish ( ) before delegating to the original SIGINT handler .
iterates over all fields that are defined and yields
Iterates over all direct child nodes of the node.
given type .; is a tuple ,
Reset the context of a node and all child nodes .
Set the line numbers of the node and children.
Set the environment for all nodes.
Return a const object if the value is representable as
Build a wheel from a source directory using PEP 517 hooks .
using PEP 517 hooks .
calling pip in a subprocess
Move all the children of the current node to newParent.
Check if an element exists between the end of the active
do n't insert it anywhere
Switch the function used to insert an element from the
insert it into the tree
Insert text data.
Get the foster parent element , and sibling to insert before
Return the final fragment
Evaluate a marker .
allow all wheels
Create a temporary directory and store its path in self.path
Remove the temporary directory created and reset state
Generates a series of temporary names .
Detect the encoding of the given byte string .
escaped markup back into a text string .
Escape a string .; ensures that for
be found for this , that it is found .
am pinned to an exact version .
Return a hash-comparer that considers my option- and URL-based
Move self._temp_build_dir to self._ideal_build_dir/self.req.name
Remove the source files from this requirement , if they are marked
satisfies or conflicts
is available .
Return a pkg_resources.Distribution for this requirement
satisfying this requirement .
args into a requirement set .
Create a package finder appropriate to this requirement command .
Represent a list of integers as a sequence of ranges :
falls into one of the ranges in ` ranges ` .
Return the hash digest of a file .
Return information about the OS distribution that is compatible
Return the distro ID of the OS distribution , as a string .
Return the name of the OS distribution , as a string .
Return the version of the OS distribution , as a string .
Return the version of the OS distribution , as a tuple of version
Return certain machine-readable information about the OS
Get the information items from the specified os-release file .
Get the information items from the lsb_release command output .
Parse the output of the lsb_release command .
Get the information items from the specified distro release file .
Parse a distro release file .
Parse a line from a distro release file .
Calculate the dependency tree for the package ` root_key ` and return
ignore when performing pip-sync ,
packages should be installed or uninstalled , given a set
uninstalls the given sets of modules .
Return a new spontaneous environment .
Perform a sanity check on the environment .
Create a new overlay environment that shares all the data with the
Iterates over the extensions by priority.
Get an item or attribute of an object but prefer the attribute .
Invokes a filter on a value the same way the compiler does it .
Parse the sourcecode and return the abstract syntax tree .
used by ` parse ` and ` compile ` .
given sourcecode and return a generator that yields
Called by the parser to do the preprocessing and filtering
template source code .
returns a callable that accepts keyword
is configured this; Load a template from the loader .
tries a number of templates
Does a typecheck and dispatches to : meth : ` select_template `
Load a template from a string .; parses the source given and
Return a dict for the globals .
is used by the; Creates a template object from a module .
Create a new : class : ` Context ` for this template .
works like the : attr : ` module ` attribute when called
Return the source line number of a line number in the
The debug info mapping.
is a stand-in function for ` urllib3.util.parse_url `
Given a url , remove the password and insert 4 dashes
is good for unicode on Python 3 .
are read-only , so when rmtree ( ) tries to
Gives the display value for a given path , making it relative to cwd
Is path is a directory containing setup.py or pyproject.toml ?
appears to be the index page of an svn repository
Yield pieces of data from a file-like object until EOF.
Convert a path to its canonical , case-normalized , absolute version .
take off .tar too
handles renaming across devices .
is within sys.prefix , if we 're running in a virtualenv .
given Distribution is an editable install .
Return a list of installed Distribution objects .
Return the path for the .egg-link file if it exists , otherwise , None .
Unzip the file (with path `filename`) to the destination `location`.
Return the contents of * filename * .
Return a context manager used by captured_stdout/stdin/stderr
Get the installed version of dist_name avoiding pkg_resources cache
Return the URL for a VCS requirement .
Parse out and remove the auth information from a netloc .
Replace the password in a netloc with "****", if it exists.
Protection of pip.exe from modification on Windows
Check if the python version in use match the ` requires_python ` specifier .
Initialize the enhanced click completion
is really a list of versions .; want a list of
Add install_req as a requirement to install.
Resolve what operations need to be done
Set a requirement to be installed .
Check if req_to_install should be skipped .
Takes a InstallRequirement and returns a single AbstractDist \
Prepare a single requirements file .
Create the installation order .
Escape &, <, >, ", ', etc.
containing loc within a string , counting newlines as line separators .
debugging parse actions .
define a delimited list of expressions - the delimiter
return the original , untokenized text for a given
decorate a returned token with its starting and ending
define string ranges for use in Word
defining parse actions that require matching at
define a parse action by mapping a function to all
create a validating parse action to be used with start
defining nested lists enclosed in opening and
simplify creating one type of ParseException
Extracts the exception line from the input string, and marks
take an exception and translate the Python internal traceback into a list
specified index ( default= `` last `` ) .
end of ParseResults list of elements .
Returns the parse results as a nested list of matching tokens , all converted to strings .
named parse results as a nested dictionary .
listing out the contents of
Pretty-printer for parsed results as a list, using the
Make a copy of this : class : ` ParserElement ` .
makes debugging and exception messages clearer .
referencing matching tokens as a nested attribute
Add a boolean predicate function to expression 's list of parse actions .
adds memoizing to the parsing logic .
Execute the parse expression with the given string .
scanString ` , simplifying the access to the tokens found
split a string using the given expression as a separator .
Overrides the default whitespace chars
be ignored ( e.g. , comments ) while doing pattern
debugging messages while doing pattern matching .
Execute the parse expression on the given file or filename .
transform the parsed
leaveWhitespace `` defined in base class , and also invokes `` leaveWhitespace `` on
create a parse action for converting parsed date string to Python datetime.date
create a parse action for converting parsed
Look for VCS schemes in the URL.
looks like an archive .
Check the Content-Type header to ensure the response contains HTML .
Send a HEAD request to the URL , and ensure the response contains HTML .
return the response .
based on the package 's canonical name .
Pull the version part out of a string .
Determine the HTML document 's base URL .
Determine if we have any encoding information in our headers .
used to generate link sort key for link tuples .
Returns the locations found via self.index_urls
Find all available InstallationCandidate for project_name
Try to find a Link matching req
given locations , skipping
Return an InstallationCandidate or None
Yields all links in the page
runs the given command ; waits for it to finish ; then
Deprecated : pass encoding to run ( ) instead .
Build the path URL to use .
Encode parameters in a piece of data.
Build the body for a multipart/form-data request .
Properly register a hook.
registered hook .
Prepares the entire request with the given parameters .
Prepares the given HTTP body data .
based on request method and body
Prepares the given HTTP auth data .
Prepares the given HTTP cookie data .
Prepares the given hooks .
True if this Response one of the permanent versions of redirect.
Content of the response, in unicode.
stored : class : ` HTTPError ` , if one occurred .
Releases the connection back to the pool .
Format an error message for an EnvironmentError
Process IPv6 address literals
Get a connection .; return a pooled connection if one is available .
returns a : class : ` urllib3.util.Timeout `
Is the error actually a timeout ?; raise a ReadTimeout or pass
Get a connection from the pool and perform an HTTP request .
Prepare the `` connection `` for : meth : ` urllib3.util.ssl_wrap_socket `
Establish tunnel connection early, because otherwise httplib
Rebuilds the base system path and all of the contained finders within it .
corresponds most closely to the version requested .
Make the given class un-picklable .
Create a new Enum subclass that replaces a collection of global constants
ensures only unique members exist in an enumeration .
create a new Enum class .
Returns the type for creating enum members , and the first inherited
Returns a Basic Auth string.
Convert the package data into something usable
Create a package finder appropriate to this list command .
Build a shebang line .
Make a script .
Take a list of specifications and make scripts from them ,
Returns a generator that yields file paths under a *directory*,
Create a : class : ` FilePerms ` object from an integer .
Make a new : class : ` FilePerms ` object based on the permissions
Called on context manager entry ( the : keyword : ` with ` statement ) ,
Loads a pipfile from a given path .
injects environment variables into TOML values
Load a Pipfile from a given filename .
Returns the SHA256 of the pipfile 's data .
Returns a JSON representation of the Pipfile.
PEP 508 specifiers .
copy data from file-like object fsrc to file-like object fdst
Copy data from src to dst
mode bits from src to dst
Copy all stat info (mode bits, atime, mtime, flags) from src to dst
cp src dst '' ) .
cp -p src dst '' ) .
delete a directory tree .
move a file or directory to another location .
given a group name .
given a user name .
Create a ( possibly compressed ) tar file from all the files under
Create a zip file from all the files under 'base_dir ' .
archiving and unarchiving .
Registers an archive format.
Create an archive file (eg.
Returns a list of supported formats for unpacking.
gets registered as an unpacker .
Registers an unpack format.
Unpack zip `filename` to `extract_dir`
Unpack tar/tar.gz/tar.bz2 `filename` to `extract_dir`
Unpack an archive.
Parse an HTML fragment as a string or file-like object into a tree
Parse a HTML document into a well-formed tree
Parse a HTML fragment into a well-formed tree fragment
Construct tree representation of the pkgs from the index.
Sorts the dict representation of the tree
Find a root in a tree by it's key
Reverse the dependency tree .
Guess the version of a pkg when pip does n't provide it
Convert tree to string representation
Converts the tree into a flat json representation.
Converts the tree into a nested json representation.
Output dependency graph as one of the supported GraphViz output formats.
Dump the data generated by GraphViz to stdout .
are not present or conflict with the
Return cyclic dependencies as list of tuples
installed version conflicts with required version
Check good hashes against ones built from iterable of chunks of
allows to replace `` None `` values by * default * or the
Removes nodes by index from an errorpath, relatively to the
is used by the; defined by path .
be used for the 'type ' rule .
need to be
Returns the document normalized according to the specified rules
{'oneof': [
validates a mapping against a validation-schema of
~cerberus.Validator.validate ` that returns
{'type': ('hashable', 'list'),
Validates value against all definitions and logs errors according
{'type': 'list', 'logical': 'anyof'}
{'type': 'list', 'logical': 'allof'}
{'type': 'list', 'logical': 'noneof'}
{'type': 'list', 'logical': 'oneof'}
{'nullable': False }
[ 'dict ' , 'string ' ] , 'validator ' : 'bulk_schema ' ,
required fields are not missing .
[ 'dict ' , 'string ' ] ,
'string ' , 'list ' ] ,
looks up the given attribute from a
double escape variables .
uses UTF-8 encoding ) .
Return a titlecased version of the value .
Sort a dict and yield (key, value) pairs.
sorts ascending , if you pass it
given iterable .
Return the smallest item from the sequence .
Return the largest item from the sequence .
Return a string which is the concatenation of the strings in the
Return the last item of a sequence .
Return a random item from the sequence .
Format the value like a 'human-readable' file size (i.e.
Converts URLs in plain text into clickable links.
Return a copy of the string with each line indented by 4 spaces .
Return a truncated copy of the string .
Return a copy of the string passed to the filter wrapped after
Convert the value into an integer.
string formatting on an object :
replace adjacent whitespace by one space .
return a list of lists containing
batches items .; works pretty much like ` slice `
given precision .
Group a sequence of objects by a common attribute.
Returns the sum of a sequence of numbers plus the value of parameter
Reverse the object or return an iterator that iterates over it the other
Get an attribute of an object .
looks up an attribute .
Dumps a structure to JSON so that it 's safe to use in `` < script > ``
Entry Point for completion of main and subcommand options.
Get the type of path completion ( `` file `` , `` dir `` , `` path `` or None )
is `` file `` or `` path `` , list all regular files
Build a wheel .
string using subprocess from a given path .
given path is a known executable from known executable extensions
looks like a possible name of python .
Given a path ( either a string or a Path object ) , expand variables and return a Path object .
Return all valid pythons in a given path
expand a list or : class : ` ~pythonfinder.models.path.PathEntry ` instance
be os.path.joined with cache_dir
Return a directory to store cached wheels for link
points to an actual artifact ( e.g .
Retrieves dependencies for the requirement from the dependency cache.
Retrieves dependencies for the install requirement from the JSON API.
know what it depends on .
know the value of Requires-Python .
Retrieves dependencies for the requirement from pipenv.patched.notpip internals.
format and quote a single header parameter .
~urllib3.fields.RequestField ` factory from old-style tuple parameters .
format and quote a single header .
Renders the headers for this request field.
Makes this request field into a multipart request field.
Generates an EmptyTag token
Generates SpaceCharacters and Characters tokens
Returns the string to activate a virtualenv .
return a filename from the various components .
Build a wheel from files in specified paths , and use any specified tags
Install a wheel to the specified paths .; kwarg `` warner `` is
Update the contents of a wheel in a generic way .
Loads .env file into sys.environ.
Adds a given path to the PATH .
Removes the virtualenv directory from the system .
Creates a Pipfile for the project , if it does n't exist .
given line .
Creates a virtualenv , if one does n't exist .
exist for the project .
given system path .
Executes the where functionality .
Executes the install functionality .
Creates a virtualenv .
Executes the freeze functionality .
Executes the purge functionality .
Executes the init functionality .
relies exclusively on
Returns the location of virtualenv-installed pip .
Emulates the system 's which .
Formats the help string.
is up-to-date .
does n't have activate_this.py , but does n't need it anyway .
run command either pulling from project or interpreting as executable .
yielding process ID and properties of each .
fail with an invalid value message .
Check a bytes string for a BOM to correctly detect the encoding
get the default value of the color flag .
named file or files as toml and returns a dictionary
string as toml
escapes if necessary and converts to unicode .
Internal helper to for context creation.
Given a logger object this returns a new undefined class that will
Render a parent block .
Looks up a variable like ` __getitem__ ` or ` get ` but returns an
Resolves a variable like : meth : ` resolve ` but returns the
Get a new dict with the exported variables .
Return the complete context as dict including the exported
create a derived context .
Super the block.
Cycles among the arguments with the current loop index.
has changed since the last call .
is being swapped out by the async implementation .
is where the example starts and the FSM state transitions are
adds a transition that associates :
adds the same transition for a list of input symbols .
returns ( action , next state ) given an input_symbol and state .
is the main method that you call to process input .
Take an IP string/int and return an object of the correct type .
split the netmask and raise AddressValueError if needed
sorted deduplicated IPv # Address .
Count the number of zero bits on the right hand side.
collapsing concurrent netblocks .
Collapse a list of IP objects.
Return a key suitable for sorting between networks and addresses .
Return prefix length from the bitwise netmask.
Return prefix length from a numeric string
Turn a netmask/hostmask string into a prefix length
Tell if self is partly contained in other .
Remove an address from a larger block .
Compare two IP objects.
join to make the current subnet .
containing the current network .
Make a ( netmask , prefix_len ) tuple from the given argument .
Turn the given IP string into an integer for comparison .
Turns a 32-bit integer into dotted decimal notation .
is a hostmask ( rather than a netmask ) .
is allocated for public networks .
Turn an IPv6 ip_str into an integer.
string into an integer .
Compresses a list of hextets .
Tuple of embedded teredo IPs.
Serializes the input token stream using the specified treewalker
Serializes the stream from the treewalker into a string
Return the current branch , or None if HEAD is n't at a branch
is a commit hash
Resolve a revision to a new RevOptions object with the SHA1 of the
equals the given name .
Return URL of the first remote encountered.
Return the relative path of setup.py to the git repo root .
stub URLs like 'user @ hostname : user/repo.git ' with 'ssh : // ' .
Make a name consistent regardless of source ( environment or file )
Get a value from the configuration .
Modify a value in the configuration .
Unset a value in the configuration .
Save the currentin-memory state .
representing the loaded configuration .
Loads configuration from configuration files
Loads configuration from environment variables
construct a dictionary with normalized keys .
Returns a generator with all environmental vars with prefix PIP_
associated with it .
make sure the command got the right number of arguments
Return a case-normalized absolute variable-expanded path .
Convert the supplied local path to a file uri.
Convert a valid file url to a local filesystem path
given url is a file url
creates the target directory and all of its parents if they do not
ensure ` mkdir_p ` is called to the function 's return value .
Create a tracked temporary directory .
Set read-write permissions for the current user on the target path.
~shutil.rmtree ` with additional error-handling .
delete files from a directory .
see if a pathlib ` Path ` object is a unc path or not
be relative .
given file-like object is closed .
have been successfully parsed .
has been a HEAD-request .
calculates the Levenshtein distance between a and b .
facilitates using communication timeouts to perform
attempts to find the prompt .
logs the user into the given server .
Sends exit to the remote shell.
Match the next shell prompt .
sets the remote prompt to something more unique than `` # `` or `` $ `` .
Return a string representing the user agent .
Convert a file : URL to a path .
be made absolute and have; Convert a path to a file : URL .
is a considered as an archive file .
Return whether a file:// Link points to a directory.
Unpack link into location.
Copy distribution files in `link_path` to `location`.
Unpack link.
downloaded file with correct hash
Create a pool key out of a request context dictionary .
Create a new : class : ` ConnectionPool ` based on host , port , scheme , and
Get a : class : ` ConnectionPool ` based on the host , port , and scheme .
Get a : class : ` ConnectionPool ` based on the request context .
Get a : class : ` ConnectionPool ` based on the provided pool key .
Similar to :func:`urllib3.connectionpool.connection_from_url`.
Merge a dictionary of override values for self.connection_pool_kw .
Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`
needed by proxies : specifically , the Accept and Host
be absolute .
be looked up from
referenced templates from the AST .
Remember all undeclared identifiers.
Parse a marker string and return a dictionary containing a marker expression .
Parse a requirement passed in as a string .; Return a Container
work on the native filesystem .
Return the default base location for distlib caches .
Convert an absolute path to a directory name for use in a cache.
Extract name, version, python version from a filename (no extension)
used to get name and version from a string .
zip a directory tree into a BytesIO object
Extended globbing function that supports * * and { opt1 , opt2 , opt3 } .
Tell if the target is newer than the source .
Copy a file respecting dry-run and force flags .
recorded changes , turn off recording , return
Add a subscriber for an event .
Remove a subscriber for an event .
Publish a event and return a list of values returned by its
Default converter for the inc:// protocol.
Read lines from a subprocess' output stream and either pass to a progress
return a list
Constructs a request to the PyPI server and returns a list of
Produce a mapping of identifier : requirement from the section .
Produce a mapping containing all candidates derived from ` identifiers ` .
specified ( abstract ) requirements into ( concrete ) candidates .
sets up all of the logging
Calls the standard formatter , but will indent all of the log messages
is using sys.stdout .
convert config values to boolean as ConfigParser do .
Return the value for option or default if defined .
Given a list of option strings this joins them in the most appropriate
Writes a usage line into the buffer .
Writes re-indented text into the buffer .
is how options; Writes a definition list into the buffer .
writes a paragraph , a heading ,
Returns a better error message when invalid configuration option
Return a comma-separated list of option strings and metavars .
is only one newline between usage and the first heading
given position .
Get a list of all options , including those in option groups .
Updates the given defaults with values from the config files and
Overriding to make updating the defaults after instantiation of
reimporting previously imported modules while inside the env
Given a local distribution and a working set , returns all dependencies from the set .
Returns the context appropriate paths for the environment .
Path to the environment python
The system path inside the environment
run inside the context of the environment
knowing which args we can use; Get the pip version in the environment .
Retrives the distributions installed on the library path of the environment
Find an egg by name in the given environment
is in the environment .
Given a package name , returns whether it is installed in the environment
Run a python command in the enviornment context .
Runs the environment 's inline activation script
activate the environment .
allows uninstallation of packages from the environment
Convert a string to a null-terminated bytes object .
Convert a null-terminated bytes object to a string .
Convert a number field to a python number .
Convert a python number to a number field .
Calculate the checksum for a member 's header by summing up all
fileobj dst .
Convert a file 's mode to a string of the form
Return True if name points to a tar archive that we
writing with gzip compression .
string s to the stream .
string s to the stream if a whole new block
Close the _Stream object.
reading a gzip compressed fileobj .
Set the stream's file pointer to pos.
Return the next size number of bytes from the stream .
Return size bytes from the stream.
is empty ,
Read data from the file .
Read at most size bytes from the file .
Read one entire line from the file .
Seek to a position in the file.
Return the TarInfo 's attributes as a dictionary .
Return a tar header as a string of 512 byte blocks .
Return the object as a ustar header block .
Return the object as a GNU header block sequence .
Split a name longer than 100 chars into a prefix
is a dictionary with file; Return a header block .
Return the string payload filled with zero bytes
Return a GNUTYPE_LONGNAME or GNUTYPE_LONGLINK sequence
Return a POSIX.1-2008 extended or global header sequence
Construct a TarInfo object from a 512 byte bytes object.
Return the next TarInfo object from TarFile object
Choose the right processing method depending on
Process a builtin type or an unknown type which
hold a GNU longname
Process a GNU sparse header plus extra headers.
Process an extended or global header as described in
extended sparse header , version 0.0 .
extended sparse header , version 0.1 .
extended sparse header , version 1.0 .
Replace fields with supplemental information from a previous
Decode a single field from a pax record.
return it ,
Open a tar archive for reading , writing or appending .
uncompressed tar archive name for reading or writing .
compressed tar archive name for reading or writing .
Close the TarFile.
Return a TarInfo object for member ` name ' .
Return the members of the archive as a list of TarInfo objects .
Create a TarInfo object for either the file ` name ' or the file
is False , only; Print a table of contents to sys.stdout .
Add the TarInfo object `tarinfo' to the archive.
Extract all members from the archive to the current working
Extract a member from the archive to the current working directory ,
Extract a member from the archive as a file object .
Extract the TarInfo object tarinfo to a physical
Make a directory called targetpath .
Make a file called targetpath .
Make a file from a TarInfo object with an unknown type
Make a fifo called targetpath .
Make a character or block device called targetpath .
Make a ( symbolic ) link called targetpath .
according to tarinfo .
Return the next member of the archive as a TarInfo object , when
Find an archive member by name from bottom to top.
Read through the entire archive file and look for readable
Check if TarFile is still open , and if the operation 's mode
hardlink member in the
debugging output to sys.stderr .
Returns the absolute path to a given relative path .
Returns a list of packages for pip-tools to consume.
Get the name of the virtualenv adjusted for windows if needed
Registers a proper name to the database.
Parse Pipfile into a TOMLFile and cache it
divided by PyPI and external dependencies .
Returns a list of all packages.
Creates the Pipfile , filled with juicy defaults .
Writes the given data structure out as TOML .
Write out the lockfile .
Given a source , find it .
Get the equivalent package name in pipfile
Adds a given index to the Pipfile .
Ensures proper casing of Pipfile packages
is retrieved , when available , for each
Return a native Python type from the list of compiled nodes .
CodeGenerator.visit_Output ` , but do not call
existing key : value items and import all key : value items from
Update this dictionary with the items from < mapping > , replacing
are modified directly , ala pass by
is in the dictionary ,
setdefault ( ) except < defaultlist > is a list of values to set
Add the values in <valuelist> to the list of values for <key>.
< key > 's list of values to < values > .; Existing items with key < key >
< values > from the values of < key > .
is in the dictionary , pop it and return its list of values .
< value > is provided , pops the first or last ( key , value ) item in the
return a key : value item .
return a key : valuelist item comprised of a key and that key 's
is provided and not in the dictionary .
Parity with dict.iteritems() except the optional <key> parameter has
Parity with dict.itervalues() except the optional <key> parameter has
Example:
Reverse the order of all items in the dictionary .
Return 2 sets:
Given an iterable of arguments and an iterable of nargs specifications ,
Given an argument string this attempts to split it into small parts .
Adds a new option named ` dest ` to the parser .
Adds a positional argument named ` dest ` to the parser .
Parses positional arguments and returns ``(values, args, order)``
given distributions .
generate a list of distributions from * dists * that are
making a dist given just a name and version .
setting it to its initial state .
Add a distribution to the cache .
populate the cache with
are converted into their
looks for distributions and returns
named distribution on the path .
find which distributions provide * name * .
Return the path to a resource file .
Return all of the exported entries in a particular category.
provided by this distribution .
Say if this instance matches ( fulfills ) a requirement .
Get the hash of some data , using a particular hash algorithm , if
Get the list of installed files for the distribution
Return the information exported by this distribution .
Read exports data from a file in .ini format.
Write a dictionary of exports to a file in .ini format .
change in the future .
Writes the `` RECORD `` file , using the `` paths `` iterable passed in .
Checks that the hashes and sizes of the files in ``RECORD`` are
are in the set 'prefix ' ,
shared location information to the SHARED file in .dist-info .
located under the `` .dist-info `` directory .
Iterates over the ``RECORD`` entries and returns paths for each line if
returns a tuple
Iterates over the ``installed-files.txt`` entries and returns paths for
Add an edge from distribution *x* to distribution *y* with the given
Add a missing * requirement * for the given * distribution * .
Prints only a subgraph
Writes a DOT output for the graph to the provided file * f * .
Perform a topological sort of the graph .
expect regular binary strings .
mapped to the index of the
Add an error to the tree.
Returns all errors for a particular path.
Returns a node for a path.
Adds an error or sub-tree to : attr : tree .
rewrites the error path to correctly represent logic errors
Dispatches a hook dictionary on a given piece of data .
is used to set , get or unset values from a .env file .
stored key/value .
given key/value .
Retrieve the value for the given key .
Removes the given key .
Run command with environment variables present.
Check whether the distribution is in the current Python installation .
installed packages based on given specifications .
install into .
Dumps a TOMLDocument into a string .
set `` allfiles `` to the absolute
Add a file to the manifest .
sorted files in directory order
adds some files from `` allfiles `` to
Validate a directive .
Select strings (presumably filenames) from 'self.files' that
Remove strings (presumably filenames) from 'files' that match
Translate a shell-like wildcard pattern to a compiled regular
Translate a shell-like glob pattern to a regular expression .
Adds a ( name , value ) pair , does n't overwrite the value if it already
Generic import function for any type of header-like object.
Returns a list of all the values for the named field.
including duplicate ones .
merging duplicate ones together .
Read headers from a Python 2 httplib message object.
Extract the cookies from the response into a CookieJar .
be sent with ` request ` , or None .
Unsets a cookie by name, by default over all domains and paths.
Make a cookie from underspecified parameters .
Convert a Morsel object into a Cookie containing the one k/v pair .
Returns a CookieJar from a key/value dictionary.
cookiejar and returns a merged CookieJar .
supports optional domain and path args in
list all the domains in the jar .
list all the paths in the jar .
True if there are multiple domains in the jar .
Updates this jar with cookies from another CookieJar or dict-like
uses this method internally to get cookie values .
get `` call this function : it 's never
Return a copy of this RequestsCookieJar .
returns a number , n constrained to the min and max bounds .
converts from the external coding system ( as passed to
returns a printable representation of the screen as a unicode
is similar to; returns a copy of the screen as a unicode string .
returns a copy of the screen as a unicode string with an ASCII
moves the cursor down with scrolling .
starts at 1 index .
puts a characters at the current cursor position .
inserts a character at ( r , c ) .
returns a list of lines representing the region .
keeps the cursor within the screen area .
Save current cursor position.
keeps the scroll region within the screen region .
scrolling from row { start } to row { end } .
display down one line .
Erases from the current cursor position to the end of the current
Erases from the current cursor position to the start of the current
Erases the entire current line .
Erases the screen from the current line down to the bottom of the
Erases the screen from the current line up to the top of the
Pull a value from the dict and convert to int
Parses ISO 8601 time zone specs into tzinfo offsets
dates into datetime objects
used to gracefully shut down the `` spinner `` instance
according to UTS46 processing .
Return a dict with the Python implementation and version .
Generate information for a bug report.
Called by : meth : ` write ` .
writing it to the virtual screen while handling
puts a character at the current cursor position .
Returns the default stream encoding if not found .
Get the size of the terminal window .
generating request headers .
is provided , move file to that point .
rewind body to a certain position .
Deep-copy a value into JSON-safe types.
clearing all the keys in a database .
treats any sequence type with the equal members as
create a : class : ` ~cerberus.Validator ` subclass .
Returns a tuple of `frozenset`s of classes and attributes.
Whitelist *what*.
Blacklist *what*.
Return the `` attrs `` attribute values of * inst * as a dict .
works on attrs instances , this works on anything .
Return the `` attrs `` attribute values of * inst * as a tuple .
Create a new instance , based on * inst * with * changes * applied .
Constructs a request to the PyPI server and returns a
are connected through the parent variable .
corresponds to this cmd_param.
other words whether or not the this cmd_param argument can still accept values
start with .
return a result depending on environment .
Evaluate a marker expression returned by the : func : ` parse_requirement `
using a reimplementation of the colorizer from
Merges the given Item with the last one currently in the given Container if
is strictly a child of another key .
parse the next item and returns it , along with its key
Returns (comment_ws, comment, trail)
Parses a key enclosed in either single or double quotes .
Parses a bare key .
parse a value at the current position .
Parses a table element .
cloning then restoring the
provided table first and bundles them into
Peeks ahead n characters.
Given a string and an iterable of delimiters , split on the first found
Given a url , return a parsed : class : ` .Url ` namedtuple .
Deprecated.
including the query string .
including host and port
Convert self into a url
Split a path into segments and perform a sanity check .
Get the template source , filename and reload helper for a template .
Loads a template .; looks up the template in the cache
Return a string describing the probable encoding of a file or
Convert a tree to a genshi tree
Parse a requirements formatted file .
imported modules in a project .
Display the difference between modules in a file and imported modules .
Remove modules that are n't imported in project from file .
referenced files .
deprecate existing functionality .
Exact matching of IP addresses.
based on the dependency tree .
Like Python 3.5's implementation of os.walk() -- faster than
Make a request using : meth : ` urlopen ` with the `` fields `` encoded in
wanting to receive the current context
pass the object on the
Given an object type this creates a decorator that will work
Attaches an argument to the command.
Attaches an option to the command.
be ignored by passing
Shortcut for password prompts.
Adds a `` -- help `` option which immediately ends the program
was installed by pip
Check for an update for pip.
abbreviated implementation name .
Return implementation version.
decrementing the minor
Return our platform name 'win32 ' , 'linux_x86_64
Return a list of supported tags for each version specified in
Returns the Requests tuple auth for a given url from netrc .
guess the filename of the given object .
look like they refer to a member of a zip
Take an object and test to see if it can be represented as a
described by RFC 2068 Section 2 .
described by RFC 2068 Section 2 and
Returns a key/value dictionary from a CookieJar.
Returns content type and parameters from given header
given HTTP Header Dict .
Iterate over slices of a string.
Returns the requested content back in unicode .
given URI .
allows you to check if an IP belongs to a network subnet
Very simple check of the cidr format in no_proxy variable.
Set the environment variable 'env_name' to 'value'
bypass proxies or not .
Select a proxy for the url , if applicable .
Given a URL that may or may not have a scheme , prepend the given scheme .
Given a url with authentication components , extract them into a tuple of
header value is a string which does n't contain
Given a url remove the fragment and the authentication part .
recorded starting position
is very similar to Version.__str__ , but has one subtle differences
Generate the python source for a node tree .
Does the node have a safe representation ?
Check if the names passed are accessed undeclared .
Create a copy of the current one .
Return an inner frame.
Fail with a :exc:`TemplateAssertionError`.
Enable buffering for the frame from that point onwards.
Return the buffer contents of the frame .
write into the frame buffer .
Simple shortcut for start_write + write + end_write.
Write a string into the output stream .
Combination of newline and write.
Add one or more newlines before the next write.
Writes a function call to the stream for the current node .
Pull all the dependencies.
Dump the function def of a macro or call block .
Dump the macro definition for the def created by macro_body .
Return a human readable position for the node .
Pops the topmost level for assignment tracking and updates the
Call a block and register it for the template .
Calls the extender .
Visit regular imports.
named imports .
If alive then mark as dead and return (obj, func, args, kwargs);
If alive then return (obj, func, args, kwargs);
be called at exit
Serialize an element and its child nodes to a string
Return the visitor function for this node or ` None ` if no visitor
Visit a node .
Called if no explicit visitor function exists for a node .
return lists in some places this method
calling the wrapper subprocess .
Build a wheel from this project .
r"""BNInception model architecture from <https://arxiv.org/pdf/1502.03167.pdf>`_ paper.
3x3 convolution with padding
Constructs a ResNet-18 model .
Constructs a ResNet-50 model .
r"""NASNetALarge model architecture from the
Constructs a ResNet-101 model .
Constructs a ResNet-152 model .
r"""AlexNet model architecture from the
r"""Densenet-121 model from
r"""Inception v3 model architecture from
r"""SqueezeNet model architecture from the `"SqueezeNet: AlexNet-level
VGG 11-layer model (configuration "A")
Selectable global pooling function with dynamic input kernel size
Download a URL to a local file.
Returns the model 's average precision for each class
PolyNet architecture from the paper
Get the cached value .
Get the value of a cached object .
Adds a new key value pair to the cache .
working directory .
Verify that DataFrames in `` frames `` have the same indexing scheme and are
Check if all values in a sequence are equal .
checks the lengths of each element in it .
Unzip a length n sequence of length m sequences into m seperate length
Perform a chained application of `` getattr `` on `` value `` with the values
setting attributes on a function .
Fold a function over a sequence with right associativity.
Invert a dictionary into a dictionary of sets.
r"""Projection vectors to the simplex domain
Run an example module from zipline.examples.
Compute slopes of linear regressions between columns of ``dependents`` and
loading data from Bank of Canada .
Load a DataFrame of data from a Bank of Canada site .
are a couple quirks in the data provided by Bank of Canada .
load data from this module .
is worse than the order 's limit price .
return the trailing mean volume over the
Validate a ` dtype ` and ` missing_value ` passed to Term.__new__ .
Check that value is a valid categorical missing_value .
passed to cls.__new__ based on the values
Return the identity of the Term that would be constructed from the
needed for each of our inputs to compute this
needs to put; Called with a column of the result of a pipeline .
held at this dividend 's ex date so
Update the position by the split ratio , and return the resulting
A note about cost-basis in zipline: all positions are considered
Creates a dictionary representing the state of this position .
Create a family of data bundle functions that read from the same
Used to mark a function as deprecated .
Returns
Get the Float64Multiply objects to pass to an AdjustedArrayWindow .
is a Float64Multiply window for each asset that can
pricing data with adjustments applied assuming that the
parsing for a 1d Pandas/numpy object containing string
find a unique asset whose symbol is the given string .
Main generator work loop.
have expired before starting a new sim day .
Get a perf message for the given datetime .
underlying adjustments db .
Returns the set of known tables in the adjustments file in DataFrame
use when unpacking sqlite tables as dataframes .
Calculate the ratios to apply to equities when looking back at pricing
Write both dividend payouts and the derived price adjustment ratios.
Writes data to a SQLite file to be read by SQLiteAdjustmentReader .
writes a value into ` out ` .
be passed to ` self.compute ` .
Call the user 's ` compute ` function on each window with a pre-built
making Aliased { Filter , Factor , Classifier } .
min_extra_rows pushes us back to a computation date .
delegating to self._wrapped_term._compute on sample dates .
making Downsampled { Filter , Factor , Classifier } .
applies pre-processors to the arguments of a function before
Wrap a function in a processor that calls ` f ` on the argument before
Build a preprocessed function with the same signature as ` func ` .
Get a Series of benchmark returns from IEX associated with ` symbol ` .
Surround `content` with the first and last characters of `delimiters`.
Get nodes from graph G with indegree 0
Draw `g` as a graph to `out`, in format `format`.
Display a TermGraph interactively from within IPython .
Format key, value pairs from attrs into graphviz attrs format
Apply a function but emulate the API of an asynchronous call .
show a progress bar for the given iterator .
Top level zipline entry point.
be exposed in IPython .
Run a backtest for the given algorithm .
The zipline IPython cell magic.
given bundle .
downloaded with the ingest command .
List all of the available data bundles.
making binary operator methods on a Filter subclass .
making unary operator methods for Filters .
creating new NumExprFactors .
re-apply ` mask ` .
are well-formed .
compute a mask of all values falling between
Parse a treasury CSV column into a more human-readable format .
Download daily 10 year treasury rates from the Federal Reserve and
subdir path to limit the number directories in any given
columns into uint32 columns .
Write the metadata to a JSON file in the rootdir.
Open an existing `` rootdir `` for writing .
given path .
return it .
sid container with empty data through the specified date .
Write all the supplied kwargs as attributes of the sid's file.
Write a stream of minute data .
given sid .
Internal method for `write_cols` and `write`.
Return the number of data points up to and including the
Truncate data beyond this date in all ctables.
Calculate the minutes which should be excluded when a window
keyed by the start and end of each range
Retrieve the pricing info for the given sid , dt , and field .
returns the position of the given minute in the
using the format used by
applied to an array of values , produces
Determine the last piece of information known on each date in the date
Forward fill values in a DataFrame with special logic to handle cases
shift ` days .
docstring `` .
allowing the use of templated docstrings .
Add a column .
Set a screen on this Pipeline .
Compile into an ExecutionPlan.
Helper for to_graph and to_execution_plan.
Render this Pipeline as a DAG .
are outputs of this pipeline .
Get the domain for this pipeline .
Create a tuple containing all elements of tup , plus elem .
has variables of the form x_0 , x_1 ,
string with numexpr .
rebound to the indices implied by
Merge the inputs of two NumericalExpressions into a single input tuple ,
Compute new expression strings and a new inputs tuple for combining
use when rendering Pipeline graphs .
Get the last modified time of path as a Timestamp .
Get the root directory for all zipline-managed files .
Build a dict of Adjustment objects in the format expected by
Load data from our stored baseline.
sure a passed column is our column .
resolve the symbol in the LEVERAGED_ETF list ,
Users should only access the lru_cache through its public API:
Weak least-recently-used cache decorator.
is a ` final ` object in the given ` mro ` .
Bind a `Column` object to its name.
Specialize `` self `` to a concrete domain .
Look up a column by name .
Construct a new dataset given the coordinates .
Take a slice of a DataSetFamily to produce a dataset
Check that the raw value for an asset/date/column triple is as
containing cls.expected_value ( asset_id , date ,
delegating to sub-loaders .
Make a random array of shape ( len ( dates ) , len ( sids ) ) with `` dtype `` .
Return uniformly-distributed floats between -0.0 and 100.0.
Return uniformly-distributed integers between 0 and 100.
Return uniformly-distributed dates in 2014.
Compute rowwise array quantiles on an input.
Handles the close of the given minute in minute emission .
Handles the start of each session .
Handles the close of the given day .
is complete , run the full period risk report
Encapsulates a set of custom command line arguments in key=value
Converts argument strings in key=value or key.namespace=value form
takes a root element , list of namespaces ,
Create a new registry for an extensible interface .
Construct an object from a registered factory.
is a minimum commission :
based on dollar value of shares .
Creates a dictionary representing the state of the risk report .
given root symbol , find the contract that is considered active
Get the rolls , i.e .; hop from contract to
Coerce buffer data for an AdjustedArray into a standard scalar
existing adjustments for a given index by appending
Return the input as a numpy ndarray .
Check that a window of length ` window_length ` is well-defined on ` data ` .
existing adjustments , handling index
produced when ` traverse ` is called on this Array .
rolling windows rows over our data .
Return a string representation of the data stored in this array .
Map a function over baseline and adjustment values in place.
Handle a TradingControlViolation , either by raising or logging and
've already placed self.max_count orders today .
is in the restricted_list .
given order exceeds either self.max_shares
given order would cause the magnitude of our position to be
hold negative shares of asset after completing this
has passed this Asset 's end_date , or before the
is greater than the allowed leverage .
are after the deadline .
Alter columns from a table.
Downgrades the assets db at the given engine to the desired version .
marking that a method is a downgrade to a version to the
db by removing the 'tick_size ' column and renaming the
db by removing the 'auto_close_date ' column .
db by adding a not null constraint on
db by copying the ` exchange_full ` column to ` exchange ` ,
Create a family of metrics sets functions that read from the same
Verify that the columns of `` events `` can be used by a
Selects the requested data for each date .
Compute the index in `dates` where the split-adjusted-asof-date
Given a sid , collect all overwrites that should be applied for this
Merge adjustments for a particular sid into a dictionary containing
Creates an AdjustedArray from the given estimates data for the given
Add entries to the dictionary of columns to adjustments for the given
Determine the last piece of information we know for each column on each
are on or after each simulation date and
Collects both overwrites and adjustments for a particular sid.
Calculates both split adjustments and overwrites for all sids.
Determines the date until which the adjustment at the given date
occur before the
occur after the
dates : pd.DatetimeIndex
split adjustments with the dict containing overwrites .
apply them to the
Collect split adjustments for future quarters.
passing ` decay_rate ` in terms of ` span ` .
passing `` decay_rate `` in terms of half
passing ` decay_rate ` in terms of center of
Check if a and b are equal with some tolerance .
Round a to the nearest integer if that integer is within an epsilon
is f ( self , other ) that coerces
Compute the expected return dtype for the given binary operator .
making binary operator methods on a Factor subclass .
making binary operator methods on a Factor .
making unary operator methods for Factors .
producing function application methods for Factor
is based on scipy.stats.mstats.winsorize
computes `` self `` and subtracts the mean from
Construct a Factor that Z-Scores each day's results.
Construct a new Factor representing the sorted rank of each column
Construct a new Factor that computes rolling pearson correlation
Construct a new Factor that computes rolling spearman rank correlation
Construct a new Factor that performs an ordinary least-squares
Construct a new factor that winsorizes the result of this factor .
computing quantiles of the output of `` self `` .
matching the top N asset values of self each day .
matching the bottom N asset values of self each day .
Construct a new Filter representing entries from the output of this
Verify that the stored rank method is valid .
compute a like-shaped array of per-row
Convert a time into microseconds since midnight .
Return a mask of all of the datetimes in `` dts `` that are between
Find the index of ``dt`` in ``dts``.
values in `` dts `` closest but not equal to `` dt `` .
be used as input to pd.concat .
Check that a list of Index objects are all equal .
Compute the set of resource columns required to serve
Verify that the columns of `` events `` can be used by an EventsLoader to
requested columns into columns that should load the next known
Eq check with a short-circuit for identical objects.
Rehydrate a LabelArray from the codes and metadata .
Convert self into a regular ndarray of ints.
Coerce self into a pandas categorical.
Coerce self into a pandas DataFrame of Categoricals.
Set scalar value into the array.
Shared code for __eq__ and __ne__ , parameterized on the actual
Make an empty LabelArray with the same categories as ``self``, filled
Map a function from str -> bool element-wise over ``self``.
Map a function from str -> str element-wise over ``self``.
rounding function for adjusting prices to the specified number
make sure the stop/limit prices are reasonable and raise
Build a zipline data bundle from the directory with csv files .
restrict Term methods to only be callable on
given period .
pre-calculates the benchmark return series for
given extensions .; be called by run_algo
Run a trading algorithm .
have a sid column .
Given an asset and dt , returns the last traded dt from the viewpoint
determines if this asset/field combination
returns a scalar value representing the value
Returns a list of adjustments between the dt and perspective_dt for the
representing the value
returns a dataframe containing history bars
returns a dataframe containing the requested
gets a window of adjusted minute data for an asset
gets a window of adjusted daily data for a sid
returns a list of adjustments for the given sid .
given sids and the given dt .
Returns all the stock dividends for a specific sid that occur
defined by the
Retrieves the future chain for the contract at the given ` dt ` according
Make a function that checks whether a scalar or array is of a given kind
Make a value with the specified numpy dtype .
repeat ` count ` times along the first axis .
repeat ` count ` times along the last axis .
Restride an array of shape
Check if a value is np.NaT .
is_missing function that handles NaN and NaT .
returns ` float ` arrays rather than int
differ from the previous value .
Compute the start and end dates to run a pipeline for .
Compute a pipeline .
Compute a lifetimes matrix from our AssetFinder , then drop columns that
Compute the Pipeline terms in the graph for the requested start and end
computed pipeline results into a DataFrame for public APIs .
Verify that the values passed to compute_chunk are well-formed .
Resolve a concrete domain for `` pipeline `` .
be called from within
Simple implementation of grouped row-wise function application.
Format a bulleted list of values.
Create a DataFrame representing lifetimes of assets that are constantly
Create a DataFrame representing assets that exist for the full duration
Create a DataFrame representing assets that all begin at the same start
Create a DataFrame representing futures for ` root_symbols ` during ` year ` .
testing data that simulates the notice/expiration date
returning True for asset/date pairs where the output
matching values starting with `` prefix `` .
matching values ending with `` suffix `` .
matching values containing `` substring `` .
checks regex matches against `` pattern `` .
indicating whether values are in `` choices `` .
needs to return an object; Called with the result of a pipeline .
produced by this classifier into an array of integer
produce a LabelArray when we
Check that all axes of a pandas object are unique .
Modify a preprocessor to explicitly allow ` None ` .
converts the input into a numpy dtype .
converts the input into a tzinfo object .
converts the input into a pandas Timestamp
Preprocessing decorator that verifies inputs have expected numpy dtypes .
Preprocessing decorator that verifies inputs have expected dtype kinds .
Preprocessing decorator that verifies inputs have expected types .
making preprocessing functions that check a predicate on the
Preprocessing decorator that verifies inputs are elements of some
Preprocessing decorator verifying that inputs fall INCLUSIVELY between
Preprocessing decorator that verifies inputs are numpy arrays with a
coerces inputs of a given type by passing
Preprocessing decorator that applies type coercions .
has an expected set of keys .
Construct a new enum object .
Get the oldest frame in the panel .
Resizes the buffer to hold a new window with a new cap_multiple .
is not safe to persist; Get a Panel that is the current data in view .
stored in our current in-view data to be values of the
window worth of data up to position zero .
based on price triggers and the
Given an order and a trade event , return a tuple of
For a market order, True.
Lives in zipline.__init__ for doctests.
Define a unique string for any set of representable args .
meets the protocol for datasource outputs .
meets the protocol for datasource TRADE outputs .
generating namestrings and
trade_count trades for each sid in sids list .
provided by Quandl .
Fetch WIKI Prices data table from Quandl
builds a daily dataset using Quandl 's WIKI Prices dataset .
streaming data from a URL , printing progress information to the
returning a BytesIO containing the loaded data .
Resample a DataFrame with minute data into the frame expected by a
Resample an array with minute data into an array with session data.
returns the first value that occurs
returns the largest high seen between
returns the smallest low seen between
returns the latest close at the given
returns the sum of all volumes
Infer the domain from a collection of terms .
Given a date , align it to the calendar of the pipeline 's domain .
Returns the date index and sid columns shared by a list of dataframes ,
Write the OHLCV data for one country to the HDF5 file.
Construct from an h5py.File and a country code.
Construct from a file path and a country code.
Build an indexer mapping ``self.sids`` to ``assets``.
are contained in the daily bars .
Retrieve the value at the given coordinates .
Get the latest day on or before `` dt `` in which `` asset `` traded .
Construct from an h5py.File.
set indentifier columns as indices .
be delimited and splits it in to a company
Generates an output dataframe from the given subset of user-provided
are no cases where multiple symbols resolve to the same
Split out the symbol: sid mappings from the raw data.
Convert a timeseries into an Int64Index of nanoseconds since the epoch .
Checks for a version value in the version table.
Inserts the version value in to the version table .
Write asset metadata to a sqlite database in the format that it is
Write asset metadata to a sqlite database.
are present in the current assets database .
database and create tables .
Returns a standard set of pandas.DataFrames:
Given an expression representing data to load , perform normalization and
Convert a tuple into a range with error handling .
Convert a tuple into a range but pass ranges through silently .
Check that the steps of `` a `` and `` b `` are both 1 .
Check if two ranges overlap .
Merge two ranges with step == 1.
helper for ``_group_ranges``
Return any ranges that intersect.
Returns a handle to data file.
series_or_df ` have data on or before first_date and on or after
given calendar and
have benchmark data for ` symbol ` from ` first_date ` to ` last_date `
have treasury data from treasury module associated with
Specialize a term if it 's loadable .
Add a term and all its children to `` graph `` .
Return a topologically-sorted iterator over the terms in `` self `` which
Calculate initial refcounts for execution of this graph.
Decrement terms recursively.
Decrement in-edges for ``term`` after computation.
input ` is an input to ` term ` ,
load/compute of ` term ` .
're going to compute at least N extra rows of ` term ` .
Load mask and mask row labels for term.
've specialized all loadable terms in the graph .
Make an extension for an AdjustedArrayWindow specialization.
Read a requirements.txt file , expressed as a path relative to Zipline root .
is tz-naive , assume it is UTC .; Normalize a time .
Builds the offset argument for event rules.
Builds the date argument for event rules.
Builds the time argument for event rules.
coerces integral floats to ints .
Constructs an event rule from the factory api.
Adds an event to the manager .
Calls the callable only when the rule is triggered .
Composes the two rules with a lazy composer .
Given a date , find that day 's open and period end ( open + offset ) .
Given a dt , find that day 's close and period start ( close - offset ) .
Drops any record where a value would not fit into a uint32 .
Read CSVs as DataFrames from our asset map.
Internal implementation of write.
Compute the raw row indices to load for each asset on a query for the
Get the colname from daily_bar_table and read all of it into memory ,
Construct and store a PipelineEngine from loader.
Call self._initialize with ` self ` made available to Zipline API
is not set , then create one based on frequency .
attached with eager=True .
Run the algorithm .
is a capital change for a given dt , this means the the change
Query the execution environment.
Fetch a csv from a remote url and register the data so that it is
Adds an event to the algorithm 's EventManager .
be called according to some timed rules .
Create a specifier for a continuous contract .
Lookup an Equity by its ticker symbol.
Lookup multuple Equities as a list.
based on the type of
Place an order.
validating parameters to the order API function .
converting deprecated limit_price and stop_price
Place an order by desired value rather than desired number of
Sync the last sale prices on the metrics tracker to a given
triggered by the simulation loop whenever the current dt
Returns the current simulation datetime .
Set the slippage models for the simulation.
Sets the commission models for the simulation .
Sets the order cancellation policy for the simulation .
be resolved to their assets
Place an order in the specified asset corresponding to the given
adjust a position to a target number of shares .
adjust a position to a target value .
adjust a position to a target percent of the
Place a batch market order for multiple assets .
Retrieve all of the current open orders.
based on the order id returned from one of the
Cancel an open order.
DEPRECATED: use ``data.history`` instead.
be checked on each bar .
Set a limit on the minimum leverage of the algorithm .
be checked prior to order calls .
Set a limit on the number of shares and/or dollar value held for the
Set a limit on the number of shares and/or dollar value of any single
Set a limit on the number of orders that can be placed in a single
Set a restriction on which assets can be ordered .
be computed at the start of each day .
Get the results of the pipeline that was attached with the name :
Internal implementation of `pipeline_output`.
providing values for at least ` start_date ` .
Return a list of all the TradingAlgorithm API methods .
Checks for the presence of an extra to the argument list.
make sure that it satisfies the given
Takes a callable and returns a tuple with the list of Argument objects ,
is restricted for all dts if it is in the static list .
Returns whether or not an asset or iterable of assets is restricted
Processes a list of splits by modifying any positions as needed .
Given a list of dividends whose ex_dates are all the next trading
based on the dividends that should be paid out
The current status of the positions.
Add a transaction to ledger , updating the current state as needed .
was placed .
Process the commission.
Process dividends for the next session.
Retrieve the dict-form of all of the transactions in a given bar or
Retrieve the dict-form of all of the orders in a given bar or for
Force a computation of the current portfolio state.
Override fields on ``self.account``.
Given a datashape type , return the associated numpy type .
returns a dataset from a blaze expression .
passed match up .
Check that a field is a datetime inside some measure .
Find the correct metadata expression for the expression.
Verify that the baseline and deltas expressions have a timestamp field .
Create a Pipeline API object from a blaze expression .
Bind a Blaze expression to resources.
Computes a lower bound and a DataFrame checkpoints .
given time range properly forward filling
map a datset to a collection of blaze expressions .
map a single bound column to a collection of blaze
Given a dict of mappings where the values are lists of
Builds a dict mapping to lists of OwnershipPeriods, from a db table.
mapping group keys to maps of keys to to lists of
Filter out kwargs from a dictionary.
Takes in a dict of Asset init args and converts dates to pd.Timestamps
was active at the time corresponding to
Retrieve asset types for a list of sids.
Retrieve all assets in `sids`.
Retrieve the most recent symbol for a set of sids .
loading assets from a table .
Resolve a symbol to an asset object without fuzzy matching .
Lookup an equity by symbol.
Lookup a list of equities by symbol .
Lookup a future contract by symbol .
Get the value of a supplementary field for an asset .
Convert asset_convertible to an asset.
Convert an object into an Asset or sequence of Assets.
cache a recarray of asset lifetimes .
Compute a DataFrame representing asset lifetimes for the specified date
given country .
Get the latest minute on or before `` dt `` in which `` asset `` traded .
calculating its held
html
 wechatid 
 
 
 10   
 



  URL
  URL
  URL
lxml.etreeelem
requestsgetpost
html&quot;
 
 
  
  
    

decodes an image from a file object as a Numpy array .
Process image files from the dataset.
be gzipped .
Generates examples from TMX file.
Generates examples from TSV file.
Generates examples from Wikiheadlines dataset file.
Generates examples from CzEng v1.6, with optional filtering for v1.7.
languages into ( potentially ) template strings .
make up each split of the dataset for the language pair .
Returns the examples in the raw ( text ) form .
Fetches a `tfds.core.DatasetBuilder` by string name.
Loads the named dataset into a ` tf.data.Dataset ` .
Extract kwargs from name str.
Try cast to int , float , bool , str , in that order .
Try importing a module , with an informative error message on failure .
Returns list from list, tuple or ndarray.
Transpose a nested dict [ list ] into a list [ nested dict ] .
See base class for details .
examples as dicts .
Calculate statistics for the specified split.
Read JSON-formatted proto into DatasetInfo proto.
< dataset_name > / < config_name > / < version > ) .
are different from the current ones .
Split setter (private method).
Update from the DatasetBuilder.
dataset_info_dir ` .
Update DatasetInfo from the JSON file in `dataset_info_dir`.
Initialize DatasetInfo from GCS bucket info files.
resolve the promise .
dled file to definitive place , write INFO file , return path .
returns Promise- > path to downloaded file .
Extract a single archive , returns Promise- > path to extraction result .
returns Promise- > path .
given Kaggle competition .
given url ( s ) .
Returns iterator over files within archive.
given path ( s ) .
extract given url_or_urls .
Returns the directory containing the manually extracted data .
Construct a list of BuilderConfigs .
Return the test split of Cifar10 .
corrupted Cifar10 test data .
Doc string for a single builder, with or without configs.
Get all builders organized by module in nested dicts .
Make statistics information table.
given datasets .
schema.org microdata for DatasetSearch from DatasetBuilder .
Generating a Gaussian blurring kernel with disk shape .
Zoom image with clipping.
Generate a heightmap using diamond-square algorithm .
Gaussian noise corruption to images.
Shot noise corruption to images.
Impulse noise corruption to images.
blurring to images .
Frosted glass blurring to images .
Zoom blurring to images.
Fog corruption to images.
Change brightness of images.
Change contrast of images.
Conduct elastic transform to images.
Pixelate images.
Conduct jpeg compression to images.
assign obj.attr to value .
grouped by their keys .
Apply a function recursively to each element of a nested data struct .
return a data struct with the same shape .
Simulate proto inheritance.
Path to tensorflow_datasets directory.
Writes to path atomically , by writing to temp file and renaming it .
Given a hash constructor , returns checksum digest and size of file .
Reraise an exception with an additional message .
attr that handles dots in attr name .
returns the examples in the raw ( text ) form .
Update the encoding format .
Update the shape .
encoded as jpeg or png .
given image into a dict convertible to tf example .
Reconstruct the image from the tf example .
Create a moving image sequence from the given image a left padding values .
Construct a linear trajectory from x0.
bouncing around .
Return the tuple ( major , minor , patch ) version extracted from the str .
True if other_version matches .
Returns labels for validation.
Whether any of the filenames exist.
based on filename .
Create temporary files for filenames and rename on exit.
Create temporary dir for dirname and rename on exit.
Shuffle a single record file in memory .
generated str records to output_files in round-robin order .
Write records from generator round-robin across writers.
Single item to a tf.train.Feature.
be updated in threads .
Increment total pbar value.
Increment current value.
Generate examples as dicts.
Copy data read from src file obj to new file in dest_path.
yielding ( path , object-like ) tuples .
Add a progression bar for the current extraction .
Returns `promise.Promise` => to_path.
has been extracted there .
Convert a ` TensorInfo ` object into a feature proto object .
given value to Feature if necessary .
given feature from the tfexample_dict .
Ensure the two list of keys matches .
Unpack the celeba config file.
Generate QuickDraw bitmap examples.
import tensorflow , and ensure its version is sufficient .
maintain compatibility across versions .
use it .
is a Dataset .
Returns SplitGenerators from the folder names.
Generate example for each image in the dict.
Create a new dataset from a template .
Append the new dataset file to the __init__.py .
Return the validation split of ImageNet2012 .
corrupted imagenet validation data .
corrupted images .
match the pattern given by shape2 .
hiding GPUs .
yielding the graph .
Execute the given TensorFlow function .
Create a new graph for the given args .
Create a unique signature for each fct/inputs .
Converts the given image into a dict convertible to tf example .
given the directory path .
target given the directory path .
Strip ID 0 and decrement ids by 1.
reserved tokens and a regex for splitting them out of strings .
compiled regex to parse out reserved tokens .
Writes lines to file prepended by header and metadata .
parsing out header and metadata .
Splits a string into tokens .
Convert a python slice [ 15:50 ] into a list [ bool ] mask of 100 elements .
Return the mapping shard_id= > num_examples , assuming round-robin .
Return the list of offsets associated with each shards .
Check that the two split dicts have the same names and num_shards .
Add the split info.
initialized from the ` repeated_split_infos ` .
Returns a list of SplitInfo protos that we have.
prevents concurrent calls to functions .
given url .
Make built-in Librispeech BuilderConfigs.
Walk a Librispeech directory and yield examples .
download urls for this config .
string = > integer .
Conversion integer => class name string.
token counts from generator .
Validate arguments for SubwordTextEncoder.build_from_corpus.
tokens for encoding .
text into a list of integers .
Decodes a list of integers into text .
Convert a single token to a list of integer ids .
Encode a single token byte-wise into integer ids .
Converts a subword integer ID to a subword string.
split token into subwords .
Initializes the encoder from a list of subwords .
Save the vocabulary to a file .
Extracts list of subwords from file.
based on the ` corpus_generator ` .
vs Dogs images and labels given a directory path .
Loads a data chunk as specified by the paths .
formatted matrix stored in filename .
Generate examples for the Smallnorb dataset.
Constructs a ` tf.data.Dataset ` from TFRecord files .
Create a dataset containing individual instruction for each shard .
Build the mask dataset to indicate which element to skip .
Map an instruction to a real datasets for one particular shard.
Converts a `tf.data.Dataset` to an iterable of NumPy arrays.
Loads the images and latent values into Numpy arrays .
Discretizes array values to class labels.
Generate examples for the Shapes3d dataset.
Strips formatting and unwanted sections from raw page content.
Build PCollection of examples in the raw (text) form.
given dataset .
Yields (labels, np_image) tuples.
function to be called using keyword arguments .
Returns arguments of fn with default=REQUIRED_ARG.
Download a file from GCS , optionally to a file .
List all files in GCS bucket.
given dataset directory .
is available on the GCS bucket gs : //tfds-data/datasets .
kaggle command with subprocess .
List of competition files.
Returns 'kaggle://' urls.
Downloads competition file to output_dir.
given the image directory path .
Returns dict {'dataset_name': 'path/to/checksums/file'}.
are stored for a given dataset .
stored within file .
associating URL to ( size , sha256 ) .
given checksums and sizes for specific dataset .
given file name ( or path ) .
shorten url to fit in max_length .
Returns name of file for (url, checksum).
Returns info dict or None.
Write the INFO file next to local file.
use on resource at path .; be None .
exists locally , at ` resource.path ` .
string = > encoded list [ int ] .
decoded string .
Call SubwordTextEncoder.build_from_corpus is encoder_cls is such .
Sharded filenames given prefix and number of shards .
Walk an Omniglot directory and yield examples .
Get alphabet and label names, union across all dirs.
Returns a human readable size string.
Add a progression bar for the current download .
Download with Kaggle API.
Returns url, possibly with confirmation token.
Synchronous version of `download` method.
Resize an image to have ( roughly ) the given number of target pixels .
given CSV .
Return the list of files and reading mask of the files to read .
Construct the split filenames associated with the split info .
Generate MovingMnist sequences.
Parses single video from the input tfrecords.
Generates examples for the dSprites data set.
listed within given CSV files .
bounded boxes listed within given CSV file .
Generate IMDB examples.
Get hashes of urls in file.
files corresponding to urls .
Get filenames for a particular split.
Get abstract (highlights) and article from a story file path.
Export the results.
Extract links from robots.txt and sitemap.xml.
Handle the requests and return the response body .
Extract intel from the response body.
Extract js files from the response body
Extract details from the response body.
Extract endpoints from JavaScript code.
Update the current installation .
subdomains according to the TLD .
uses a threadpool to execute a function .
Extract a string based on regex pattern supplied by user .
Determine whether or not a link should be crawled
Parse a list for non-matches to a regex .
Write the results.
Return the passed time .
Calculate the entropy of a string .
extracts valid headers from interactive input .
Extract the top level domain from an URL .
Match IP:PORT or DOMAIN:PORT in a losse manner
Present the user a prompt .
start the market thread and register backtest broker thread
driven data flow
the standard message which can be transfer
account_cookie
()
()
pivot
  
  ,


deal
ATTENTION CHANGELOG 1.0.28

/

resume the account from standard message
do  run 


order_handlerorder_status
order_handlerdeal_status
order_queue
SMA
A<B then A>B  AB BA
2018/05/23 

 mean absolute deviation
macd Series
MultiIndexcondDateTimeIndexcond
today all
save stock_day

check!
order/market
pandas  gpcw20171231.zip
return shanghai margin data
return shenzhen margin data

login 


datetime date , 
6
code==> list


 self.user_cookie  portfolio
account
make a simple account with a easier way
accountportfolio
QA_USER

 `` % Y- % m- % d '' 
 n  ()
 n  ()
,,towards/
, start,end=QA_util_get_real_datelist


,* tb//
,*
index
QA_util_save_csv(data,name,column,location)

clients


  
mongo
add a account/stratetgy
account
Account
return the account/strategy back '
check the account whether in the protfolio dict or not
portfolio







Get the local time of the next schedule time this job will run .
MA

LC=REF(CLOSE,1);
MIKE

TR:(-)--
1.>80 <20 
:type series: List
:type seriesLenght: int
:type start: int

'zyfw',  'jyps'# 'zygcfx' 


save account
QUANTAXIS Log Module
save file
stock_ip_listexclude_ip_listip
 '2011-09-11'   20110911
datetime.datatime
 '2018-01-01'   datatime 
 '2018-01-01'   float   time.time() 
 '2018-01-01 00:00:00'   float   time.time() 
datestampdatetime

 

''
return medium


turn the dict '
plot the market_data
dataframegroupby,bycodedatetime
DataStruct
reindex
IO --> hdf5
QADATASTRUCT/apply

code,start,end

()


bar
 QA 
pingip
bar



sell 0 -- buy 2 -- 



#todo   
  CU    

pd.concat listDataStruct,
DataStruct
dataframe from tushare

 '20180101'   float   time.time() 
datastock_block
blockname
getcode 
getblock , block_nameliststr
get_both_code 
k

/


Execute a command on the command-line .
Create the tables needed to store the information .
Retrieves the job with the selected ID .
existing job .
Adds a new job into the cache .
Adds a job run result to the history table .
tick  
tick



save stock info
save stock_list
save index_list
save etf_list
save future_list
save future_day
save future_day_all
save future_min
save stock_min
save index_day
save index_min
save etf_day
save etf_min
save stock_xdxr
save stock_block
select save_engine , tushare ts Tushare  Tushare  tdx 
'
 , 
purpose:
ctp tick
get the account
get the risk message
get the user

save stock_week
save stock_info
save stock_transaction
run a shell commad
''
''
''
 
, codelist, num

QUANTAXIS   --xlsfile--csvfile


(broker)
trade 
{
  
600  
 / 
Return the bot 's response based on the input .
Return a response based on a given input statement .
provided is a valid response .
:rtype: dict
Imports the specified module based on the
Raises an exception if validate_class is not a
Returns the amount of time taken for a given
Print progress bar
Get the first match unit metric object supported by pint library
Returns the firt match ` pint.unit.Unit ` object for from_unit and
matched input statement .
is called when a logic adapter is unable to generate any
Provide an analysis of significant features in the string .
is appropriate for this
Takes a statement string .
eliminates possibly repetitive responses to prevent
:type input_statement: Statement
Compare the two input statements .
Return the calculated similarity of two
Return the class for the statement model .
Return Statement object when given data
Returns a list of statements in the database
Creates a new statement matching the keyword arguments specified .
Creates multiple statement entries.
Returns a random statement from the database
Add a list of strings to the statement as tags .
Preprocess the input statement .
Create a file from the database that can be used to
Train the chat bot based on the provided list of
Check if the data file is already downloaded .
Check if the data file is already extracted .
Download a file from the given url .
Extract a tar file at the specified file path .
Return the number of entries in the database .
Removes the statement that matches the input text .
Returns a list of objects from the database.
Modifies an entry in the database.
Returns a random statement from the database.
Drop the database.
Populate the database with the tables .
Return a response to the statement in the posted data .
Reads a dotted file path and returns the file path .
return the data from a corpus json file .
Return a list of file paths to each data file in the specified corpus .
Return the data contained within a specified corpus .
Return a string of text containing part-of-speech , lemma pairs .
Update the provided statement .
Remove all data from the database .
Remove any consecutive whitespace characters from the statement text .
escaped html characters into unescaped html characters .
Converts unicode characters to ASCII character equivalents.
Convert strings to numbers
Convert time to hour, minute
Extract date from quarter of a year
Converts relative day to time
adverbs to dates
dates from duration
Finds coming weekday
Finds previous weekday
Finds next weekday
Extract datetime objects from a string of text.
Search for close matches to the input.
Set window layout.
Get a response from the chatbot and display it .
Display svelte components in iPython.
Save object as json on CNS.
Save dict of numpy array as npz file.
Save numpy array as image file on CNS.
file on CNS .
Create view frustum matrix.
alogn specified axes .
specified axes .
Generate LookAt modelview matrix.
Sample random camera position.
Parse vertex indices in '/' separated form (like 'i/j/k', 'i//k' ...).
Unify lengths of each row of a.
Load 3d mesh form .obj' file.
fit into -1 .. 1 cube
sampled activations , which requires network access .
Create input tensor.
Import model GraphDef into the current graph.
layout to between [ 0,1 ] .
is returned .; be a list of ndarrays .
stitch it into a single image
Call the user defined aggregation function on each cell and combine into a single json object
make it current .
Collapse `shape` outside the interval (`a`,`b`).
resizes a tensor t to have shape target_shape .
specified layer sampled from iterating over
Computes the covariance matrix between the neurons of two layers .
using prerecorded correlations
interpolating between each pair of N objectives .
Register a gradient function to a random string.
Convenience wrapper for graph.gradient_override_map().
setting custom gradients for TensorFlow functions .
A naive, pixel-based image parameterization.
Computes 2D spectrum frequencies.
using 2D Fourier coefficients .
Simple laplacian pyramid paramaterization of an image.
sampling graph .
Multiply input by sqrt of emperical (ImageNet) color correlation matrix.
valid rgb colors .
Add Inception bottlenecks and their pre-Relu versions to the graph.
creating Objective factories .
Visualize a single neuron of a single channel .
Visualize a single channel
Visualize a direction
Visualize a single ( x , y ) position along the given direction
Visualize a direction ( cossine similarity )
used as penalty .
Minimizing this objective is equivelant to blurring input each step .
Interpolate between layer1, n_channel1 and layer2, n_channel2.
Encourage the boundaries of an image to have less variation and of color C .
neighboring images to be similar .
Encourage diversity between each batch element .
optimized image and orig_img .
Like channel, but for softmax layers.
Convert obj into Objective class.
Gradient for constrained optimization on an L2 unit ball.
tranfomed to be constrained in a L2 unit ball .
tranfomed to be constrained in a L_inf unit ball .
Flexible optimization-base feature vis.
Even more flexible optimization-base feature vis.
x , y
Write a file for each tile
Load image file as numpy array.
decode a string .
Load GraphDef from a binary proto file.
Load a file .
Ensures the specified spatial shape by either padding or cropping .
Given an arbitrary rank-3 NumPy array , produce one representing an image .
Given a normalized array , returns byte representation of image encoding .
Given an arbitrary rank-3 NumPy array ,
Serialize 1d NumPy array to JS TypedArray .
applying f to inner dimension of acts .
Set target style variables.
Create a data URL representing an image from a PIL.Image .
Display an image.
Display a list of images with optional labels .
Display a nupmy array without having to specify what it represents .
Strip large constant values from graph_def.
Visualize a TensorFlow graph .
Resize an ndarray image of rank 3 or 4 .
composites them .
Produces a tensor paramaterized by a interpolated lower resolution tensor .
Create an intractive TensorFlow session.
Read from any URL .
Read from any URL with a file handle .
Returns the path that remote_url would be cached at locally .
Compositional Pattern Producing Network
Returns a model instance such as `model = vision_models.InceptionV1()`.
given model 's layer .
given models ' layers .
Given two layers , combines their activations according to mode .
Given a layout and activations , overlays a grid on the layout and returns
Ignore sizes of dimensions (1, 2) of a 4d tensor in shape inference.
Return frozen and simplified graph_def of default graph.
Embed meta data as a string constant in a TF graph.
extract meta data hidden in graph_def .
handcoding graph traversal please no
using before parameter .
using after parameter .
using around parameter .
|coro|
Constructs a : class : ` Colour ` from an HSV tuple .
Returns the cog 's description , typically the cleaned docstring .
walks through this cog 's commands and subcommands .
are defined in this cog .
marks a function as a listener .
Sets the footer for the embed content .
Sets the author for the embed content .
Adds a field to the embed object .
Modifies a field to the embed object.
Converts this embed object into a dict.
Returns a friendly URL version of the avatar the user has.
is mentioned in the specified message .
Returns a : class : ` list ` of : class : ` User ` \s that the user is friends with .
Returns a : class : ` list ` of : class : ` User ` \s that the user has blocked .
r"""|coro|
Creates a : class : ` DMChannel ` with this user .
is your friend .
is blocked .
be created at the given date .
returns the first element in the iterable that meets
Returns string's width.
Resolves an invite from a :class:`Invite`, URL or ID
escapes Discord 's markdown .
Adds two numbers together.
Rolls a dice in NdN format.
Repeats a message multiple times .
Joins a voice channel
Plays a file from the local filesystem
does n't predownload )
Changes the player's volume
Queries the duration of the call .
returns the : class : ` list ` of : class : ` User ` that are currently in this call .
Creates a partial : class : ` Webhook ` .
Creates a partial : class : ` Webhook ` from a webhook URL .
TextChannel ` ] : The text channel this webhook belongs to .
Returns a friendly URL version of the avatar the webhook has.
|maybecoro|
representing the users that have reacted to the message .
see this channel .
are currently inside this voice channel .
Returns the channels that are under this category .
TextChannel ` ] : Returns the text channels that are under this category .
Returns the voice channels that are under this category .
Handles permission resolution for a :class:`User`.
have been overridden from
Returns the channel-specific overwrites for a member or a role .
Returns all of the channel's overwrites.
Handles permission resolution for the current :class:`Member`.
enables receiving the destination 's message history .
Asset ` : Returns an asset of the emoji , if it is custom .
Role ` ] : A : class : ` list ` of roles that is allowed to use this emoji .
schedules a task in the background for you with
Starts the internal task in the event loop .
Adds an exception type to be handled during the reconnect logic .
being handled during the reconnect logic .
acts as a decorator to register a coroutine to be
.Guild.me ` except it may return the : class : ` .ClientUser ` in private message contexts .
send_help(entity=<bot>)
Compute the next delay
True if self has the same or fewer permissions as other .
updates this permission object .
:class:`AuditLogChanges`: The list of changes this entry has.
transforms a function into a : class : ` .Command `
transforms a function into a : class : ` .Group ` .
adds a check to the : class : ` .Command ` or its
.check ` that is added that checks if the member invoking the
Similar to :func:`.has_role` except checks if the bot itself has the
Similar to :func:`.has_any_role` except checks if the bot itself has
.check ` that is added that checks if the member has all of
Similar to :func:`.has_permissions` except checks if the bot itself has
.check ` that checks if the person invoking this command is the
.check ` that checks if the channel is a NSFW channel .
adds a cooldown to a : class : ` .Command `
Updates :class:`Command` instance with updated attribute.
Creates a copy of this : class : ` Command ` .
Retrieves the parameter OrderedDict without the context or self parameters .
Retrieves the fully qualified parent command name .
Retrieves the parents of this command .
Retrieves the fully qualified command name .
is currently on cooldown .
Resets the cooldown on this command .
registers a coroutine as a local error handler .
registers a coroutine as a pre-invoke hook .
registers a coroutine as a post-invoke hook .
Gets the `` short '' documentation of a command .
Returns a POSIX-like signature useful for help command output.
Adds a : class : ` .Command ` or its subclasses into the internal list
Remove a : class : ` .Command ` or subclasses from the internal list
walks through all commands and subcommands .
Get a : class : ` .Command ` or subclasses from the internal list
invokes : func : ` .command ` and adds it to
Creates a copy of this : class : ` Group ` .
Creates a main websocket for Discord from a : class : ` Client ` .
meets the predicate .
Sends the IDENTIFY packet .
Sends the RESUME packet .
handles the general gateway loop .
Creates a voice websocket for the : class : ` VoiceClient ` .
Clears the paginator to have no pages .
Adds a line to the current page .
terminate a page .
Retrieves the bot mapping passed to : meth : ` send_bot_help ` .
cleaned up invoke prefix .; are `` @ name `` instead of `` < @ id > `` .
Similar to :attr:`Context.invoked_with` except properly handles
Retrieves the signature portion of the help page .
prevent abuse .
Returns the largest name length of the specified command list .
text to fit into the : attr : ` width ` .
Indents a list of commands after the specified heading.
send the page output from : attr : ` paginator ` to the destination .
Adds the minified bot heading with commands to the output .
formatting information on a subcommand .
Adds the formatting information on a command 's aliases .
format commands and groups .
Plays an :class:`AudioSource`.
Sends an audio packet composed of the data .
Clears the internal state of the bot .
blocking call that abstracts away the event loop
registers an event to listen to .
Asset ` : The same operation as : meth : ` Guild.icon_url_as ` .
Asset ` : The same operation as : meth : ` Guild.banner_url_as ` .
Asset ` : The same operation as : meth : ` Guild.splash_url_as ` .
returns an array of user IDs matched with
returns an array of channel IDs matched with
returns an array of role IDs matched with
returns the content in a `` cleaned up ''
returns the content that is rendered
Returns a URL pointing to the large image asset of this activity if applicable .
Returns a URL pointing to the small image asset of this activity if applicable .
:class:`str`: The album cover image URL from Spotify's CDN.
implements when mentioned or other prefixes provided .
Adds a global check to the bot .
Removes a global check from the bot .
~discord.User ` or : class : ` ~discord.Member ` is the owner of
.listen ` .
Removes a listener from the pool of listeners .
registers another function as an external
Adds a `` cog '' to the bot .
Removes a cog from the bot .
Loads an extension.
Unloads an extension.
reloads an extension .
Indicates if the guild is a 'large ' guild .
belongs to this guild .
Similar to :attr:`Client.user` except an instance of :class:`Member`.
TextChannel ` ] : A list of text channels that belongs to this guild .
CategoryChannel ` and their associated channels .
TextChannel ` ] : Returns the guild 's channel used for system messages .
Gets the @ everyone role that all members have by default .
is `` chunked '' .
Returns the shard ID for this guild if applicable .
found that matches the name provided .
enables receiving the guild 's audit logs .
latency between a HEARTBEAT and a HEARTBEAT_ACK in seconds .
List[Tuple[:class:`int`, :class:`float`]]: A list of latencies between a HEARTBEAT and a HEARTBEAT_ACK in seconds.
Returns a :class:`list` of :class:`Member` with this role.
Create a list of blueprints , optionally grouping them under a
Register the blueprint to the sanic app.
Create a blueprint route from a decorated function .
Create a blueprint websocket route from a decorated function .
Create a blueprint websocket route from a function .
Create a listener from a decorated function .
Create a blueprint middleware from a decorated function .
enables the process of creating a global exception
Create a blueprint static route from a decorated function .
Add an API URL under the **GET** *HTTP* method
used for adding exceptions to : class : ` SanicException ` .
Raise an exception based on SanicException .; Returns the HTTP response
run later , after the loop has started .
Decorate a function to be registered as a route
Decorate a function to be registered as a websocket route
register a function as a websocket route .
Enable or disable the support for websocket.
provides the app user a mechanism by which an already
Decorate a function to be registered as a handler for exceptions
Register an application level middleware that will be attached
be called before a request .
serve files from .
Register a blueprint on the application.
provided for invoking the : func : ` blueprint ` method
Take a request from the HTTP Server and return a response object
Run the HTTP Server and listen until keyboard interrupt or term
run ` .
used by ` run ` and ` create_server ` .
Returns response object with body in json format.
Returns response object with body in text format.
Return a streaming response object with file data .
be used to
cause a 302 redirect ( by default ) .
Writes a chunk of data to the streaming response .
runs the ` streaming_fn ` callback that writes
leverages this insert method to
be used to implement a Middleware plugin to
executes an exception handler and returns a response
Provide a default behavior for the objects of : class : ` ErrorHandler ` .
Creates SSLContext instance for usage in asyncio.create_server.
Trigger event callbacks (functions or async)
Start asynchronous HTTP Server on an individual process.
Check if the connection needs to be kept alive based on the params
Check if elapsed time since last response exceeds our configured
provided to enable the logging of responses in case if
Writes response content synchronously to the transport .
are closed and the sanic app encounters
is called when KeepAlive feature is used ,
Parse a request body and returns fields and files
Stop reading when gets None
return the auth header token .
parse ` query_string ` using ` urllib.parse.parse_qs ` .
parse ` query_string ` using ` urllib.parse.parse_qsl ` .
return the original client ip based on X-Forwarded-For
was borrowed from distutils.utils .
Load a configuration from an environment variable pointing to
Update the values from the given object .
Looks for prefixed environment variables and applies
Parse a parameter string into its constituent name , type , and
Add a handler to the route list
Check if a URL pattern exists in a list of routes provided based on
based on the specified view name .
Get a request handler based on the URL of the request , or raises an
Get a list of supported methods for a url and optional host .
is stream or not .
Returns the executable .
Create a new process and a subprocess in it with the same arguments as
kill child processes of a process ( maximum two level ) .
kill child processes of a process .
Watch project files, restart worker process if a change happened.
Return view function for use with the routing system, that
perform 'get ' request on url
present in the headers given .
Preserve shape of the image.
Preserve dummy channel dim.
Bleaches out pixels, mitation snow.
Add fog to the image.
Add sun flare.
shadows to the image .
Barrel / pincushion distortion.
described in [ Simard2003 ] _ ( with modifications ) .
Flip a bounding box vertically around the x-axis.
Flip a bounding box horizontally around the y-axis.
depending on the value of ` d ` .
Crop a bounding box using the provided coordinates of bottom-left and top-right corners in pixels and the
Rotates a bounding box by 90 degrees CCW ( see np.rot90 )
Rotates a bounding box by angle degrees
Transposes a bounding box along given axis .
Flip a keypoint vertically around the x-axis.
Rotates a keypoint by 90 degrees CCW ( see np.rot90 )
Scales a keypoint by scale_x and scale_y .
Crop a keypoint using the provided coordinates of bottom-left and top-right corners in pixels and the
Unified rounding in all python versions .
Normalize coordinates of a bounding box.
Denormalize coordinates of a bounding box.
Normalize a list of bounding boxes .
Denormalize a list of bounding boxes .
Calculate the area of a bounding box in pixels .
bounding boxes and return only those boxes whose visibility after transformation is above
Convert a bounding box from a format specified in ` source_format ` to the format used by albumentations :
Convert a bounding box from the format used by albumentations to a format , specified in ` target_format ` .
Convert a list bounding boxes from a format specified in ` source_format ` to the format used by albumentations
Convert a list of bounding boxes from the format used by albumentations to a format , specified
Check if bbox boundaries are in range 0 , 1 and minimums are lesser then maximums
Remove bounding boxes that either lie outside of the visible area by more then min_visibility
bounding boxes .
Convert input argument to min-max tuple
Check if keypoint coordinates are in range [ 0 , 1 )
Check if keypoints boundaries are in range [ 0 , 1 )
watching source files for changes .
re-run a script whenever its source changes .
Partition the ``addrinfo`` list by address family.
given host and port .
Close all open connections and asynchronously wait for them to finish.
Rewrite the `` remote_ip `` and `` protocol `` fields .
Undo changes from `_apply_xheaders`.
Sets the default locale .
Loads translations from CSV files in a directory.
Loads translations from `gettext`'s locale tree
Returns the closest match for the given locale code .
Returns the Locale for the given locale code .
Returns the translation for the given message for this locale .
given date ( which should be GMT ) .
given date as a day of week .
given list of parts .
given integer .
set context for translation , accepts plural forms .
handle unmatched optional
be implemented to return an appropriate instance of ` ~.httputil.HTTPMessageDelegate `
Appends new rules to the router .
Returns an instance of `~.httputil.HTTPMessageDelegate` for a
Matches current instance against the request.
Returns a tuple (reverse string, group count) for a url.
parse it for links .
given cursor .
Imports an object by name.
Provides the errno from an Exception object.
masking function .
returning newly-available data .
Sets the class to use when the base class is instantiated .
Returns the old value of the named argument without replacing it .
named argument in `` args , kwargs `` with `` new_value `` .
Converts a `tornado.httputil.HTTPServerRequest` to a WSGI environment.
RequestHandler ` subclasses to enable streaming body support .
Use this decorator to remove trailing slashes from the request path .
require that the user be logged in .
Called in async handlers if the client closed the connection .
Resets all headers and content for this response.
Sets the status code for our response .
Sets the given response header name and value .
Adds the given response header and value .
undoing a previous ` set_header ` call .
Returns the value of the argument with the given name .
Returns the value of the argument with the given name
Decodes an argument from the request.
An alias for
Returns the value of the request cookie with the given name .
outgoing cookie name/value with the given options .
Deletes the cookie with the given name .
Deletes all the cookies the user sent with this request .
timestamps a cookie so it can not be forged .
timestamps a string so it can not be forged .
Returns the given signed cookie if it validates , or None .
Returns the signing key version of the secure cookie .
Sends a redirect to the given ( optionally relative ) URL .
Writes the given chunk to the output buffer .
given arguments as the response .
used to render the final js links for the
used to render the final embedded js for the
used to render the final css links for the
used to render the final embedded css for the
Generate the given template with the given arguments .
be used as the default template namespace .
Flushes the current output buffer to the network.
ending the HTTP request .
Take control of the underlying stream .
Sends the given HTTP error code to the browser .
implement custom error pages .
The locale for the current session.
Determines the user 's locale from `` Accept-Language `` header .
authenticated user for this request .
The XSRF-prevention token for the current user/session.
generate the xsrf token in its raw form .
Convert a cookie string into a the tuple form returned by
matches the `` _xsrf `` argument .
given relative static file path .
Raises an exception if the given app setting is not defined .
Alias for `Application.reverse_url`.
Computes the etag header to be used for this request .
Checks the `` Etag `` header against requests 's `` If-None-Match `` .
given output transforms .
customize logging of uncaught exceptions .
Starts an HTTP server for this application on the given port .
Appends the given handlers to our handler list .
~.httputil.HTTPMessageDelegate ` that can serve a request
named `` name ``
Writes a completed HTTP request to the logs .
Sets the `` Etag `` header based on static url version .
Sets the content and caching headers on the response .
True if the headers indicate that we should return 304 .
Returns the absolute location of `` path `` relative to `` root `` .
return the absolute path .
Retrieve the content of the requested resource which is located
Returns the time that `` self.absolute_path `` was last modified .
Returns the `` Content-Type `` header to be used for this request .
customize cache control behavior .
Constructs a versioned url for the given path .
Converts a static URL path into a filesystem path.
Generate the version string to be used in static URLs .
returns it as a string .
is valid within HTML or XML .
Un-escapes an XML-escaped string.
objects for the given JSON string .
given value from a URL .
Parses a query string like urlparse.parse_qs , but returns the
Converts a string argument to a byte string.
Converts a string argument to a unicode string.
converting byte strings to unicode .
plain text into HTML with links .
Returns the current thread 's ` IOLoop ` .
Clears the ` IOLoop ` for the current thread .
given handler to receive the given events for `` fd `` .
The IOLoop catches and logs exceptions, so it's
Starts the ` IOLoop ` , runs the given function , and stops the loop .
Runs the `` callback `` at the time `` deadline `` from the I/O loop .
Runs the `` callback `` after `` delay `` seconds have passed .
Runs the `` callback `` at the absolute time designated by `` when `` .
Calls the given callback on the next IOLoop iteration .
Schedules a callback on the ``IOLoop`` when the given
Runs a function in a `` concurrent.futures.Executor `` .
Runs a callback with error handling .
Starts the timer .
Stops the timer .
Concatenate url and arguments regardless of whether
Parses a Range header .
Returns a suitable Content-Range header:
Parses a form request body .
Parses a `` multipart/form-data `` body .
used by HTTP .
Returns a (method, path, version) tuple for an HTTP 1.x request line.
Returns a (version, code, reason) tuple for an HTTP 1.x response line.
Parse a Content-type like header .
Inverse of _parse_header.
used by HTTP auth .
Returns ``(host, port)`` tuple from ``netloc``.
converting a result of `` parse_qs `` back to name-value pairs .
escaping in cookie values .
Parse a `` Cookie `` HTTP header into a dict of name/value pairs .
Adds a new value for the given key .
given header as a list .
Returns an iterable of all (name, value) pairs.
Updates the dictionary with a single header line .
Returns a dictionary from HTTP header text.
A dictionary of ``http.cookies.Morsel`` objects.
Returns the amount of time it took for this request to execute .
Returns the client 's SSL certificate , if any .
listening sockets bound to the given port and address .
Adds an ` .IOLoop ` event handler to accept new connections on `` sock `` .
given string is a well-formed IP address .
Try to convert an `` ssl_options `` dictionary to an
wrapping the given socket .
run a synchronous method asynchronously on an executor .
completes , so does the other .
given `` exc `` as the ` Future ` 's exception .
given `` exc_info `` as the ` Future ` 's exception .
call `` callback `` when `` future `` is complete .
according to `` mode `` .
Generate this template with the given arguments .
Loads a template .
Decorator for asynchronous generators.
Runs multiple asynchronous operations in parallel.
Wait for multiple asynchronous futures in parallel.
Converts ``x`` into a `.Future`.
Wraps a `.Future` (or other yieldable object) in a timeout.
Return a ` .Future ` that resolves after the given number of seconds .
Convert a yielded object into a ` .Future ` .
yield the next available result .
resumes the generator , running until it reaches a
Append the given piece of data ( should be a buffer-compatible object ) .
Get a view over at most `` size `` bytes ( possibly fewer ) at the
Advance the current buffer position by ``size`` bytes.
read until we have matched the given regex .
read until we have found the given delimiter .
read a number of bytes .
reads all data from the socket until it is closed .
given data to this stream .
Call the given callback when the stream is closed .
Close this stream.
complete the current read operation from buffered data .
appends the result to the read buffer .
complete the currently-pending read from the buffer .
find a position in the read buffer that satisfies
Adds `state` (IOLoop.
Return `` True `` if exc is ECONNRESET or equivalent .
Connects the socket to a remote address without blocking .
Convert this `IOStream` to an `SSLIOStream`.
is valid according to the configured
Wait for the initial SSL handshake to complete.
logging output as configured .
Add logging-related flags to ``options``.
Creates a AsyncHTTPClient .
Timeout callback of request.
Timeout callback of _HTTPConnection instance.
Calculates the HMAC-SHA1 OAuth 1.0a signature for the given request .
Redirects to the authentication URL for this service.
authenticated user data upon redirect .
Redirects the user to obtain OAuth authorization for this service .
Gets the OAuth authorized user and access token .
override this to get basic information about the
given URL auth an OAuth2 access token .
Just like `~OAuthMixin.authorize_redirect`, but
given API path , e.g. , `` statuses/user_timeline/btaylor ``
Handles the login for the Google user , returning an access token .
Handles the login for the Facebook user , returning a user object .
given relative API path , e.g. , `` /btaylor/picture ''
Wait for `.notify`.
Wake `` n `` waiters .
True `` .; are awakened .
is true .
wake one waiter .
Decrement the counter.
Attempt to lock.
Read a single HTTP response .
Clears the callback attributes .
Implements `.HTTPConnection.write`.
Closes the connection .
serving requests on this connection .
Client-side websocket support.
Sends the given message to the client of this Web Socket .
ping frame to the remote end .
Closes this Web Socket.
enable support for allowing alternate origins .
Set the no-delay flag for this stream .
Runs the given callback with exception handling .
aborts the WebSocket connection by closing the socket
Verifies all invariant- and required headers
Computes the value for the Sec-WebSocket-Accept header ,
sent by the server to this client connection .
set to keyword arguments
Send ping frame.
returning its Future if it is a coroutine .
Closes the WebSocket connection .
Return `` True `` if this connection is closing .
sending periodic pings to keep the connection alive
Send a ping to keep the websocket alive
Closes the websocket connection .
Sends a message to the WebSocket server .
Reads a message from the WebSocket server .
Defines an option in the global namespace.
Parses global options from the command line.
Parses global options from a config file.
An iterable of (name, value) pairs.
created by `` define `` .
The names and values of options in a group.
The names and values of all options.
Defines a new command line option .
given on the command line ( defaults to
loads the config file at the given path .
stderr ( or another file ) .
Convert a SQL row to an object supporting dict and attribute access .
Execute a SQL statement .
Query for a list of results.
Query for exactly one result.
accepting connections on the given port .
accepting connections on the given sockets .
given port on the given address .
Starts this server in the ` .IOLoop ` .
listening for new connections .
Put an item into the queue , perhaps waiting until there is room .
Put an item into the queue without blocking .
return an item from the queue .
return an item from the queue without blocking .
enqueued task is complete .
are processed .
Returns the number of processors on this machine .
Starts multiple worker processes .
Runs ``callback`` when this process exits.
resolves when the process exits .
Initializes the `` SIGCHLD `` handler .
Removes the `` SIGCHLD `` handler .
Called by libcurl when it wants to change the file descriptors
Called by libcurl to schedule a timeout .
Called by IOLoop when there is activity on one of our
Called by IOLoop when the requested timeout has passed .
Called by IOLoop periodically to ask libcurl to process any
were completed by the last
Starts the mock S3 server on the given port at the given path .
Closes the HTTPClient , freeing any resources used .
Executes a request , returning an ` HTTPResponse ` .
freeing any file descriptors used .
Executes a request , asynchronously returning an ` HTTPResponse ` .
Configures the ` AsyncHTTPClient ` subclass to use .
Cleanup unused transports.
Double confirmation for transport close.
taking into account
create new connection .
till found one that is not finsihed and
Close all ongoing DNS calls.
specified host/port or clear all dns local cache .
Create connection.
get the correct SSL context
parsed JSON data .
Return the next frame from the socket .
Send a frame over the websocket with message as its payload .
Send pong message.
Send ping message.
sending the specified code and message .
filtered by their attributes .
domain matching adhering to RFC 6265 .
path matching adhering to RFC 6265 .
string parsing adhering to RFC 6265 .
is used with StreamParser for incremental protocol parsing .
Sets ``Content-Disposition`` header.
Clone itself with replacement some attributes.
containing all parsed Forwarded header ( s ) .
Hostname of the request.
initiated HTTP request .
return a datetime object
The value of If-Modified-Since HTTP header, or None.
The value of If-Unmodified-Since HTTP header, or None.
The value of If-Range HTTP header, or None.
Return request cookies.
The content of Range HTTP header.
be read , False otherwise .
Read request body if present.
using encoding from .charset .
Return BODY as JSON.
Return POST parameters.
is about to exit , we need cleanup everything and
Set keep-alive connection mode.
accepting new pipelinig messages and close
Force close connection
incoming request .
Handle errors.
Run an app locally
yields chunks of size n .
reading some data from stream , inserting it to buffer head .
Returns a tuple of (data, end_of_http_chunk).
is n == -1
registered receivers .
Translate log_format into form usable by modulo formatting
produces a middleware that normalizes
Encode a list of fields using the multipart/form-data MIME format
Writes chunk of data to a stream .
Write request/response status and headers.
load the netrc file from the path specified by the env-var
Parses a MIME type into its components .
Create a BasicAuth object from an Authorization HTTP header .
Create BasicAuth from url.
The value of content part for Content-Type HTTP header.
The value of charset part for Content-Type HTTP header.
The value of Content-Length HTTP header.
sends a request .
Perform HTTP request.
Initiate websocket connection.
transform it to CIMultiDict
Perform HTTP GET request.
Perform HTTP OPTIONS request.
Perform HTTP HEAD request.
Perform HTTP POST request.
Perform HTTP PUT request.
Perform HTTP PATCH request.
Perform HTTP DELETE request.
Close underlying connector.
Do URL requoting on redirection handling .
Return IP address for given hostname
Emits next multipart reader object.
Reads body part data.
Reads body part content chunk of the specified size.
Reads body part by line by line.
reads all the data to the void .
assumes that body part contains text data .
assumes that body parts contains JSON data .
assumes that body parts contains form
according the specified Content-Encoding
charset parameter from Content-Type header or default .
specified in Content-Disposition header or None
reader instance from HTTP response .
Emits the next multipart body part.
till the final boundary .
Dispatches the response by the ` Content-Type ` header , returning
is read completely .
Wrap boundary parameter value in quotes, if necessary.
Adds a new body part to multipart writer .
append JSON part .
append form urlencoded part .
Size of the payload.
Update destination host, port and connection type (ssl).
Convert request version to two elements tuple.
Update request headers.
Update request cookies header.
Set request content encoding.
Analyze transfer-encoding header.
Set basic auth.
bytes objects .
Start response processing.
Read response payload .
Read response payload and decode .
Read and decodes JSON response.
chunked transfer encoding .
Enables response compression encoding.
Set or update response cookie.
Delete cookie.
The value of Last-Modified HTTP header, or None.
Default handler for Expect header.
Construct url for route with additional params.
Add static files view.
Shortcut for add_route with method HEAD
Shortcut for add_route with method OPTIONS
is true another
Shortcut for add_route with method POST
Shortcut for add_route with method PUT
Shortcut for add_route with method PATCH
Shortcut for add_route with method DELETE
Shortcut for add_route with ANY methods for a class-based view
routes to route table .
Parses RFC 5322 headers from a stream.
extra info from connection transport
requested one of a specific
Bytes representation of the HTML content.
Unicode representation of the HTML content
encoding string to be used , extracted from the HTML and
`PyQuery <https://pythonhosted.org/pyquery/>`_ representation
`lxml <http://lxml.de>`_ representation of the
Given a CSS Selector , returns a list of
Given an XPath selector , returns a list of
Search the :class:`Element <Element>` (multiple times) for the given parse
found links on page , in asis form .
given link absolute .
found links on page , in absolute form
Supports the `` < base > `` tag
Returns a dictionary of the attributes of the :class:`Element <Element>`
find the next page , if there is one .
Handle page creation and js rendering.
Reloads the response in Chromium , and replaces HTML content
replace it by a HTMLResponse .
was created close it first .
run it in a thread .
want to run , it will wrap each one
img_tensor: N, C, H, W
[ tgt_sequence_length , src_sequence_length , batch_sizse ]
[ tgt_sequence_length , src_sequence_length , batch_size ]
Get shape of variable.
Get variable by name.
Dropout except test.
Calculate time span.
return 18000x128x128 np array
Distributed Synchronous SGD Example
valid the topology
is legal for layers
Mutation for a graph
Main function of SMAC for CLI interface
is urgly , we put all the initialization work in this method , because initialization relies
receive_trial_result
Convert the values of type `loguniform` back to their initial range
generate one instance of hyperparameters
generate mutiple instances of hyperparameters
Computes gradient of the Lovasz extension w.r.t sorted errors
IoU for foreground class
ignored ) class
Binary Lovasz hinge loss
Flattens predictions in the batch (binary case)
Binary Cross entropy loss
Multi-class Lovasz-Softmax loss
Flattens predictions in the batch
Cross entropy loss
nanmean compatible with generators.
main loop logic for trial keeper
Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]
embedding for a specific file by given file path .
Generate json by prediction.
Calculate the f1 score .
Evalutate with predictions/
Send command to Training Service.
Receive a command from Training Service .
search space in hyperopt .
Change json to parameters.
change parameters in NNI format to parameters in hyperopt format(This function also support nested dict.
Delete index infromation from params
Update search space definition in tuner by search_space in parameters.
Returns a set of trial (hyper-)parameters, as a serializable object.
Record an observation of the objective function
Unpack the idxs-vals format into the list of dictionaries that is
get suggestion from hyperopt
Import additional data for tuning
"Lowest Mu" acquisition function
Calculate the lowest mu
embedding network for the QA model .
received from nni_manager , which contains :
has three keys : trial_job_id , event , hyper_params
Call tuner to process final results
Call assessor to process intermediate results
tuner in case the
parse reveive msgs to global variable
eval the model
Run on end of each epoch
Create a full id for a specific bracket 's hyperparameter configuration
Randomly generate values for hyperparameters from hyperparameter space i.e., x.
return the values of n and r for the next round
means the ith round .
update trial's latest result with its sequence number, e.g., epoch number or batch number
is finished and the corresponding round ( i.e. , i ) has all its trials finished ,
Randomly generate num hyperparameter configurations from search space
generating one round of hyperconfigs , this function records the generated hyperconfigs ,
get one trial job , i.e. , one hyperparameter configuration .
is search space
Returns a set of trial graph config, as a serializable object.
Generates a CNN .
Generates a Multi-Layer Perceptron .
Generate search space from Python source code.
Expand annotations in user code.
Generate send stdout url
Generate send error url
validate if a digit is valid
supports importing data
load search space content
update experiment profile
import additional data to the experiment
import data to the experiment
check key type
check number range
keras dropout layer.
real keras layer.
judge the layer type.
get layer description .
build layer from description .
get layer width .
Build the GRU cell .
Build GRU sequence.
returns a 2d convolution layer with full stride .
downsamples a feature map by 2X .
build mnist network , run and send result to NNI .
Build the whole neural network for the QA model.
read content from a file
install python package from pip
Get parameters from command line
Building network for mnist
get the startTime and endTime of an experiment
get the status of an experiment
Update the experiment status in config file
check if the id is valid
Parse the arguments for nnictl stop
get the file name of config file
Convert time stamp to date time format
check if restful server is running
Stop the experiment which is running
Get experiment information
Show the status of experiment
internal function to call get_log_content
get trial log path
show the url of web ui
get the information of all experiments
get the interval of two times
show experiment information in monitor
monitor the experiment
export experiment metadata to csv
remote directory to local machine
create ssh client
search space from json format to hyperopt format
Json to pramaters.
Delete index information from params
Update search space.
Returns a dict of trial (hyper-)parameters, as a serializable object.
Record the result from a trial
Load json file content
Generate the Parameter Configuration Space ( PCS ) which defines the
Generate the scenario .; is used to configure SMAC and
Load or create dataset
The distance between two layers.
The attribute distance.
The distance between the layers of two neural networks.
The distance between two skip-connections.
The distance between the skip-connections of two neural networks.
The distance between two neural networks.
Calculate the edit distance .
The Euclidean distance between two vectors.
embed the neural architectures based on their edit-distance .
Check if the target descriptor is in the descriptors .
Fit the regressor with more data.
fit the regressor .
Fit the regressor for the first time.
Predict the result.
Generate new architecture.
estimate the value of generated graph
add child to search tree itself .
return the content of the tree in a dict .
Train a network from a specific graph .
Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.
is determined by the properties of itself , and the ` hash_id ` s of input layers
update hash id of each layer, in topological order/recursively
Initialize root logger.
Create simple convolutional model
Load MNIST dataset
get all of config values
set { key : value } paris to self.config
save config to local file
set { key : value } paris to self.experiment
remove an experiment by id
load config from local file
load data from file
Build the vocab from corpus .
Shuffle the step
Get batches data and shuffle.
Get char input.
Get word input.
Given word return word index .
Get answer's index of begin and end.
Get bucket by length.
tokenize function in Tokenizer .
generate new id and event hook for new Individual
initialize populations for evolution tuner
update data
trial_end
assess_trial
Copy directory from HDFS to local
Copy file from HDFS to local
Copy directory from local to HDFS
Copy a local file to HDFS directory
use boston dataset
according to parameters
Train model and predict result
Add a skip-connection to the descriptor .
NetworkDescriptor to json representation
Add a layer to the Graph .
Add a new node to node_list and give the node an ID .
be created in advance .; Add a new layer to the graph .
Redirect the layer to a new node .
Replace the layer with a new layer.
Return the topological order of the node IDs from the input node to the output node .
Given two node IDs , return all the pooling layers between them .
Search for all the layers and nodes down the path.
be widened caused by an operation .
Insert a relu-conv-bn block after the target block .
Widen the last dimension of the output of the pre_layer.
Insert the new_layers after the node with start_node_id.
Add a weighted add skip-connection from after start node to end node .
Add a weighted add concatenate connection from after start node to end node .
Extract the the description of the Graph as an instance of NetworkDescriptor .
clear weights of the graph
Return a list of layer IDs in the main chain .
Returns the main chain node ID list .
Run the tuner .
Process commands in command queues.
Enqueue command into command queues
process a command .
match their corresponding type
Random generate variable value within their bounds
skip connection graph
create new layer for the graph
is legal or not .
core transform function for graph.
represent an lower bound
mu: float or array_like of floats
Predict by Gaussian Process Model
Call rest get method
Call rest post method
Call rest put method
Call rest delete method
update the best performance of completed trial job
be early stop by curve fitting algorithm
Returns a set of trial neural architecture, as a serializable object.
Record an observation of the objective function.
Call the generators to generate the initial architectures for the search .
Generate the next neural architecture .
Update the controller with evaluation result of a neural architecture .
Add model to the history, x_queue and y_queue
Get the best model_id from history using the metric value
Get the model by model_id
sample some init seed within bounds .
Update the self.x_bounds and self.x_types by the search_space.json
Pack the output
Generate next parameter for trial
receive result from trial .
Trains GP regression model
generate all possible configs for hyperparameters from hyperparameter space .
parse type of quniform or qloguniform
Enumerate all possible combinations of all parameters
Log message into stdout
Write buffer data into logger/stdout
Run the thread , logging everything .
Extract scalar reward from trial result.
tuple to solve unhashable problem .
Initialize dispatcher logging configuration
opted for a single multidimensional KDE compared to the
sample a new configuration
has finished , this function should be called; register finished runs .
Check the search space is valid : only contains 'choice ' type
layer normalization .
multihead attention .
Return positinal embedding.
Point-wise feed forward net.
Generate search space.
Check if restful server is ready
Check if restful server is ready , only check once
Vapor pressure model
logx linear
dr hill zero background
logistic power
pow4
Morgan-Mercer-Flodin
exp4
Weibull model
http://www.pisces-conservation.com/growthhelp/janoschek.htm
need to follow and input
generate stdout and stderr log path
print log information
Find nni lib from the following locations in order
Run nni manager process
set trial configuration
set local configuration
Call setClusterMetadata to pass trial
set kubeflow configuration
Call startExperiment ( rest POST /experiment ) with yaml file content
start rest server and start experiment
resume an experiment
start a new experiment
fit all default curves parameter seperately
filter the poor performing curve
return the predict y of 'model ' when epoch = pos
return the value of the f_comb when epoch = pos
normalize weight
given the weight 's sample
given the weight 's sample and target position
likelihood
priori distribution
posterior probability
using mcmc sampling .
predict the value of target position
Detect the outlier
detect the outlier
deeper conv layer.
deeper dense layer.
wider previous dense layer.
wider previous conv layer.
wider next conv layer.
wider batch norm layer.
wider next dense layer.
add noise to the layer .
initilize dense layer weight .
initilize conv layer weight .
initilize batch norm layer weight .
parse log path
copy data from remote machine to local machien
get path list according to different platform
start tensorboard process in local machine
is smaller the better
Select the lowest mu value
Minimize constraints fun summation
use 20newsgroups dataset
using Bayesian optimization
including creating Bayesian optimization-based parametric models
generate a new bracket
recerive the number of request and generate trials
change json format to ConfigSpace format dict<dict> -> configspace
receive the information of trial end and generate next configuaration .
reveice the metric data and update Bayesian optimization with final result
data_transforms for cifar10 dataset
data_transforms for mnist dataset
Compute the mean and std value of dataset .
EarlyStopping step on each epoch
have false positives .
is that we try to move towards upperbound , by randomly choose one
user home directory
Change relative path to absolute path
Change the time to seconds
Parse path in config file
Validate searchspace content,
Validate whether the kubeflow operators are valid
Validate whether the common values in experiment_config is valid
check whether the file of customized tuner/assessor/advisor exists
Validate whether assessor in experiment_config is valid
Valid whether useAnnotation and searchSpacePath is coexist
validate the trial config in pai platform
Validate whether experiment_config is valid
get urls of local machine
Parse an annotation string.
Parse an annotation function.
Parse `nni.variable` expression.
Parse `nni.function_choice` expression.
args to a dict such that every key and value in the dict is the same as the value of the arg .
lambda expression node .
Replace a node annotated by ` nni.variable ` .
Replace a node annotated by ` nni.function_choice ` .
Annotate user code.
Load yaml file content
Detect if the port is used
Create the Gaussian Mixture Model
Selecte R value
Reports intermediate result to Assessor.
Reports final result to tuner.
get args from command line
build model from json representation
train model on each epoch in trainset
Freeze BatchNorm layers.


 90 
 api 


ricequant
  cookies
 joinquant 

str to dict
ID
 tesseract 
, , 30




worker
 user 
 cookies




(20)

, weight 



joinquant





Read a Caffe LMDB file where each value contains a `` caffe.Datum `` protobuf .
Memory information in bytes
was utilized .
Get number of devices
Get a specific GPU device
Copied from tensorflow example; extract the tarball from Alex 's website .
Build a tf.placeholder from the metadata in the given tensor spec , or return an existing one .
Check that op is in the subgraph induced by the dependencies of targets .
Check that op is in the subgraph induced by the dependencies of fetches .
Summarize a tensor by different methods .
Call :func:`add_tensor_summary` under a reused 'activation-summary' name scope.
matching the regex , under a
Summarize the moving average for scalar tensors .
Visualize some intermediate results ( proposals , raw predictions ) inside the pipeline .
build the trainer 's tower function under `` TowerContext ( is_training=False ) `` ,
trained model to use it in TensorFlow Serving or cloudML .
trained model to use it as a frozen and pruned inference graph in
Run inference from a training model checkpoint.
receives encoded images buffers .
Run the pruned and frozen inference graph .
Mostly equivalent to `tf.layers.batch_normalization`, but difference in
Select / reorder components from datapoints.
Gather useful debug information from a datapoint.
apply gradient processors .
using the TF callable .
Running multiple ` predict_dataflow ` in multiple threads , and aggregate the results .
Flatten the tensor except the first dimension.
A wrapper around `tf.layers.Dense`.
Call _init_runtime under different CUDA_VISIBLE_DEVICES , you 'll
Fetch a batch of data without waiting
Same as in :meth:`AsyncPredictorBase.put_task`.
Almost equivalent to `tf.layers.batch_normalization`, but different (and more powerful)
described in the paper :
generated from z
return a ( b , 1 ) logits
Compute pairwise intersection areas between boxes.
pairwise intersection-over-union between box collections .
Maxout as in the paper `Maxout Networks <http://arxiv.org/abs/1302.4389>`_.
Parameterized ReLU as in the paper ` Delving Deep into Rectifiers : Surpassing
A shorthand of BatchNormalization + ReLU.
reproduces the paper can be found at https : //github.com/ppwwyyxx/GroupNorm-reproduce/ .
Extract the images into a 4D uint8 numpy array [ index , y , x , depth ] .
Extract the labels into a 1D uint8 numpy array [ index ] .
is not available , create a dummy class which throws ImportError when used .
is not available , create a dummy function which throws ImportError when used .
Log deprecation warning.
call dataflow.__iter__ ( ) again and fill into the queue .
Create a hook-only callback which maintain EMA of the queue size .
shapes except for the batch dimension
Wrap a dataflow to tf.data.Dataset .
pairwise intersection-over-area between box collections .
's already here .
Return the directory structure of `` dir '' .
abosolute file name .
used by detection .
merges several instance files together .
Surround a context with a timer.
add the time spent inside to TotalTimer .
Print the content of the TotalTimer , if it 's not empty .
reset state of each augmentor
terminate when main process exit .
Set the `` death signal '' of the current process , so that
Start process(es) with SIGINT ignored.
Execute a command with timeout , and return STDOUT and STDERR
Put obj to queue , but will give up when the thread is stopped
Take obj from queue , but will give up when the thread is stopped
Visualize use weights in convolution filters.
Visualize activations for convolution layers.
Make the static shape of a tensor less specific .
~= -E_ { x \sim P ( x|s ) } [ \log Q ( x|s ) ] , where x are samples , and Q is parameterized by vec .
OpenAI official code actually models the "uniform" latent code as
Mutual information between x (i.e.
see `` Dynamic Filter Networks '' ( NIPS 2016 )
Estimate filters for convolution layers
Implements a steerable Gaussian filter.
Assign `self.g_vars` to the parameters under scope `g_scope`,
set ` self.g_loss ` and ` self.d_loss ` .
need to set tower_func because it 's a TowerTrainer ,
applying this decorator :
Apply a regularizer on trainable variables matching the regex , and print
Get the cost from the regularizers in `` tf.GraphKeys.REGULARIZATION_LOSSES `` .
Same as `tf.layers.dropout`.
Return a proper background image of background_shape , given img .
Apply a function on the wrapped tensor .
Produce a saliency map as described in the paper :
A wrapper around `tf.layers.Conv2D`.
A wrapper around `tf.layers.Conv2DTranspose`.
setup the assign operator for that variable .
Using schedule , compute the value to be set at a given point .
build the model which takes the input variables
Convert a caffe parameter name to a tensorflow parameter name as
map the output of any variable getter .
Return a context to freeze variables ,
be able to `` import caffe `` to use this; Load a caffe model .
Get caffe protobuf.
populate some configs from others
Convert to a nested dict.
Update from command line args.
Get a corresponding model loader by looking at the file name .
return a set of strings
Decorator for function to support argscope
given module to support argscope .
casting , clipping )
img: an RGB image of shape (s, 2s, 3).
be easier to use than : meth : ` tf.Print ` .
`Peek Signal to Noise Ratio <https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio>`_.
Pad tensor in H, W
Correlation Cost Volume computation.
Resize input tensor with unkown input-shape by a factor
Architecture in Table 4 of FlowNet 2.0.
Architecture in Table 3 of FlowNet 2.0.
Architecture of FlowNetSimple in Figure 2 of FlowNet 1.0.
Architecture of FlowNetCorr in Figure 2 of FlowNet 1.0.
Will not modify img
Draw top3 proposals for each gt.
Overlay a mask on top of the image .
send data to a ZMQ socket addr .
Convert a DataFlow to a : class : ` multiprocessing.Queue ` .
Aligned version of tf.image.crop_and_resize , following our definition of floating point boxes .
Slice anchors to the spatial size of this featuremap.
[ 0,255 ]
be verified by :; is shape * 0.5-0.5 .
Get largest rectangle after rotation.
Apply a mapping on certain argument before calling the original function .
memoized , but keep one cache per default graph .
performs memoization ignoring the arguments used to call
Ensure a 2D shape.
use with 4D symbolic functions .
Decorate a method or property of a class , so that this method can only
performs memoization on methods .
reuses the current variable scope if the
Return a context which either opens and caches a new name scope ,
return the results .
Reduce the gradients , apply them with the optimizer ,
Call the function ` tower_fn ` under : class : ` TowerContext ` for each tower .
Copy values of variables on GPU 0 to other GPUs.
given in seconds
Get a good RNG seeded with time , pid and the object .
called in the code to this function is guaranteed to return True the
be used with tqdm .
Similar to `from ctypes.util import find_library`, but try
Note:
Return the three quantization functions fw , fa , fg , for weights , activations and gradients respectively
Implemented Trained Ternary Quantization :
produce visualizations like the following :
Similar to :func:`stack_patches` but with a generator interface.
DataFlow ` .
Convert a 1-channel matrix of intensities to an RGB image employing a colormap .
Draw text on an image.
cooperate with : class : ` LinearWrap ` .
Convert polygons to binary masks.
is equal to pool_size .
Global average pooling as in the paper `Network In Network
perform kronecker product with .
variables to a dict , and save as
Save variables in dic to path.
Work around TF problems in checkpoint path handling.
Load all variables from a checkpoint to a dict.
is only used in training .
Returns a relaxed (possibly reshaped/upcast-ed) version of value,
Put a ` tf.Summary ` .
Put a scalar .
Put an image .
Put an : class : ` tf.Event ` .
existing json under : meth : ` logger.get_logger_dir ( ) ` named `` stats.json '' ,
Add stats to json and dump to disk.
Sample the images using the given coordinates , by bilinear interpolation .
Enable trace for calls to any function.
Apply a set of default rules to make a fast : class : ` InputSource ` .
TrainConfig ` and a : class : ` Trainer ` , to
Delegate property to self.loop
given the settings .
Register callbacks to the trainer.
do in one iteration .
be called after the main graph is built .
Create the session and set ` self.sess ` .
hook it onto ` self.sess ` to create ` self.hooked_sess ` .
Implemented by three lines :
Same as :meth:`train()`, except:
Return a tf.ConfigProto to use as default session config .
Get a list of tensors in the default graph by a list of names .
Get either tf.Operation of tf.Tensor from names.
ops to enqueue on all worker queues .
replace variables in avg_grads
add to the list of `` shadow_vars `` .
averaged gradients to ps vars , and then copy the updated
Get the op to copy-initialized all local variables from PS .
Get the op to sync local model_variables to PS .
have some : class : ` InputSource ` which does n't match the inputs of
Sample RPN proposals by the following steps:
is enabled by default .
return a tuple of ( s , r , a , o ) ,
Run the environment for one step .
Get the recent state ( with stacked history ) of the environment .
Execute one step in any of the runners.
log the time of some heavy callbacks
containing metadata about the current tower .
Get a tensor in this tower .
Get a variable used in this tower .
See : meth : ` BaseTowerContext.get_collection_in_tower ` .
make a dir recursively , but do nothing if the dir exists
Download URL to a directory.
Yields:
Get the path to some dataset under `` $ TENSORPACK_DATASET `` .
Restore from a collection backup.
Get items from this collection that are added in the current tower .
Iterate on the raw PTB data.
Set the directory for global logging .
set log directory to
The class-balanced cross entropy loss,
Deterministic bilinearly-upsample the input images.
forked dataflows should only be reset * * once and only once * * in spawned processes .
True if spec_or_tensor is compatible with this TensorSpec .
Print a description of the current model parameters .
used by layer registry , to print shapes of inputs/outputs of layers .
r"""Loss for Siamese networks (cosine version).
described in the paper
given tensors into an nfeatures-dim space .
Get all anchors in the largest possible image , shifted , floatbox
Label each anchor as fg/bg/ignore.
consists of the following :; Return a training dataflow .
are broadcasted to all devices .
Hierarchical allreduce for DGX-1 system.
Average the gradients.
level 2~5 .
Instance Normalization, as in the paper:
Add summaries for RPN proposals.
Sample some boxes from all proposals for training.
Generate final results from predictions of all proposals.
fg x ?
Returns: N x #class x 4
summed to one for each box .
given by some client .
sent from some client .
Create a self-contained inference-only graph and write final graph ( in pb format ) to disk .
Converts a checkpoint and graph to a servable for TensorFlow Serving.
Use a Ray task to read a chunk of SQL source .
Creates a decorator which overwrites a decorated class ' __doc__
logs the time usage of a code block
based on environment variables and internal defaults .
func to the object .
Convert categorical variable into indicator variables.
func to the object in the plasma store .
Shuffle the order of the data in this axis based on the ` lengths ` .
Deploy a function along a full axis in Ray.
Deploy a function along a full axis between two data sets in Ray.
Query columns of the DataManager with a boolean expression.
Converts Modin DataFrame to Pandas DataFrame.
Shuffle the order of the data in this axis based on the ` func ` .
Deploy a function to a partition in Ray.
Gets the object out of the plasma store .
Apply a function to the object stored in this partition .
stored in this partition to a Pandas DataFrame .
Put an object in the Plasma store and wrap it in this object .
missing values for an array-like object .
are merged .
Check if is possible distribute a query given that args
given sql arg is query or table
Extract all useful infos from the given table
columns names and python typos from metadata
Check query sanity
columns names and python typos from query
Check partition_column existence and type
Return a columns name list and the query string
Put bounders in the query
Computes the index after a number of rows have been removed .
given various metadata .
Returns the numeric columns of the Manager .
clean dataframe and pick numeric indices .
Joins a pair of index objects ( columns or rows ) by a given strategy .
Joins a list or two objects together .
Concatenates two objects together.
Copartition two QueryCompiler objects.
Improve simple Pandas DataFrame to an advanced and superior Modin DataFrame.
Inter-data operations (e.g.
Helper method for inter-manager and scalar operations.
Perform an operation between two objects .
Uses other manager to update corresponding values in this manager .
is true else from other .
mapping scalar operations across a Manager .
Fits a new index for this Manger.
sets a default level_0 index .
Transposes this DataManager.
reduce the data to a Pandas Series .
Counts the number of non-NaN objects for each column or row .
Returns the mean for each numerical column or row .
Returns the minimum from each column or row .
Calculates the sum or product of the DataFrame .
Returns the product of each numerical column or row .
are true , potentially over an axis .
columns dtypes to given dtypes .
map that reduce Manager to series but require knowledge of full axis .
Returns index of first non-NaN/NULL value.
Returns the first occurrence of the maximum over requested axis.
Returns the first occurrence of the minimum over requested axis.
Returns index of last non-NaN/NULL value.
Returns median of each column or row.
Returns the memory usage of each column .
Returns quantile of each column or row.
using function that needs full axis .
Generates descriptive statistics.
dropped along given axis .
evaluated on columns .
calculated for each label along given axis .
Replaces NaN values with the method provided.
are set to the average .
Sorts the data with respect to either the columns or the indices .
select indices along full axis .
containing quantiles along an axis for numeric columns .
Returns the last n rows .
Returns the first n columns.
Get column data for target labels.
Get row data for target labels.
Set the column defined by ` key ` to the ` value ` provided .
Remove row data for target index and columns.
Insert new column data.
Recompute the index after applying function .
applies all manual partitioning functions .
dummy variables for certain columns .
involves making copies of the index in memory .
Perform the map step
Gets the lengths of the blocks .
Gets the widths of the blocks .
Updates the current DataFrame inplace .
check validity of other in inter-df operations
use default pandas function
Apply an absolute value function to all numeric columns.
Add this DataFrame to another or a scalar/list .
are True over requested axis
Apply a function along input axis of DataFrame .
Synonym for DataFrame.fillna(method='bfill')
Creates a shallow copy of the DataFrame .
Get the count of non-null objects in the DataFrame .
Perform a cumulative maximum across the DataFrame .
Perform a cumulative product across the DataFrame .
descriptive statistics that summarize the central tendency ,
Finds the difference between elements on the axis requested
Return new object with labels in requested axis removed.
Create a new DataFrame from the removed NA values from this one .
removed , optionally only considering certain columns
element-wise that this is equal to other .
rows or columns based on their labels
Divides this DataFrame against another DataFrame/Series/scalar.
element-wise that this is greater than or equal to other .
Get the counts of dtypes in this object .
Get the counts of ftypes in this object .
element-wise that this is greater than other .
Get the first n rows of the DataFrame .
Get the index of the first occurrence of the max value of the axis .
contained in values .
element-wise that this is less than or equal to other .
element-wise that this is less than other .
mean across the DataFrame .
Computes median across the DataFrame.
Returns the memory usage of each column in bytes
Perform min across the DataFrame.
Mods this DataFrame against another DataFrame/Series/scalar.
Perform mode across the DataFrame.
Multiplies this DataFrame against another DataFrame/Series/scalar.
element-wise that this is not equal to other .
Return Series with number of distinct
Pow this DataFrame against another DataFrame/Series/scalar.
Return the product of the values for the requested axis
given quantile over requested axis ,
Reset this index to default and create column from current index .
Mod this DataFrame against another DataFrame/Series/scalar.
Round each element in the DataFrame.
Subtract a DataFrame/Series/scalar from this DataFrame .
Div this DataFrame against another DataFrame/Series/scalar.
Returns a random sample of items from an axis of object.
desired index to given axis .
Sort a DataFrame by one of the indices (columns or index).
Sorts by a column/row or list of columns/rows.
Computes variance across the DataFrame.
Get the number of elements in the DataFrame .
returns the data .
Apply some callable function to the data in this partition .
Add the function to the apply function call stack.
Use a Ray task to read a chunk of a CSV into a pyarrow Table .
Computes the number of rows and/or columns to include in each partition .
get a block of NaNs .
based on the provided number of splits .
Unpack the user input for getitem and setitem and compute ndim
enlarge the global index .
Compute the ndim of result from locators
broadcast or reshape item .
remote write and replace blocks .
is one ) .
Helper for _enlarge_axis, compute common labels and extra labels.
Splits the DataFrame read into smaller DataFrames and handles all edge cases .
Use a Ray task to read columns from Parquet into a Pandas DataFrame .
Use a Ray task to read a chunk of a CSV into a Pandas DataFrame .
Use a Ray task to read columns from HDF5 into a Pandas DataFrame .
Use a Ray task to read columns from Feather into a Pandas DataFrame .
Get the index from the indices returned by the workers .
Constructs a DataFrame from a CSV file .
Read csv file from local disk.
Load a h5 file from the file path or buffer , returning a DataFrame .
Read a pandas.DataFrame from Feather format .
Reads a SQL query or database table into a DataFrame .
datetime format .
Applies `map_func` to every partition.
Copartition two BlockPartitions objects.
Take the first ( or last ) n rows or columns from the blocks
Concatenate the blocks with another set of blocks .
Convert this object into a Pandas DataFrame from the partitions.
gets the internal indices stored in the partitions .
Convert a global index to a block index and local index .
Convert indices to a dict of block index to internal index mapping.
Applies a function to a list of remote partitions.
select indices .
Applies a function to a select subset of full columns/rows.
Apply a function to along both axis
Apply a function that requires two BaseFrameManager objects .
based on the ` shuffle_func ` .
Creates a parser function from the given sep .
Make a feature mask of categorical features in X .
selected features and other features
Apply a transform function to portion of selected features .
encode for NaNs and infinities in the data .
contains only categorical features .
X , then transform X .
using one-hot encoding .
learning pipeline .
Setup Memory object for memory caching.
update the _optimized_pipeline field .
Print out best pipeline at the end of optimization process.
Use the optimized pipeline to predict the target for a feature set .
Call fit and predict in sequence .
Return the score on the given testing data using the user-specified scoring function .
Use the optimized pipeline to estimate the class probabilities for a feature set .
Provide a string of the individual without the parameter prefixes .
has passed , save a new optimized pipeline .; used in the per generation hook in the optimization loop .
Export the optimized pipeline as Python code.
missing values in a feature set .
Check if a dataset has a valid feature set and labels .
Compile a DEAP pipeline into a sklearn pipeline.
iterate through all objects in the pipeline and set a given parameter .
have elapsed .
Combine the stats with operator count and cv score and preprare to be written to _evaluated_individuals
Determine the fit of the provided individuals .
Preprocess DEAP individuals before pipeline evaluation.
Update self.evaluated_individuals_ and error message during pipeline evaluation.
Update self._pbar and error message during pipeline evaluation.
Perform a replacement , insertion , or shrink mutation on an individual .
have a different depth between min_ and max_ .
Count the number of pipeline operators as a measure of pipeline complexity.
Update values in the list of result scores and self._pbar during pipeline evaluation.
Generate a Tree as a list of lists .
transform them using OneHotEncoder .
transform them using PCA .
Fit the StackingEstimator meta-transformer.
adding two synthetic feature ( s ) .
scoring function : balanced accuracy .
adding two virtual features .
Decode operator source and import operator class.
iterates through all objects in the pipeline and sets sample weight .
create operator class .
is a positive integer .
is a float integer in the range [ 0. , 1 .
is called when TPOT is run on the command line .
converts mymodule.myfunc in the myfunc
Perform a TPOT run .
Fit FeatureSetSelector for feature selection
Make subset after fit
Get the boolean mask indicating which features are selected
do crossover , that is , they share a primitive .
Picks a random individual from the population, and performs mutation on a copy of it.
applying only the variation part
Initializes the stats dict for individual
is the : math : ` ( \mu + \lambda ) ` evolutionary algorithm .
Randomly select in each individual and exchange each subtree with the
chosen primitive from * individual * by a randomly
given dataset split .
Return operator class instance by name.
Generate source code for a TPOT Pipeline.
Convert the unstructured DEAP pipeline into a tree data-structure.
Generate all library import calls for use in TPOT.export().
Generate code specific to the construction of the sklearn Pipeline.
Generate code specific to the construction of the sklearn Pipeline for export_pipeline.
Indent a multiline string by some number of spaces.
Get the next value in the page .
do n't use any reserved parameter .
Get the next page in the iterator .
Getter for query parameters for the next request.
Requests the next page from the path provided .
are more pages with results .
Main comparison function for all Firestore types.
performs image detection and annotation for a batch of files .
Run asynchronous image detection and annotation for a list of images.
Run asynchronous image detection and annotation for a list of generic
Called by IPython when this module is loaded as an IPython extension .
Create a : class : ` GoogleAPICallError ` from an HTTP status code .
Create a : class : ` GoogleAPICallError ` from a : class : ` requests.Response ` .
Create a : class : ` GoogleAPICallError ` from a : class : ` grpc.StatusCode ` .
Create a : class : ` GoogleAPICallError ` from a : class : ` grpc.RpcError ` .
Make a request over the Http transport to the Cloud Datastore API .
Make a protobuf RPC request .
Construct the URL for a particular API call .
Perform a `` lookup `` request .
Perform a `` runQuery `` request .
Perform a `` beginTransaction `` request .
Perform a `` commit `` request .
Perform a `` rollback `` request .
Perform an `` allocateIds `` request .
Creates a request to read rows in a table .
Creates a request to mutate rows in a table .
belongs to a table .
used in requests .
create a row associated with this table .
Creates this table.
Check whether the table exists .
Delete this table.
List the column families owned by this table .
List the cluster states owned by this table .
Read a single row from this table .
Read rows from this table.
Mutates multiple rows in bulk.
Read a sample of row keys in the table .
Truncate the table
create a mutation batcher associated with this instance .
are eligible for retry .
Periodically send heartbeats.
Report an individual error event.
Convert a scalar value into a query parameter .
Converts a dictionary of parameter values into query parameters.
Converts DB-API parameter values into query parameters.
using a JSON/HTTP client .
using a HTTP/JSON client .
using a gRPC client .
Create an operation future from a gapic client.
google.protobuf.Message: the current operation metadata.
Refresh the operation and update the result if needed .
was cancelled .
Remove a role from the entity .
predefined is in list of predefined json values
Build an _ACLEntity object from a dictionary of data.
Gets an entity object from the ACL.
Add an entity to the ACL.
creating an Entity .
Reload the ACL data from Cloud Storage.
save ` and : meth : ` save_predefined ` .
Save this ACL for the current bucket .
Save this ACL for the current bucket using a predefined ACL .
Return a fully-qualified incident string .
Return a fully-qualified annotation string .
Return a fully-qualified artifact string .
Return a fully-qualified role_assignment string .
Return a fully-qualified subscription string .
Return a fully-qualified tag string .
Return a fully-qualified signal string .
Creates an annotation on an existing incident .
Escalates an incident.
Sends a summary of the shift for oncall handoff .
bigtable.admin role memebers
bigtable.reader role memebers
bigtable.user role memebers
bigtable.viewer role memebers
create a policy from a protobuf message .
Render a protobuf message .
create a policy from a JSON resource .
Render a JSON policy resource .
used to define database schema .
Creates an instance of this class from a protobuf .
Helper for session-related API calls.
Create this database within its instance
Test whether this database exists.
Reload this database.
Update DDL for this database.
Drop this database.
Execute a partitionable DML statement .
wraps a batch read / query .
Perform a unit of work in a transaction , retrying on abort .
Reconstruct an instance from a mapping.
Return state as a dictionary.
Create session as needed.
Create snapshot if needed.
Start a partitioned batch read operation .
partitioned read .
Start a partitioned query operation .
Process a single, partitioned query or read.
Return a fully-qualified location string .
Return a fully-qualified model string .
Return a fully-qualified model_evaluation string .
Return a fully-qualified annotation_spec string .
Return a fully-qualified table_spec string .
Return a fully-qualified column_spec string .
Creates a dataset .
Deletes a dataset and all of its contents .
Creates a model .
sleep intervals based on the exponential back-off algorithm .
Call a function and retry if it fails .
Opens the stream .
Closes the stream .
Queue a message to be sent on the stream .
recover the stream and retry on error .
Start the background thread and begin consuming the thread .
consuming the stream and shutdown the background thread .
Resumes the response stream.
Return a fully-qualified project string .
Return a fully-qualified fingerprint string .
Deletes a POSIX account .
Adds an SSH public key and returns the profile information .
returns the profile information .
Convert a protobuf GC rule to a native object .
Converts the garbage collection rule to a protobuf.
Converts the union into a single GC rule as a protobuf .
Converts the intersection into a single GC rule as a protobuf.
Converts the column family to a protobuf .
Create this column family .
Delete this column family.
Wraps a gRPC exception class, if needed.
shutdown all helper threads .
Triggered whenever the underlying RPC terminates without recovery .
Creates a watch snapshot listener for a document .
Called everytime there is a response from listen .
Assembles a new snapshot from the current set of changes and invokes
Returns the current count of all documents , including the changes from
clear the docs on RESET or filter mismatch .
given a few components , some optional .
send a request to the API .
perform the actual API request over HTTP .
Make a request over the HTTP transport to the API .
string to filter on metric or resource labels .
Copy the query and set the query time interval .
Copy the query and add filtering by group .
Copy the query and add filtering by monitored projects .
Copy the query and add filtering by resource labels .
Copy the query and add filtering by metric labels .
Copy the query and add temporal alignment .
Copy the query and add cross-series reduction .
Yield all time series objects selected by the query .
Return key-value pairs for the list_time_series API call.
given its API representation .
Update specific properties from its API representation.
API call:  create the project via a ``POST`` request.
API call:  update the project via a ``PUT`` request.
delete the project via a `` DELETE `` request .
Get the meaning from a protobuf value .
creating an entity based on a protobuf .
meaning information ( from an entity ) to a protobuf .
Converts an entity into a protobuf.
Validate rules for read options, and assign to the request.
creating a key based on a protobuf .
Given a value , return the protobuf attribute name and proper value .
Given a protobuf for a Value , get the correct value .
Assign 'val' to the correct subfield of 'value_pb'.
Convert the current object to protobuf .
given its API representation
API repr (JSON format) for entry.
Construct a DB-API time value from the given ticks value .
Return a fully-qualified registry string .
Return a fully-qualified device string .
Creates a device in a device registry .
Sets the access control policy on the specified resource .
Associates the device with the gateway .
Return a fully-qualified queue string .
Return a fully-qualified task string .
Creates a queue .
Creates a task and adds it to a queue .
yields exponential timeout values .
Formats parameters in operation in the way BigQuery expects.
Formats parameters in operation in way BigQuery expects.
Set description from schema.
Set the rowcount from query results.
execute a database operation .
Try to start fetching data , if not yet started .
Fetch multiple results from the last ``execute*()`` call.
return a gRPC channel object .
returning sink resources .
create the sink via a PUT request
API call:  test for the existence of the sink via a GET request
API call:  sync local sink configuration via a GET request
API call:  update sink configuration via a PUT request
delete a sink via a DELETE request
Helper for '_merge_by_type'.
Helper for '_merge_chunk'.
pending chunk with next value .
Merge values into rows.
Consume the next partial result set from the stream .
are no results .
Return a fully-qualified product_set string .
Return a fully-qualified product string .
Return a fully-qualified reference_image string .
returns a new ProductSet resource .
returns a new ReferenceImage resource .
imports a list of reference images to specified
parse `` LogEntry `` protobuf into a dictionary .
Convert a log entry protobuf to the native object .
Convert a sink protobuf to the native object .
Convert a metric protobuf to the native object .
Create an instance of the Logging API adapter.
Create an instance of the Metrics API adapter.
Create an instance of the Sinks API adapter.
Return a page of log entry resources .
API call:  log an entry resource via a POST request
API call:  delete all entries in a logger via a DELETE request
associated with this client .
API call:  create a sink resource.
retrieve a sink resource .
API call:  update a sink resource.
delete a sink resource .
API call:  create a metric resource.
retrieve a metric resource .
API call:  update a metric resource.
delete a metric resource .
Return a fully-qualified tenant string .
Creates a new tenant entity .
specified tenant .
Apply a list of decorators to a given function .
be applied to a wrapped method .
Wrap an RPC method with common behavior.
Pre-flight ``Bucket`` name validation.
Create a property descriptor around the : class : ` _PropertyMixin ` helpers .
update a hash with them .
Get MD5 hash of bytes (as base64).
Reload properties from Cloud Storage.
Update field of this object's properties.
Sends all changed properties in a PATCH request .
Sends all properties in a PUT request.
Start a thread to dispatch requests queued up by callbacks .
Map the callback request to the appropriate gRPC request.
given messages .
Remove the given messages from lease management .
given messages to lease management .
Modify the ack deadline for the given messages .
Explicitly deny receipt of messages.
Submits a job to a cluster .
Updates a job in a project .
Starts a job cancellation request .
Return a fully-qualified instance string .
Return a fully-qualified app_profile string .
Return a fully-qualified cluster string .
Lists information about instances in a project.
Updates an instance within a project.
updates an instance within a project .
Creates a cluster within an instance .
Updates a cluster within an instance .
Creates an app profile within an instance .
Updates an app profile within an instance.
Return a fully-qualified annotation_spec_set string .
Return a fully-qualified dataset string .
Return a fully-qualified annotated_dataset string .
Return a fully-qualified example string .
Return a fully-qualified data_item string .
Return a fully-qualified instruction string .
Exports data and annotations from dataset.
Starts a labeling task for image .; labeling task is
Starts a labeling task for video .; labeling task is
Starts a labeling task for text .
Creates an instruction for how data should be labeled .
Fully-qualified name of this variable.
Value of the variable, as bytes.
Retrieve the timestamp at which the variable was updated .
Update properties from resource in body of ``api_response``
specified message type .
protobuf Message classes in a given import module .
Resolve a potentially nested key .
Retrieve a key 's value from a protobuf Message or dictionary .
Set helper for protobuf Messages.
Set a key 's value on a protobuf Message or dictionary .
given value if the
Create a field mask by comparing two messages .
Return a fully-qualified topic string .
Return a fully-qualified snapshot string .
See the resource name rules .; Creates a subscription to a given topic .
existing subscription to a point in time or to a given snapshot ,
Completes the specified prefix with keyword suggestions .
creating session instances .
Associate the pool with a database .
Check a session out from the pool .
Delete all sessions in the pool.
Return a session to the pool .
Refresh maybe-expired sessions in the pool.
Begin all transactions for sessions added to the pool .
Attach a logging handler to the Python root logger
log the specified logging record .
Helper:  construct concrete query parameter from JSON resource.
Factory: construct parameter from JSON resource.
Construct JSON API representation for the parameter.
expected by fluentd .
Analyzes the sentiment of the provided text.
Call `` Commit `` on the GAPIC client with retry / sleep .
produce a new sleep time .
Write `` protobufs to this transaction .
Begin the transaction .
Roll back the transaction.
commit the changes accumulated .
call the wrapped callable .
Try to commit the transaction .
Return a fully-qualified profile string .
returns a new profile .
Gets the specified profile .
Updates the specified profile and returns the updated result .
Searches for profiles within a tenant.
Perform an online prediction .
Perform a batch prediction .
retrieve JSON credentials while creating client .
used for HTTP transport .
Return a fully-qualified alert_policy string .
Return a fully-qualified alert_policy_condition string .
Get multiple items from a Queue.
Maps BigQuery error reasons to an exception.
Returns a job reference for an API resource representation.
was started .
Datetime at which the job finished.
Helper for job-type specific statistics-based properties.
Helper for :meth:`from_api_repr`
begin the job via a POST request
API call:  test for the existence of the job via a GET request
API call:  refresh job properties via a GET request.
API call:  cancel job via a POST request
Refresh the job and checks if it is complete .
Start the job and wait for it to complete and get the result .
Get a value in the `` self._properties [ self._job_type ] `` dictionary .
Set a value in the `` self._properties [ self._job_type ] `` dictionary .
Remove `` key `` from the `` self._properties [ self._job_type ] `` dict .
Merge this job config with a default job config.
google.cloud.bigquery.table.EncryptionConfiguration: Custom
google.cloud.bigquery.schema.SchemaField ] : Schema of the
google.cloud.bigquery.table.TimePartitioning: Specifies time-based
Generate a resource for : meth : ` _begin ` .
Return file counts from job statistics, if present.
google.cloud.bigquery.dataset.DatasetReference: the default dataset
google.cloud.bigquery.table.TableReference: table where results are
Dict[str, google.cloud.bigquery.external_config.ExternalConfig]:
Build an API representation of the query job config.
Return query plan from job statistics , if present .
Return the query execution timeline
processed from job statistics , if present .
billed from job statistics , if present .
Return the DDL target table , present
Return the number of DML rows affected by the job .
referenced tables from job statistics , if present .
Return undeclared query parameters from job statistics, if present.
Return the estimated number of bytes processed by the query .
Return a pandas DataFrame from a QueryJob
Factory: construct instance from the JSON repr.
Union[Datetime, None]: Datetime when the stage started.
Union[Datetime, None]: Datetime when the stage ended.
were inputs for this stage .
Construct an UnknownJob from the JSON representation.
Update description of the zone.
named set of DNS name servers .
set bound to this zone .
Generate a resource for `` create `` or `` update `` .
create the zone via a PUT request
delete the zone via a DELETE request
sets for this zone .
Run image detection and annotation for an image.
Sample ID: go/samples-tracker/1534
Sample ID: go/samples-tracker/1512
Return a fully-qualified job string .
Creates a new job .
Retrieves the specified job , whose status is OPEN or recently EXPIRED
Deletes a list of `` Job `` \ s by filter .
using the provided `` SearchJobsRequest `` .
Compute a type URL for a klass .
given type URL .
Convert an ``Any`` protobuf into the actual class.
Factory:  construct an instance from a protobuf.
Factory: construct an instance from a dictionary.
Polls the status of the current operation.
Checks the status of the current operation.
Update the state of the current object based on operation .
Check if the operation has finished .
Parses the response to a `` ReadModifyWriteRow `` request .
Parses a Family protobuf into a dictionary .
Helper for :meth:`set_cell`
Helper for :meth:`delete`
Helper for :meth:`delete_cell` and :meth:`delete_cells`.
Gets the total mutations size for current row
Sets a value in this row.
cell in this row .
Deletes cells in this row.
Makes a ``CheckAndMutateRow`` API request.
Appends a value to an existing cell .
existing cell .
Makes a ``ReadModifyWriteRow`` API request.
Creates a Retry object given a gapic retry configuration .
Creates a ExponentialTimeout object given a gapic retry configuration .
default retry and timeout objects for each method in a gapic
is done , False otherwise .
Return the exception raised by the call , if any .
provided callable to the future .
provided result .
given exception .
registered to this Future .
transport.send ( ) .
Creates a new read session .; divides the contents of a
rows from the table in the format prescribed by the read session .
bytes ] : The qualifier encoded in binary .
List[:class:`~.external_config.BigtableColumn`]: Lists of columns
List[:class:`~.external_config.BigtableColumnFamily`]: List of
~google.cloud.bigquery.schema.SchemaField ` ] : The schema
Build an API representation of this object.
Factory: construct an :class:`~.external_config.ExternalConfig`
Run linters.
Run the system test suite .
Build the docs .
Convert a string representation of a binary operator to an enum .
Convert a string representation of a direction to an enum .
Convert a specific protobuf filter to the generic filter type .
Convert a cursor pair to a protobuf .
Parse a query response protobuf to a document snapshot .
matching query to a limited set of fields .
Filter the query on a field.
Helper for :meth:`order_by`.
Modify the query to add an order clause on a specific field .
Limit a query to return a fixed number of results .
offset in a query .
be used for a `` start_at `` or `` end_at `` cursor .
Start query results at a particular document value.
Start query results after a particular document value.
End query results before a particular document value.
End query results at a particular document value.
Convert all the filters into a single generic Filter protobuf.
Helper:  convert field paths to message.
based on cursors , where clauses .
based on orders .
Convert the current query into the equivalent protobuf .
Deprecated alias for : meth : ` stream ` .
Read the documents in the collection that match this query .
Monitor the documents in this collection that match this query.
Constructs a ModelReference .
Construct the API resource representation of this access entry
dataset ID string .
List[google.cloud.bigquery.dataset.AccessEntry]: Dataset's access
Union[datetime.datetime, None]: Datetime at which the dataset was
Grab prefixes after a :class:`~google.cloud.iterator.Page` started.
Convert a JSON blob to the native object .
Factory:  construct instance from resource.
changing : attr : ` bucket_policy_only_enabled ` from true to false .
Set the properties for the current object.
Factory constructor for blob object.
create a notification resource for the bucket .
Creates current bucket.
Get a blob object by name .
used to find blobs in the bucket .
List Pub / Sub notifications for this bucket.
Delete this bucket.
Deletes a blob from the current bucket .
Deletes a list of blobs from the current bucket .
Copy the given blob to the given bucket , optionally with a new name .
Rename the given blob using copy and delete operations .
Set default KMS encryption key for objects in the bucket.
set labels assigned to this bucket .
assigned to this bucket .
Retrieve IAM configuration for this bucket.
set lifecycle rules configured for this bucket .
configured for this bucket .
Add a `` delete '' rule to lifestyle rules configured for this bucket .
Deprecated ) Set ` Bucket.location `
logging for this bucket .
Retrieve the effective time of the bucket 's retention policy .
set the retention period for items in the bucket .
Set the retention period for items in the bucket.
Set the storage class for the bucket.
Configure website-related properties.
Retrieve the IAM policy for the bucket .
Update the IAM policy for the bucket .
revoking read access for anonymous users .
Create a signed upload policy for uploading objects .
Lock the bucket's retention policy.
Generates a signed URL for this bucket .
Convert a datetime to microseconds since the unix epoch .
Convert a microsecond-precision timestamp to datetime .
Convert a nanosecond-precision timestamp to a native datetime .
Return an RFC 3339-compliant timestamp.
preserving nanoseconds .
Return a timestamp message .
Creates a cluster in a project .
Deletes a cluster in a project .
Gets the resource representation for a cluster in a project .
regions/ { region } /clusters in a project .
completes , the
Actually publish all of the messages on the active batch.
has elapsed .
Publish a single message .
set given its API representation
Helper method for :meth:`from_api_repr`, :meth:`create`, etc.
set APIs .
Update name of the change set.
Append a record set to the 'additions ' for the change set .
Append a record set to the 'deletions ' for the change set .
Generate a resource for `` create `` .
set via a POST request .
Helper for logging-related API calls.
Helper for log sink-related API calls.
Helper for log metric-related API calls.
Return a page of log entries .
Creates a sink bound to the current client .
Creates a metric bound to the current client .
Return the default logging handler based on the local environment .
logging handler to the root logger .
Return a fully-qualified key_ring string .
Return a fully-qualified crypto_key_path string .
Return a fully-qualified crypto_key_version string .
Create a new `` KeyRing `` in a given Project and Location .
Create a new `` CryptoKey `` within a `` KeyRing `` .
Return a fully-qualified span string .
Sends new spans to new or existing traces .
Creates a new span .
occurs the message is
Return the current ack deadline based on historical time-to-ack .
Return the current load .
Check the current load and pause the consumer if needed .
Check the current load and resume the consumer if needed .
Send a request using a separate unary request instead of over the
Queue a request to be sent to the RPC .
Sends an empty request over the streaming pull RPC .
Begin consuming messages .
Return the initial request for the RPC .
received Pub/Sub messages .
be recovered .
Add a `` change '' to this batch to create a document .
Add a `` change '' to replace a document .
Add a `` change '' to update a document .
Add a `` change '' to delete a document .
accumulated in this batch .
is a tuple or list .
Convert non-none datetime to microseconds.
Convert a zoneless ISO8601 time string to naive datetime time
Convert a microsecond-precision timestamp to a native datetime .
Convert a timestamp to a string .
string value to bytes , if necessary .
bytes to a unicode value , if necessary .
specified message type
Convert a Timestamp protobuf to a datetime object .
Convert a datetime object to a Timestamp protobuf .
Convert a duration protobuf to a Python timedelta object .
Validate a URI path and get the leaf object 's name .
Makes a secure channel for an RPC service.
Makes a secure stub for an RPC service.
Makes an insecure stub for an RPC service.
Convert a timestamp from ( naive ) UTC to this timezone .
intended for : class : ` ~vision.helpers.VisionHelpers ` .
Return a function that will detect a single feature .
be called asynchronously in a thread pool .
end all pending callbacks .
Lists the specified events .
Deletes all error events of a given project .
document ref .
owns the current collection .
Create a sub-document underneath the current collection .
Get fully-qualified parent path and prefix for this collection .
Create a document in the Firestore database with the provided data .
List all subdocuments of the current collection.
Create a `` select '' query with this collection as parent .
Create a `` where '' query with this collection as parent .
Create an "order by" query with this collection as parent.
Create a limited query with this collection as parent .
offset in a query with this collection as parent .
Start query at a cursor with this collection as parent .
Start query after a cursor with this collection as parent.
End query before a cursor with this collection as parent.
End query at a cursor with this collection as parent.
Read the documents in this collection .
Get list of supported languages for translation.
Detect the language of a string or list of strings .
Translate a string or list of strings .
be managed by the leaser .
Remove messages from lease management.
being managed .
Create an instance of the gapic Logging API.
Uses the gapic client to report the error .
Return a fully-qualified table string .
requested rows in key order , optionally
Mutates multiple rows in a batch.
based on the output of a predicate Reader filter .
Refresh self from the server-provided protobuf.
Create this instance .
Check whether the instance already exists .
Reload the metadata for this instance.
Gets the access control policy for an instance resource .
Sets the access control policy on an instance resource .
create a cluster associated with this instance .
List the clusters in this instance .
create a table associated with this instance .
List the tables in this instance .
create AppProfile associated with this instance .
Lists information about AppProfiles in an instance.
A :class:`~google.cloud.bigquery.table.TableReference` pointing to
execute the view with Legacy or Standard SQL .
Convert a mapping to a row tuple using the schema .
Convert a JSON row to the native object .
Grab total rows when :class:`~google.cloud.iterator.Page` starts.
convert a string or Table to TableReference .
convert a string or TableReference to a Table .
Construct a table reference from table ID string.
Construct a BigQuery Storage API representation of this table.
google.cloud.bigquery.table.TimePartitioning: Configures time-based
Union[int, None]: Expiration time in milliseconds for a partition.
defining clustering for the table
Union[datetime.datetime, None]: Datetime at which the table will be
Union[google.cloud.bigquery.ExternalConfig, None]: Configuration for
Union[str, None]: Time partitioning of the table if it is
Return items as ``(key, value)`` pairs.
Return a value for key , with a default value if it does not exist .
construct a DataFrame .
construct DataFrame .
tqdm is installed .
Create a pandas DataFrame by loading all pages of a query .
Create an empty dataframe.
Return a : class : ` TimePartitioning ` object deserialized from a dict .
Convert response, content -> (multipart) email.message.
Convert requests.Response -> [(headers, payload)].
Override Connection:  defer actual HTTP request.
Prepares headers and body for a batch request.
responses to the futures created .
Submit a single ` multipart/mixed ` request with deferred requests .
Restart iteration after :exc:`.ServiceUnavailable`.
Perform a `` StreamingRead `` API request for rows in a table .
Perform an `` ExecuteStreamingSql `` API request .
Perform a `` ParitionRead `` API request for rows in a table .
Perform a `` ParitionQuery `` API request .
read ` .
Begin a read-only transaction on the database .
Returns the user-agent string for this client info .
rows from the table in the format prescribed by the read
Create an instance of the gapic Trace API.
Sends new traces to Stackdriver Trace or updates existing traces .
Gets a single trace by its ID .
match the filter conditions .
Convert a JSON bucket to the native object .
Factory: return client with anonymous credentials.
Get the email address of the project 's GCS service account
Factory constructor for bucket object.
Get a bucket by name .
Create a new bucket .
Get all buckets in the project associated to the client .
Construct an ID for a new job.
Check that a stream was opened in read-binary mode .
Get the email address of the project 's BigQuery service account
Construct a reference to a dataset.
create the dataset via a POST request .
API call:  create a table via a PUT request
Fetch the dataset referenced by `` dataset_ref ``
referenced by `` model_ref `` .
Fetch the table referenced by `` table `` .
Change some fields of a dataset.
[Beta] Change some fields of a model.
Change some fields of a table.
[Beta] List models in the dataset.
Delete a dataset.
[Beta] Delete a model
Delete a table
Get the query results object for a query job .
Detect correct job type from resource and instantiate.
cancel a job from a job ID .
Starts a job for loading data into a table from CloudStorage .
Upload the contents of this table from a file-like object.
Upload the contents of a table from a pandas DataFrame.
Perform a resumable upload .
Initiate a resumable upload .
Perform a multipart upload .
Copy one or more tables to another table.
Start a job to extract a table into Cloud Storage files .
Run a SQL query .
rows into a table via the streaming API .
rows into a table without applying local type conversions .
List the partitions in a table .
List the rows of the table .
Helper function for schema_from_json that takes a
takes a schema list and file
Takes a file object or file path that contains json that describes
Takes a list of schema field objects .
Creates an instance from a protobuf .
Make a copy of this instance .
Test whether this instance exists.
Update this instance .
Mark an instance and all of its databases for permanent deletion.
create a database within this instance .
List databases for the instance.
Convert a database protobuf to the native object .
be resolved .
Get the result of the operation , blocking if necessary .
Add a callback to be executed when the operation is complete .
done callbacks .
Set the Future 's result .
Set the Future 's exception .
Builds an HTTP context object from a Flask (Werkzeug) request object.
List entries via client.
List entries via client across multiple projects.
Sink log entries to storage.
Sink log entries to bigquery.
Sink log entries to pubsub.
is installed .
Builds customer encryption key headers
raise an `` InvalidResponse `` exception .
Add one query parameter to a base URL.
Set the blob's default chunk size.
Getter property for the URL path to this Blob.
Default query parameters.
The public URL for this blob.
Generates a signed URL for this blob .
Determines whether or not this blob exists.
Deletes a blob from Cloud Storage .
Get the download URL for the current blob .
Perform a download without any error handling .
Download the contents of this blob into a file-like object.
named file .
Download the contents of this blob as a string.
Determine the content type from the current object .
Get the object / blob metadata which is writable .
Get required arguments for performing an upload .
perform the upload .
Upload the contents of this blob from a file-like object.
provided string .
Create a resumable upload session .
granting read access to anonymous users .
Concatenate source blobs into this one.
Rewrite source blob into this one.
Update blob's storage class via a rewrite-in-place.
has the correct form .
Converts a native Python value into a Firestore protobuf ``Value``.
Encode a dictionary into protobuf `` Value `` -s .
Convert a reference value string to a document .
Converts a Firestore protobuf ``Value`` to a native Python value.
Converts a protobuf map of Firestore ``Value``-s.
Parse a document ID from a document protobuf .
Do depth-first walk of tree , yielding field_path , value
Set a value into a document for a field_path
Make ``Write`` protobufs for ``create()`` methods.
Make ``Write`` protobufs for ``set()`` methods.
Make ``Write`` protobufs for ``update()`` methods.
Make a `` Write `` protobuf for `` delete ( ) `` methods .
Get the transaction ID from a `` Transaction `` object .
Modify a `` Write `` protobuf based on the state of this write option .
Return a fully-qualified uptime_check_config string .
Creates a new uptime check configuration .
Performs asynchronous video annotation.
Legacy access to owner role.
Update owners.
Legacy access to editor role.
Update editors.
viewer role .
Update viewers.
Get information about document references .
Get a document reference from a dictionary .
Parse a ` BatchGetDocumentsResponse ` protobuf .
Lazy-loading getter GAPIC Firestore API.
string corresponding to this client 's project .
The RPC metadata for this client's associated database.
Get a reference to a collection .
Get a reference to a document in a collection .
Create a write option for write operations .
Retrieve a batch of documents .
List top-level collections of the client's database.
Helper for :meth:`commit` et al.
Begin a transaction on the database .
Roll back a transaction on the database.
Commit mutations to the database.
Helper for :meth:`execute_update`.
Perform an `` ExecuteSql `` API request with DML .
Perform a batch of DML statements via an `` ExecuteBatchDml `` request .
Converts the : class : ` TimestampRange ` to a protobuf .
Converts the row filter to a protobuf.
Return a fully-qualified organization_deidentify_template string .
Return a fully-qualified project_deidentify_template string .
Return a fully-qualified organization_inspect_template string .
Return a fully-qualified project_inspect_template string .
Return a fully-qualified project_job_trigger string .
Return a fully-qualified dlp_job string .
Return a fully-qualified organization_stored_info_type string .
Return a fully-qualified project_stored_info_type string .
Redacts potentially sensitive info from an image.
has been de-identified .
Returns a list of the sensitive information types that the DLP API
Creates an InspectTemplate for re-using frequently used configuration
Creates a new job to inspect storage or calculate risk metrics .
Creates a job trigger to run DLP actions such as scanning storage for
Creates a pre-built stored infoType to be used for inspection .
Helper for trace-related API calls.
Sends new spans to Stackdriver Trace or updates existing traces .
Creates a new Span .
Report error payload.
routing header string for the given request parameters .
Construct a field description protobuf .
Construct a struct parameter type description protobuf.
Perform bi-directional speech recognition .
yields the config followed by the requests .
Return a fully-qualified project_data_source string .
Return a fully-qualified project_transfer_config string .
Return a fully-qualified project_run string .
Creates a new data transfer configuration .
Updates a data transfer configuration .
transfer runs for a time range [ start\_time , end\_time ] .
Gets the most recent threat list diffs .
is used to check whether a URI is on a given threatList .
Gets the full hashes that match the requested hash prefix .
Return a fully-qualified asset string .
Return a fully-qualified asset_security_marks string .
Return a fully-qualified source string .
Return a fully-qualified finding string .
Creates a source .
Creates a finding .; exist for finding creation
is tracked with a long-running
Updates security marks.
Add row range to row_ranges list from the row keys
row range to given request message
describes this field .
dict which can be passed to
Builds the Error Reporting object to report.
Makes the call to the Error Reporting API.
Reports a message to Stackdriver Error Reporting
Reports the details of the latest exceptions to Stackdriver Error
Return the topmost transaction .
Begins a transaction .
Rolls back the current transaction.
Adds an entity to be committed .
Lists an organization or source's findings.
create the metric via a PUT request
API call:  test for the existence of the metric via a GET request
API call:  sync local metric configuration via a GET request
API call:  update metric configuration via a PUT request
delete a metric via a DELETE request
Convert a dict to protobuf .
Convert a span attribute dict to protobuf , including Links , Attributes ,
Convert a value to protobuf .
Creates an cluster instance from a protobuf .
Reload the metadata for this cluster.
Check whether the cluster already exists .
Create this cluster .
Update this cluster .
Delete this cluster.
Create cluster proto buff message for API calls
Create a new cell from a Cell protobuf .
Convert the cells to a dictionary.
Get a time series of cells stored on this instance .
Get a single cell value stored on this instance .
Consume the streamed responses until there are no more .
Helper for :meth:`__iter__`.
Helper for :meth:`consume_next`.
Updates the given message request as per last scanned key
Helper for :meth:`build_updated_request`
match the specified filter conditions .
Gets the details of a specific Redis instance .
Updates the metadata and configuration of a specific Redis instance .
stops serving and data is; Deletes a specific Redis instance .
Performs synchronous speech recognition: receive results after all audio
Performs asynchronous speech recognition: receive results via the
Return a fully-qualified region string .
Return a fully-qualified workflow_template string .
Creates new workflow template .
Retrieves the latest workflow template .
Instantiates a template and begins execution .
Parse a resource fragment into a schema field .
Return a `` SchemaField `` object deserialized from a dictionary .
Return a dictionary representing this schema field .
Opens a database specified by the parameters from parse_options ( ) .
Loads keys from database.
Does a single read operation .
Does a single update operation .
Does a single operation and records latency .
Runs workload against the database.
Run a single thread of the workload .
wrapped versions of the ` api ` member 's methods to the class .
returns a new product resource .
Makes changes to a Product resource.
Copy ``entity`` into ``entity_pb``.
Extract response data from a commit response.
Adds a new mutation for an entity with a partial key .
Adds a new mutation for an entity with a completed key .
Adds a new mutation for a key to be deleted .
be saved during : meth : ` commit ` .
Remember a key to be deleted during : meth : ` commit ` .
Begins a batch .
Commits the batch .
Rolls back the current batch.
Creates a new table in the specified instance .
Creates a new table from the specified snapshot .
drop/delete a row range from a specified table .
Creates a new snapshot in the specified cluster from the specified
Return a fully-qualified company string .
Creates a new company entity .
specified company .
Construct a zone bound to this client.
fill out the; Parses a resume into a `` Profile `` .
Sample ID: go/samples-tracker/1533
Convert a Query instance to the corresponding protobuf .
Update the query 's namespace .
Update the Kind of the Query .
Set the ancestor for the query
based on a property name , operator and a value .
returned the query .
used to sort query results .
used to group query results .
Execute the Query ; return an iterator for the matching entities .
Build a query protobuf .
Process the response from a datastore query .
Batch.insert ` et aliae .
Update one or more existing table rows .
Delete one or more table rows.
Return if a dataset exists .
Return if a table exists .
Queries for entities.
creating , deleting or modifying some
Return the GAE resource using the environment variables .
Return the labels for GAE app .
Return a fully-qualified service_account string .
Generates an OAuth 2.0 access token for a service account .
Generates an OpenID Connect ID token for a service account .
Signs a blob using a service account 's system-managed private key .
signed by third party identity provider to an OAuth 2.0
Return a fully-qualified entry string .
allows clients to use; Get an entry by target resource name .
Convert a path tuple into a full path string .
Consume a gRPC stream that should contain a single response .
cache the full path for this document .
Create a sub-collection underneath the current document .
Create the current document in the Firestore database .
Replace the current document in the Firestore database .
existing document in the Firestore database .
Delete the current document in the Firestore database.
Retrieve a snapshot of the current document .
List subcollections of the current document.
Get a value from the snapshot data .
Helper for :meth:`Session.run_in_transaction`.
Helper for :func:`_delay_until_retry`.
Return a fully-qualified database string .
Creates a new Cloud Spanner database and starts to prepare it for
Updates the schema of a Cloud Spanner database by
Get the scopes corresponding to admin / read-only state .
used for the Table Admin API .
create a instance associated with this client .
owned by the project .
List the clusters in the project .
Deletes a metric descriptor .
owned by a project in either the specified zone or all
Gets the details of a specific cluster .
Sets labels on a cluster.
Return a fully-qualified metric string .
Gets a logs-based metric .
Return a fully-qualified metric_descriptor string .
Return a fully-qualified monitored_resource_descriptor string .
Construct a KeyRange protobuf.
Return keyrange's state as a dict.
Construct a KeySet protobuf.
Return keyset's state as a dict.
corresponding state mapping .
Map errors for Unary-Unary and Stream-Unary gRPC callables.
Wrap errors for Unary-Stream and Stream-Stream gRPC callables.
Create a secure channel with credentials .
Get the next response from the stream .
handles partial objects on Python 2 .
Determine default project explicitly or implicitly as fall-back.
found ( unless stop requested ) .
Getter for a wrapped API object.
exists ) .
Retrieve entities, along with their attributes.
Save entities in the Cloud Datastore.
Delete keys from the Cloud Datastore.
Allocate a list of IDs from a partial key .
Proxy to :class:`google.cloud.datastore.key.Key`.
Proxy to :class:`google.cloud.datastore.query.Query`.
Add a row to the batch .; meets one of the size
Sends the current .
Verify that a topic path is in the correct format .
returned by the server .
Helper for :meth:`reload`.
create the notification .
Test whether this notification exists.
Update this notification from the server configuration .
Delete this notification.
Creates a Redis instance based on the specified tier and memory size .
snapshot file from GCS into a Redis instance .
Return a fully-qualified notification_channel string .
Return a fully-qualified notification_channel_descriptor string .
Return a batch to use as a context manager .
Helper for :meth:`log_empty`, :meth:`log_text`, etc.
API call:  log a text message via a POST request
API call:  log a structured message via a POST request
API call:  log a protobuf message via a POST request
Add a text entry to be logged during : meth : ` commit ` .
Add a struct entry to be logged during : meth : ` commit ` .
Add a protobuf entry to be logged during : meth : ` commit ` .
saved log entries as a single API call .
API call:  reload the config via a ``GET`` request.
get a variable via a `` GET `` request .
API call:  list variables for this config.
set or not nullable .
set or not nullable
row data to row with appropriate types .
row data to rows with appropriate types .
Coerce 'value' to a JSON-compatible representation.
Coerce 'value' to an JSON-compatible representation.
Maps a field and value to a JSON-safe value.
Convert a repeated/array field to its JSON representation .
Convert a record/struct field to its JSON representation .
Convert a field into JSON-serializable values .
string to camel case .
Get a nested value from a dictionary .
Set a nested value in a dictionary .
Remove a nested key fro a dictionary .
Build a resource based on a `` _properties `` dictionary , filtered by
Sample ID: go/samples-tracker/1510
sure a `` Reference '' database ID is empty .
Add the ID or name from an element to a list.
Convert a legacy `` Path '' protobuf to a flat path .
Convert a tuple of ints and strings in a legacy `` Path '' .
Parses positional arguments into key path with kinds and IDs.
protected data by combining raw data set from the constructor .
Duplicates the Key .
Creates new key from existing partial key by adding final ID/name .
Return a protobuf corresponding to the key .
Convert to a base64 encode urlsafe string for App Engine.
Convert urlsafe string to :class:`~google.cloud.datastore.key.Key`.
Creates a parent key for the current path .
The parent of the current key.
The entry point for the worker thread.
Starts the background thread .
Signals the background thread to stop.
attempts to send pending logs before termination .
Queues a log entry to be written by the background thread .
Overrides Transport.send().
Lex a field path into tokens ( including dots ) .
Split a field path into valid elements ( without dots ) .
Parse a * * field path * * from into a list of nested field names .
Create a * * field path * * from a list of nested field names .
Get a ( potentially nested ) value from a dictionary .
create a FieldPath from the string formatted per the API .
create a FieldPath from a unicode string representation .
Check whether `` other `` is an ancestor .
Return field paths for all parents.
Gets the latest state of a long-running operation .
match the specified filter in the request .
Starts asynchronous cancellation on a long-running operation .
Deletes a long-running operation .
Extract the config name from a full resource name .
Extract the variable name from a full resource name .
Return the maximum value in this histogram .
Return the minimum value in this histogram .
Add the value to this histogram.
Return the value that is the Nth precentile in the histogram .
Creates a job .
A :class:`~google.cloud.bigquery.model.ModelReference` pointing to
Union[datetime.datetime, None]: Datetime at which the model was
Union[datetime.datetime, None]: Datetime at which the model was last
Union[datetime.datetime, None]: The datetime when this model
str: URL path for the model's APIs.
Construct a model reference from model ID string.
Leases tasks from a pull queue for ``lease_duration``.
format a LogRecord in in Stackdriver fluentd format .
Get trace_id from flask request headers.
Get trace_id from webapp2 request headers.
Get trace_id from django request headers.
get trace_id from web application request header .
Return a fully-qualified group string .
Lists the existing groups .
supported for synthesis .
Synthesizes speech synchronously: receive results after all text input
are unsigned .
query parameters for creating a signed URL .
Convert 'expiration' to a number of seconds in the future.
offset from the current time .
Canonicalize headers for signing.
Canonicalize method, resource
Generate a V2 signed URL to provide query-string auth'n to a resource .
Generate a V4 signed URL to provide query-string auth'n to a resource .
Return a fully-qualified glossary string .
input text and returns translated text .
Detects the language of text within a request .
Returns a list of supported languages for translation.
Translates a large volume of text in asynchronous batch mode .
Creates a glossary and returns the long-running operation .
Deletes a glossary , or cancels glossary construction if the glossary
Return a copy of time_series with the points removed .
Build the combined resource and metric labels , with resource_type .
Build a : mod : ` pandas ` dataframe out of time series .
putting well-known resource labels first .
Starts a thread and marks it as a daemon thread .
swallowing and logging any exceptions .
Create a project bound to the current client .
existing project and it 's relevant metadata by ID .
List the projects visible to this client .
Return a fully-qualified session string .
be used to perform transactions; Creates a new session .
set as a stream .
Executes a batch of SQL DML statements .
rows from the database using key lookups and scans , as a simple
includes the mutations to be applied
Creates a set of partition tokens that can be used to execute a query
Creates a set of partition tokens that can be used to execute a read
issued when end user interacts with customer 's application
Create an instance of the GAPIC Datastore API.
Default unit test session.
Run the snippets test suite .
Indent some text.
Return the time that the message was originally published .
given message .
Release the message from lease management .
Inform the policy to lease this message continually .
Resets the deadline for acknowledgement .
acknowldge the given message .
Runs a query while printing status updates
Underlying function for bigquery cell magic
use for queries
use for queries performed through IPython
Return a fully-qualified database_root string .
Return a fully-qualified document_root string .
Return a fully-qualified document_path string .
Return a fully-qualified any_path string .
Gets a single document .
Lists documents.
Creates a new document .
inserts a document .
Deletes a document .
Gets multiple documents.
Starts a new transaction .
Runs a query .
Streams batches of document updates and deletes, in order.
Return a fully-qualified log string .
Deletes all the log entries in a log .
Writes log entries to Logging .
Use this method to retrieve log entries from Logging .; log entries .
Expand a matched variable with its value .
Expand a path template with the given variables .
Replace a variable match with a pattern that can be used to validate it .
Validate a path against the path template .
Creates an instance app_profile from a protobuf .
Create an AppProfile proto buff message for API calls
Reload the metadata for this cluster
Check whether the AppProfile already exists .
Create this AppProfile .
Update this app_profile .
Return a fully-qualified sink string .
Return a fully-qualified exclusion string .
Creates a sink that exports specified log entries to a destination .
replaces the following fields in the; Updates a sink .
Creates a new exclusion in a specified parent resource .
Helper for :func:`_make_list_value_pbs`.
Convert a Value protobuf to cell data .
Convert a list of ListValue protobufs into a list of list of cell data .
given Cloud Storage
gets the update history of assets that overlap a time window .
Extract and parse Avro schema from a read session.
Parse all rows in a stream block.
Copy a StreamPosition .
using the most recent offset .
Create a : class : ` pandas.DataFrame ` of all rows in the stream .
A generator of all pages in the stream.
rows from the block only once .
Get the next row in the page .
Create a : class : ` pandas.DataFrame ` of rows in the page .
Return a fully-qualified instance_config string .
Creates an instance and begins preparing it to begin serving .
begins allocating or releasing resources as
Return a fully-qualified scan_config string .
Return a fully-qualified scan_run string .
Make a copy of this client .
List available instance configurations for the client's project.
List instances for the client's project.
determining when to retry .
Run the unit test suite .
Detect correct entry type from resource and instantiate.
Retrieve the metadata key in the metadata server .
Define loss of TF graph
initializes the variables of a TensorFlow session that were not
Train a TF graph .
Compute the accuracy of a TF model on some data
deprecated function .
computes the current class prediction
normalize a batch of vectors .
compute kl-divergence KL ( p || q )
clip the perturbation to epsilon norm ball .
Returns the list of devices that multi-replica code should use .
string names of all available GPUs
casts the clipping range if needed .
does more automatic casting of
compute f ( a , b ) .
Create the Jacobian graph to be ran later in a TF session
set using the Jacobian
Run evaluation on a saved model
Preprocessing the inputs before calling session.run ( )
Move the outputs of; Postprocess the outputs of the Session.run ( ) .
write images from single batch into datastore .
Writes all image batches to the datastore .
Writes only images from one batch to the datastore .
reading from the datastore .
Adds batch with give ID and list of properties.
given batch .
Reads list of dataset images from the datastore.
Initializes dataset batches from the list of images in the datastore.
Init list of adversarial batches from dataset batches and submissions.
generated adversarial examples .
Load a saved model , gather its predictions , and save a confidence report .
Prints out accuracy, coverage, etc.
MNIST CleverHans tutorial
Generate symbolic graph for adversarial examples and return.
Runs the untargeted attack .
Run the attack on a specific target class .
computes a batch start and end index
excluding the class indexed by class_ind
Converts a class vector (integers) to binary class matrix.
Take in an array of correct labels and randomly select a different label
Create a logger object with the given name .
is always the same
Return the union of l1 and l2 , with a deterministic ordering .
like zip but with these properties:
Calls shell command with argument substitution.
are numpy arrays .
Load and preprocess MNIST dataset
Preprocess CIFAR10 dataset
Load a saved model and print out its accuracy on different data distributions
The actual implementation of the evaluation.
PyTorch implementation of the Fast Gradient Method.
Read png images from input directory in batches.
Saves images to the output directory.
Run the sample attack
Load training and test data.
Plots a success-fail curve from a confidence report stored on disk ,
Plot a success fail curve from a confidence report
Make a success-failure curve .
Train a TF graph
unused by the attack on all GPUs .
Return a tensor that constructs adversarial examples for the given
process simultaneously.
Perform the attack on the given instance for the given targets .
create a new graph on the
Return a list of assignment operations that syncs the parameters
initialize a variable using a numpy array and set trainable .
initialize layer parameters on the device previously set
Create an assignment operation for each weight on all devices.
used for virtual
Take in a dictionary of parameters and applies attack-specific checks
Iterate with exponential backoff on failures.
Lists names of all blobs by their prefix.
pending mutations .
Adds mutation of the entity to the mutation buffer.
given key to the mutation buffer .
given its key .
MNIST tutorial for Carlini and Wagner's attack
Selects the Attack Class using string input .
MNIST cleverhans tutorial
Removes directory tree as a superuser.
runs worker .
downloads submission to local directory .
Creates a temporary copy of extracted submission .
Runs docker command without time limit.
Runs docker command and enforces time limit.
Runs attack inside Docker.
Runs defense inside Docker.
Read `dataset_meta` field from bucket
execute attacks .
Runs one attack work.
evaluates all attack work .
execute defenses .
Runs one defense work.
evaluates all defense work .
Run attacks and defenses
Returns a hashable summary of the types of arg_names within kwargs.
Construct the graph required to run the attack through generate_np .
return them as a NumPy array .
Construct the inputs to the attack graph to be used by generate_np .
Get the label to use in generating an adversarial example for x .
described in https : //arxiv.org/abs/1511.06581
MNIST tutorial for the Jacobian-based saliency map approach (JSMA)
minimize ` loss `
Calculate the average gradient for each shared variable across all
Creates the symbolic graph of an adversarial example given the name of
standard output and Tensorflow summary .
Evaluate the accuracy of the model on adversarial examples
Run the evaluation on multiple attacks .
Runs some code that will crash if the GPUs / GPU driver are suffering from
warns that the function is deprecated .
used to be needed to support tf 1.4 and early , but support for tf 1.4 and earlier is now dropped .
Wrapper around tf.nn.softmax_cross_entropy_with_logits_v2 to handle
Enforces size of perturbation on images, and compute hashes for all images.
organize it by batches and rename images .
file with target class for given dataset batch .
using tf 's full eigen decomposition .
returns smoothed version of min eigen vector .
Computes the min eigen value and corresponding vector of matrix M .
scipy estimate of min eigenvalue for matrix M .
running one step of descent .
Run one step of gradient descent for optimization.
Run the optimization , call run_one_step with suitable placeholders .
Loads target classes.
Applies DeepFool to a batch of inputs
TensorFlow implementation of DeepFool.
consider nb_candidate classes when
defined as given by Tensorflow
Convert a pytorch model into a tensorflow op that allows backprop
PyTorch implementation of the clip_eta in utils_tf.
Solves for the optimal input to a linear function under a norm constraint.
y_target is None ) then use the
Perform the EAD attack on the given instance for the given targets .
surrounded by a box made of * s
Validates the submission .
Make a confidence report and save it to disk .
creates a symoblic graph of the MadryEtAl attack on
testing this attack .
Return the model 's classification of the input data , and the confidence
is correct and its confidence on each example in
Run attack on every example in a dataset.
evaluating an expression across a whole
computes a tensor on numpy inputs by batches .
sure a ` y ` argument is a vliad numpy dataset .
Creates a preprocessing graph for a batch given a function that processes
fed as inputs to the softmax layer ) .
produced by the softmax layer ) .
Provides access to the model's parameters.
be returned later by get_params .
Return a layer output .
Takes in confidence values for predictions and correct
find nearest neighbors in training data .
Given a data_activation dictionary that contains a np array with activations for each layer ,
Given an dictionary of nb_data x nb_classes dimension , compute the nonconformity of
Given an array of nb_data x nb_classes dimensions , use conformal prediction to compute
Performs a forward pass through the DkNN on an numpy array of data.
Performs a forward pass through the DkNN on a TF tensor by wrapping
Runs the DkNN on holdout data to calibrate the credibility metric .
input features based
computing saliency maps
TensorFlow implementation of the foward derivative / Jacobian
implements either the Basic Iterative Method
Residual unit with 2 sub layers.
Relu, with optional leaky support.
create variables for the projected dual object .
using the Lanczos algorithm .
constructs minimization objective from dual variables .
provides matrix product interface with PSD matrix .
returns the tf graph corresponding to the entire matrix M .
find a value for nu that makes M PSD
Computes the min eigen value and corresponding vector of matrix M or H
compute the certificate based either current value
Defines the right convolutional layer according to the
Defines a CNN model using Keras sequential model
Looks for the name of the softmax layer.
Looks for the name of abstracted layer.
producing the logits .
returned by get_layer_names .
Expose the hidden features of a model given a layer name .
based on the filename extension .
Calls shell command with parameter substitution.
Makes directory readable and writable by everybody.
prepare temporary directory .
moves it into self._extracted_submission_dir .
Verifies size of Docker image.
sample data for the submission .
Verifies correctness of the submission output.
Validates submission.
Save loss in json format
Pairwise Euclidean distance between two matrices.
Pairwise cosine distance between two matrices.
Exponentiated pairwise distance between each element of A and
normalized exponentiated pairwise distance between all the elements
Masking matrix such that element i , j is 1 iff y [ i ] == y2 [ i ] .
sampling probabilities for the elements of x for neighbor
Soft Nearest Neighbor Loss
The optimized variant of Soft Nearest Neighbor Loss.
Save an image , represented as an ndarray , to the filesystem
Converts an ndarray to a PIL image.
Turns a batch of images into one big image .
Generates the adversarial sample for the given input .
TensorFlow Eager implementation of the Fast Gradient Method.
be used with ` feed_dict ` .
given suffix .
ending in ` suffix ` contained within ` path ` .
given text and frame composed of ' # ' characters .
Saves dictionary as CSV file.
runs master .
is already populated asks whether we should continue .
needed for evaluation of attacks .
needed for evaluation of defenses .
Saves statistics about each submission.
sorted ( by score ) results of the evaluation .
Reads dataset metadata.
Computes results (scores, stats, etc...) of competition evaluation.
given work pieces .
given work pieces into file .
Shows current status of competition evaluation.
Cleans up data of failed attacks .
Cleans up data about attacks which generated zero images .
deletes entries with keys .
Cleans up all data about defense work in current round.
Cleans up datastore and deletes all information about current round .
see https : //arxiv.org/abs/1511.07528
Create a multi-GPU model similar to the basic cnn in the tutorials .
Create a multi-GPU model similar to Madry et al .
Build the graph for cost from the logits if logits are provided .
Bottleneck residual unit with 3 sub layers.
Reads classification results from the file in Cloud Storage.
analyzes one classification result .
matrix to the file .
writes to datastore .
reading it from the datastore .
Reads and returns single batch from the datastore.
Computes classification results.
based on submission filename .
Loads list of submissions from the directory.
saves them to Datastore .
Writes all submissions to datastore .
Init list of submission from Datastore.
targeted and non-targeted ) .
Finds submission by ID.
Returns human readable submission external ID.
Loads and verifies metadata.
Runs submission inside Docker container.
Tensorflow 2.0 implementation of the Fast Gradient Method.
Computes the gradient of the loss with respect to the input tensor .
Saves a pdf of the current matplotlib figure .
train a model that simulates the `` remote ''
creates the substitute by alternatively
MNIST tutorial for the black-box attack from arxiv.org/abs/1602.02697
Pad a single image and then crop to the original size with a random
dataset augmentation to a batch of exmaples .
Augment a batch by randomly cropping and horizontally flipping it .
Project `perturbation` onto L-infinity ball of radius `epsilon`.
Computes difference between logit for `label` and next highest logit.
TensorFlow implementation of the Spatial Transformation Method.
Apply image transformations in parallel.
projected optimization , generalized to work with approximate
Generate symbolic graph for adversarial examples.
Compute a new value of ` x ` to minimize ` loss_fn ` .
Analogous to tf.Optimizer.minimize
Initialize t, m, and u
parent class documentation .
using SPSA .
command line arguments .
read all submissions .
ranking and saves it .
Run all attacks against all defenses and compute results.
loads dataset and determines clipping range .
directory with all images .
target classed for all dataset images into given file .
A reasonable attack bundling recipe for a max norm threat model and
using random search .
Runs attack bundling.
working on one specific AttackGoal .
bundling on one batch of data .
Saves the report and adversarial examples .
have not yet been run the desired
chooses the strongest
Runs the MaxConfidence attack using SPSA as the underlying optimizer .
Returns a dictionary mapping the name of each criterion to a NumPy
run in the next batch .
indicating whether a new adversarial example is better
counts only for examples that are still correctly classified
Return the counts for only those examples that are below the threshold
Clip an image, or an image batch, with upper and lower threshold.
Compute the distance between two images .
Gradient direction estimation
given l2 / linf balls in a batch .
approach the boundary .
Efficient Implementation of BlendedUniformNoiseAttack in Foolbox.
search for stepsize .
Choose the delta at the scale of distance
Generate adversarial images in a for loop.
Required for targeted attack .
Main algorithm for Boundary Attack ++.
TensorFlow implementation of the Fast Feature Gradient.
Builds the 35x35 resnet block.
Builds the 17x17 resnet block.
Inception model from  http://arxiv.org/abs/1602.07261.
Creates the Inception Resnet V2 model .
Returns the scope with the default parameters for inception_resnet_v2 .
copy them into place
update submission statistics .
Print statistics into log.
Copies submission from Google Cloud Storage to local directory.
target directory .
Validates one submission and copies it to target directory .
mapping from submission IDs to original filenames .
Runs validation of all submissions.
fail plots .
is unclaimed .
Writes all work pieces into datastore .
Reads all work pieces from the datastore.
are assigned to shard with given id .
Reads undone work from the datastore.
pick next unclaimed piece of work to do .
work piece in datastore as completed .
stored in this class .
Initializes work pieces from adversarial batches.
Initializes work pieces from classification batches.
TensorFlow implementation of the Fast Gradient Method.
Returns the graph for Fast Gradient Method adversarial examples .
read the weights from checkpoint based on json description .
forward pass through the layer weights at layer_index .
Returns a hexdigest of all the python files in the module.
The current implementation of report printing.
The deprecated implementation of report printing.
trained to minimize Cross Entropy and Maximize Soft Nearest
Train a TF Eager model
Compute the accuracy of a TF Eager model on some data
initialize the dual variables of the class .
performs one step of gd ( variant ) for min eigen value .
eigenvector which corresponds to minimum eigenvalue .
smallest eigenvector and eigenvalue using Lanczos in pure TF .
Provides access to the model's Variables.
dropping random units .
Perform the L_2 attack on the given instance for the given targets .
Run the attack on a batch of instance and labels .
Load model if present at the specified path.
running cleverhans from a different directory than tutorial .
Downloads the image that corresponds to the given row .
resizes it and saves it locally .
Custom py_func with gradient support
throught the network
given layer .
displays two images : the original and the adversarial sample
displays a grid of images to show full misclassification
logits when the input is perturbed in an interval in adv direction .
Generate linear extrapolation plot.
Encode IQFeed API messages.
Send data query to IQFeed API.
Request historical 5 minute data from DTN.
Build Pandas Dataframe in memory
Load ticker list from txt file
Write Pandas Dataframe to InfluxDB database
window and set it foreground
Get rectangle of app or desktop resolution
Take a screenshot and save it to ` tmp.png ` filename by default
pocoairtest
Read a tag from the buffer , and return a ( tag_bytes , new_pos ) tuple .
Return a constructor for a decoder for fields of a particular type .
invokes modify_value on every value
Return a constructor for a decoder for a fixed-width field .
Returns a decoder for a float field.
Returns a decoder for a double field.
Returns a decoder for a string field.
Returns a decoder for a bytes field.
Returns a decoder for a group field.
Returns a decoder for a map field.
Skip a varint value .
Skip a length-delimited value .
Skip sub-group.
Constructs the SkipField function .
Constructs the dependency graph .
Evaluate the model by making predictions of target values and comparing
A flexible and advanced prediction API.
using the trained model .
Return a classification , for each example in the `` dataset `` , using the
get enviroment variables for slaves
get a ring structure that tends to share nodes with the tree
get a ring connection used to recover local data
get the link map , this is a bit hacky , call for better algorithm
generate a faster alternative to MSVC setup scripts .
create a suitable classifier model based on the provided
Adds the specified column to this SFrame .
Adds columns to the SFrame.
Removes the column with the given name from the SFrame .
given names .
changes the names of; Rename the columns using the 'names ' dict .
Returns the number of rows .
Returns the column names .
Returns the column types .
create a suitable regression model based on the provided
is removable only if all of its children are as well .
remove nodes from a list
Loads WAV file(s) from a path.
given message type in the local database .
registered messages from a specified file .
String hash ( djb2 ) with consistency between py2/py3 and persistency between runs ( unlike ` hash ` ) .
bounding boxes ( ground truth or predictions ) by
Create a : class : ` ~turicreate.toolkits.SupervisedLearningModel ` ,
using the trained supervised_learning
Evaluate the model on the given dataset .
Disassemble a code object .
Convert a decision tree model to protobuf format .
Check that the predictionsa are either probabilities of prob-vectors .
checking for the evaluation metrics .
Fetches the meta data for the current library.
Convert a trained XGBoost model to Core ML format .
maps to the Python built-in; Dumps a serializable object to JSON .
Visualizes drawings (ground truth or predictions) by
using the SFrame ` data ` .
including ( where
extract the leaf indices of
missing features associated with
Dump the models into a list of strings .
including ( where relevant )
Convert a boosted tree model to protobuf format .
corresponding vote totals according to the
Create a
load a previously saved NearestNeighborClassifier model .
Return the predicted class for each observation in * dataset * .
predicted class labels for instances in * dataset * .
Return top-k most likely predictions for each observation in
is done by predicting the; Evaluate the model 's predictive accuracy .
A compact version of __repr__ for each of the steps.
perform fit_transform ( ) on all but last step .
fit a transformer using the SFrame ` data ` and then return a transformed
Transform the SFrame ` data ` using a fitted model .
load an object with a specific version of the class .
Return a model object; Compute the PageRank for each vertex in the graph .
Initializes the gcc toolset for the given version .
Now, the vendor specific flags.
Adds a dependency from 'targets ' to 'sources '
Gets the value of ` variable ` on set on the first target in ` targets ` .
Sets a target variable.
Binds a target to the corresponding update action .
Creates a new build engine action .
self that 'action_name ' is declared in bjam .
Returns the pixel data stored in the Image object .
Displays the image .
gets passed into the
Visualize the model .
based on selected features and their
Create a nearest neighbor model , which can be searched efficiently and
List the fields stored in the model , including data , model , and
Return the value of a given field .
Return a dictionary of statistics collected during creation of the
retrieve the nearest neighbors
Construct the similarity graph on the reference dataset , which is
split an SFrame into two SFrames based on the ` session_id ` such
Reads the MS Build XML file at the path and returns its contents .
Reads the MS Build JSON file at the path and returns its contents .
Merges the values between the current and previous run of the script.
Finds the value in the list that corresponds with the value of compare .
found in the root and converts them using the func
Converts an EnumProperty node to JSON format.
Converts an BoolProperty node to JSON format.
Converts a StringListProperty node to JSON format.
Converts a StringProperty node to JSON format.
Converts a XML node to a JSON equivalent.
contains an Argument .
Preprocesses occurrences of Argument within the root.
Retrieves the attribute of the given name from the node .
Gets the path to the file .
Gets the output path for a file given the toolchain , rule and output_dir
Writes a JSON file at the path with the values provided .
Appends the value to the list .
Decompile a function into ast.FunctionDef node.
Compile a function from an ast.FunctionDef instance.
decompile apython pyc or pyo binary file.
extracting definitions .
Parses list of lines.
Expands the macro reference .
Processes the file contents .
Convert a _imputer model to the protobuf spec .
is mainly for testing purposes .
Declares a new feature with the given name , values , and attributes .
Sets the default value of the given feature , overriding any previous default .
Returns the default property values for the given features .
are valid features .
Return the values of the given feature .
'value_string ' is a value_string
Returns the implicit feature associated with the given implicit value .
is a valid feature .; raises an exception .
Helper for expand_subfeatures.
corresponding to implicit features
Adds the given values to the given feature .
is a valid value-string for the given feature .
Declares a subfeature .
Sets the components of the given composite property .
given feature specified by the given property set .
Expand all composite properties in the set so that all components
is an ordinary subfeature of the parent_property 's
As is_subfeature_of, for subproperties.
Given a property set which may consist of composite and implicit
Given a set of properties , add default values for features not
Given an expanded property set , eliminate all redundancy : properties
Given a property-set of the form
Combine all subproperties into their parent properties
Given a property , return the subset of features consisting of all
Retrieves the interpretation function used .
Returns the description and output type for a given interpretation .
Returns a list of the available interpretations and what they do.
load a previously saved SentenceSplitter instance .
using the given data .
Transforms the data .
Transforms short text into a dictionary of TFIDF-weighted 3-gram
Interprets an integer column as a categorical variable.
Sets up the content transforms .
transforms the SFrame ` data ` using a fitted model .
Save the model as a directory , which can be loaded with the
Create a new mock object .
Replace a method , attribute , etc .
Verify that all of the expected calls have been made .
Verify the called method is expected .
Returns a possible group from the end of the call queue or None if no
is an instance of our
see if the RHS is an instance of class_name .
see if RHS is almost equal to float_value
Check whether the given key/value pair is in the rhs dict .
see whether actual_seq has same elements as expected_seq .
is equal to rhs .
Remove a method call from the group .
are called at least once .
set the regression interface params .
set the regressor interface params .
set transform interface params .
batch form , where each row contains a sequence of length
assuming SFrame stores data flattened
Set the inputs of the network spec.
Set the outputs of the network spec.
make it a neural network classifier .
Add optional inputs and outputs to the model spec.
embedding layer to the model .
Add a softmax layer to the model .
Add an activation layer to the model.
Add an element-wise operation layer to the model.
Add upsample layer to the model.
Add scale layer to the model.
bias layer to the model .
Add sequence repeat layer to the model.
Add a convolution layer to the network .
refer to NeuralNetwork.proto for details .; Add a padding layer to the model .
Add a cropping layer to the model .
Add a simple recurrent layer to the model .
Add a Gated-Recurrent Unit ( GRU ) layer to the model .
Add a Uni-directional LSTM layer to the model .
Add a Bi-directional LSTM layer to the model .
flattens the channel , height and width axis .; Leaves the sequence axis as is .; Add a flatten layer .
Add a slice layer .; numpy slice [ start_index : end_index : stride ] ,
Add a data reorganization layer of type `` SPACE_TO_DEPTH '' or `` DEPTH_TO_SPACE '' .
Add a Batch Normalization layer .
Add a permute layer .; has dimensions in the order [ Seq , C , H , W ]
refer to NeuralNetwork.proto for details .; Add a reshape layer .
specified by the parameter mode ,; Add a reduce layer .
Add a LRN ( local response normalization ) layer .; see the LRNLayerParams message in Core ML neural network
mean , variance and normalizes the input .
Add L2 normalize layer.
specified function ( mode ) to all the elements of the input .; Add a Unary layer .
Add a Split layer that uniformly splits the input along the channel dimension
Add a load constant layer .
Add a custom layer .
Add pre-processing parameters to the neural network object
specifying a set of
Returns an instance of previously registered scanner
Installs the specified scanner on actual target 'target ' .
Creates a skeleton function object that contains just the provided
calling imp.find_module directly .
Save a module as an import
Ensure de-pickler imports any package child-modules that
read or written to by codeblock co
Modified to support __transient__ on new objects
do not serialize correctly in python2.x -- this fixes the bugs
Save a file
saving numpy ufunc objects
Pulls out all the symbols from a descriptor proto.
Adds the FileDescriptorProto and its types to this database .
Convert a normalizer model to the protobuf spec .
submit nslave jobs , each must contain args as parameter
Create a ( binary or multi-class ) classifier model of type
Get the right converter function for Keras
Convert a Keras model to Core ML protobuf specification ( .mlmodel ) .
Create a DBSCAN clustering model .
find the path to xgboost dynamic library files .
Check if a model is of the right type .
Convert a LIBSVM model to Core ML format .
list.append ( ) .
Inserts the item at the specified position .; list.insert ( ) .
appending the given iterable .; list.extend ( ) .
Appends the contents of another repeated field of the same type to this
list.remove ( ) .
given index .; list.pop ( ) .
Adds a new element at the end of the list and returns it .
appending the given sequence of elements of the same type
Returns the elements of B that are not in A .
do n't appear in set2 and returns the result .
Returns true iff all elements of 'small' exist in 'large'.
contains the same elements as ' b ' , irrespective of their order .
Annotate your images loaded in either an SFrame or SArray Format
Recover the last annotated SFrame .
Convert a DictVectorizer model to the protobuf spec .
Internal function.
execute toolkit on the turicreate server .
Truncates the remainder part after division .
is valid for Message Descriptor .
is not a FieldMask .
Converts a path name from snake_case to camelCase.
Converts a field name from camelCase to snake_case.
specified by a sub-tree from source to destination .
Adds the field paths descended from node to field_mask .
Packs the specified message into current Any message .
Unpacks the current Any message into specified message.
RFC 3339 date string format .
Parse a RFC 3339 date string format to Timestamp .
Converts nanoseconds since epoch to Timestamp.
Converts microseconds since epoch to Timestamp.
Converts milliseconds since epoch to Timestamp.
Converts Timestamp to datetime.
Converts datetime to Timestamp.
Converts a Duration to microseconds.
Converts a Duration to milliseconds.
Converts microseconds to Duration.
Converts milliseconds to Duration.
Converts Duration to timedelta.
Convertd timedelta to Duration.
Set Duration by seconds and nonas.
string according to proto3 JSON spec .
Gets all direct fields of Message Descriptor to FieldMask.
Merges mask1 and mask2 into this FieldMask.
Intersects mask1 and mask2 into this FieldMask.
specified in FieldMask from source to destination .
Adds a field path into the tree .
Calculates the intersection part of a field path with this tree .
begin with prefix to this tree .
specified by this tree from source to destination .
Configures a new resource compilation command specific to a condition ,
Convert a Nu Support Vector Regression ( NuSVR ) model to the protobuf spec .
Create a : class : ` ~turicreate.linear_regression.LinearRegression ` to
Export the model in Core ML format.
using the trained
making target value predictions and comparing
overlapping frames .
Calculate a `` periodic '' Hann window .
Calculate the short-time Fourier transform magnitude .
Return a matrix that can post-multiply spectrogram rows to make mel .
Convert waveform to a log magnitude mel-frequency spectrogram.
Creates a random SFrame with ` num_rows ` rows and randomly
Input:
Convert a svm model to the protobuf spec .
Create an :class:`ActivityClassifier` model.
num_classes - 1 ]
using the trained activity classifier .
Return a classification , for each `` prediction_window `` examples in the
Count the occurrances of the different characters in the files
The main function of the script
Save a protobuf model specification to file .
Load a protobuf model specification from file
contains any .
Evaluate a CoreML regression model and compare against predictions
Evaluate a CoreML classifier model and compare against predictions
Evaluate a classifier specification for testing .
Rename a feature in the specification .
cleaning steps on the data so various type comparisons can
Performs a robust equality test between elements.
Evaluate a transformer specification for testing .
Returns a list of the names of the inputs to this model.
Compute the in degree , out degree and total degree of each vertex .
replace the index'th emphasized text with s
sets up variable context before
is the last example
given location .; loading , project global
Loads parent of Jamfile at 'location'.
Given 'name ' which can be project-id or plain directory name ,
Returns the name of module corresponding to 'jamfile-location ' .
given location .
Load a Jamfile at the given directory .
has no location
Initialize the module for a project .
Make 'project-module' inherit attributes of project
given id with the given project module .
'project ' .
Returns the value of the specified attribute in the
corresponding to the 'project-module ' .
loaded Jamfiles .
Recursively walks through the b2/src subdirectories and
Load a Python module that should be useable from Jamfiles .
named attribute from the specification given by the user .
Prints the project attributes.
Given a free-standing function 'callable ' , return a new
set a project global constant .
set a project global constant , whose value is a path .
Calculates conditional requirements for multiple requirements
Creates a feature extractor from an input array feature , return
Add a single build XML output file to our data .
Process the target dependency DAG into an ancestry tree so we can look up
Given a build action log , process into the corresponding test log and
goes to the corresponding attribute in the result .
Print the detailed info of failed or always print tests .
Get a summary of _NeuralNetwork_pb2.WeightParams
Summarize network into the following structure.
Print the network information summary.
produces a starting spec using the parts .
Convert a Support Vector Classtion ( SVC ) model to the protobuf spec .
Extract the ordering of the input layers .
Extract the ordering of output layers .
Generate blob names for each one of the edge.
remove the layer and its input/output edges
is layer_idx .
Defuse the fused activation layers in the network.
edges that represents transition from not 1D to 1D , and 1D to not 1D
Insert permutation layers before a 1D start point or after 1D end point
Replace the old node with the new one.
better know .
Create a Transformer object to transform data for feature engineering .
breaking it up into frames .
Performs both audio preprocessing and VGGish deep feature extraction.
Return the Core ML spec
Remove redundant statements.
prevent circular imports , this extends isinstance ( )
refer to a Python value inside Jam language code .
is delimited by a '- ' .
Apply a set of standard transformations to string to produce an
Get the decision from this node to a child node .
Return the node as a dictionary .
dump this tree as a json blob .
Return the prediction score ( if leaf node ) or None if its an
Return the prediction path from this node to the parent node .
Given a weighted graph with observed class labels of a subset of vertices ,
Check if a Turi create model is pickle safe .
Check if class is a Turi create model .
get the type of the GLC class .
get a GLC object from a persistent ID in the pickle file .
Provide a persistent ID for `` saving '' GLC objects by reference .
Close the pickle file, and the zip archive file.
Reconstruct a GLC object using the persistent ID .
were created .
Convert scikit-learn pipeline, classifier, or regressor to Core ML format.
Generate a new Message instance from this Descriptor and a byte string .
Construct a class object for a protobuf described by descriptor .
are supported .
decoding a single Image or an SArray of Images
Resizes the image or SArray of Images to a specific width , height , and
byte array .
bytes to bits
Generate a linear lookup table .
given a weight parameter field
quantize weight blob .
Quantize the weight blob
Quantize WeightParam field in Neural Network Protobuf
compare the performance of a full precision vs quantized model
convert a full precision ( float ) MLModel to a
Create a recommender that uses item-item similarities based on
Get the keras layer name from the activation name .
Convert a dense layer from keras to coreml .
Convert an activation layer from keras to coreml.
Convert an ReLU layer with maximum value from keras to coreml.
Convert convolution layer from keras to coreml.
Convert separable convolution layer from keras to coreml.
Convert a Batch Normalization layer .
Convert a flatten layer from keras to coreml .
Convert concat layer from keras to coreml.
pooling layer from keras to coreml .
padding layer from keras to coreml .
Convert a softmax layer from keras to coreml .
Convert an SimpleRNN layer from keras to coreml.
Convert an LSTM layer from keras to coreml.
Convert a GRU layer from keras to coreml .
Convert a bidirectional layer from keras to coreml .
[ lower : ] = expr
[ lower : upper ] = expr
obj[:] = expr
Create a content-based recommender model in which the similarity
Return a set of symbols in ` node ` that are assigned .
outputs into conditional and stable
Group lhs and rhs into conditional, stable and undefined
Load rabit library.
Intialize the rabit module , call this once before using anything .
used by the module ,
Load latest check point.
Checkpoint the model .
object detection annotations ( ground truth or predictions ) to
Create a RankingFactorizationRecommender that learns latent factors for each
_sources/reference.rst into separate files
encodes the field number and
Returns the number of bytes required to serialize a single varint
seconds as a human-friendly string , e.g .
Returns the module holding the conversion functions for a
Converts a generic sklearn pipeline, transformer, classifier, or regressor
Set the default prediction value(s).
Add a branch node to the tree ensemble .
Add a leaf node to the tree ensemble .
Creates a new 'PropertySet ' instance for the given raw properties ,
Creates new 'PropertySet ' instances after checking
Creates a property-set from the input given by the user , in the
provided by the user .
are neither incidental nor free .
are not dependency properties .
Returns dependency properties.
are not dependencies .
Returns incidental properties.
using the requirements passed as an argument .
Computes the target path that should be used for
Creates a new property set containing the properties in this one ,
Returns all values of 'feature'.
contained properties associated with 'feature
training recommender models .; Based on simple
Compare the prediction or recommendation performance of recommender models
given cutoff for each user .
Create a recommender-friendly train-test split of the provided data set .
Get the current settings of the model .; depend on the type of
Set current options for a model.
Processes the dataset parameter for type correctness .
used in the
Return a score prediction for the user ids and item ids in the provided
Get the k most similar items for each item in items .
Get the k most similar users for each entry in ` users ` .
Recommend the `` k `` highest scored items for each user .
Recommend the `` k `` highest scored items based on the
Compute a model 's precision and recall scores for a particular dataset .
Evaluate the prediction error for each user-item pair in the given data
matching the data set this model was
For a collection of item -> item pairs, returns information about the
Creates a feature vectorizer from input features , return the spec for
Read in the Boost version from a given boost_root .
mimicks the way Travis-CI clones a project 's repo .
Installs specific toolset on CI system.
Create a : class : ` ~turicreate.svm_classifier.SVMClassifier ` to predict the class of a binary
Load a keras model from disk
displaying the Plot object
saving the Plot object in a vega representation
Get the right value from the scikit-tree
Traverse through the tree and append to the tree spec.
Convert a generic tree regressor model to the protobuf spec .
scaled to [ 0 , 1 ] and returns them appropriately scaled and
Create a : class : ` StyleTransfer ` model .
Takes input and returns tuple of the input in canonical form (SFrame)
Stylize an SFrame of Images given a style index or a list of
takes an image of; Save the model in Core ML format .
used for training the model
Convert an MXNet model to the protobuf spec.
Load a libsvm model from a path on disk .
Annotate an input or output multiArray feature in a Neural Network spec to
Annotate an input or output image feature in a Neural Network spec to
Annotate an input or output Image feature in a Neural Network spec to
Annotate an input or output MLMultiArray feature in a Neural Network spec
given model specification , returns a dictionary with a shape range object for each input feature name .
compute results for more than one output shape .
Returns true if any one of the channel, height, or width ranges of this shape allow more than one input value.
Generate a macro definition or undefinition
Generate the filename
Generates the length limits
Generate the take function
Generate the make_string template
does not exist
Generate the beginning part
Generate the closing part
Calculates the deep features used by the Sound Classifier .
Creates a : class : ` SoundClassifier ` model .
load a previously saved SoundClassifier instance .
Return the classification for each examples in the `` dataset `` .
Save the model in Core ML format .
Return predictions for ``dataset``.
Return top-k predictions for the ``dataset``.
Convert data into canonical form.
provided by this iterator
Registers new generator instance 'g'.
Creates new instance of the 'generator ' class and registers it .
be preferred to
Returns a list of source type which can possibly be converted
caches the result of '__viable_source_types_real ' .
Returns the list of source types , which , when passed to 'run '
Caches the result of 'viable_source_types_for_generator'.
Returns usage requirements + list of created targets.
be pruned , because it 's guaranteed
is not so , exists with; have types .
be used to construct target of specified type
construct target by finding viable generators , running them
create target of 'target-type ' with 'properties '
differers from $ ( self ) in
Creates another generator that is the same as $ ( self ) , except that
be run with the specified
invoke this generator on the given sources .
Constructs the dependency graph that will be returned by this
Determine the name of the produced target from the
are created after consuming 'sources ' .
convert 'source ' to the types that this generator can
Converts several files to consumable types.
Returns the sketch summary for the given set of keys .
Convert a Nu-Support Vector Classification ( NuSVC ) model to the protobuf spec .
Convert a linear regression model to the protobuf spec .
needed information from the top-level DBAPI module ,
multiple CSVs , and
Reads a JSON file representing a table into an SFrame .
Convert the result of a SQL database query to an SFrame.
Convert an SFrame to a single table in a SQL database.
Print the first M rows and N columns of the SFrame in human readable
is an SArray of identical length as the current Frame ,
Convert this SFrame to pandas.DataFrame.
Converts this SFrame to a numpy array
according to a
multiple rows in a new SFrame via a
Sample a fraction of the current SFrame 's rows .
split the rows of an SFrame into two SFrames .
rows according to the given column .; is according to and
Save the SFrame to a file system for later use .
Writes an SFrame to a CSV file .
Writes an SFrame to a JSON file .
existing SFrame into a directory .
Get a reference to the : class : ` ~turicreate.SArray ` that corresponds with
columns where the name of the column or the type of column
Returns an SFrame with a new column.
Returns an SFrame with multiple columns added.
Returns an SFrame with a column removed.
Returns an SFrame with one or more columns removed.
Returns an SFrame with two column positions swapped.
is expected to be a
Add the rows of an SFrame to the end of this SFrame.
Perform a group on the key_column_names followed by aggregations on the
left ) SFrame with the given
Filter an SFrame by values inside an iterable object.
Opens a new app window .
Pack columns of the current SFrame into one single column.
Splits a datetime column of SFrame to multiple columns , with each value in a
multiple columns with each value in
Convert a `` wide '' column of an SFrame to one or two `` tall '' columns by
Concatenate values from one or two columns into one column, grouping by
given columns , using the given sort order .
missing value is either `` None ``; missing values from an SFrame .
rows with missing values from this SFrame .
missing values with a given value in a given column .
Returns an SFrame with a new column that numbers each row
Adds the FileDescriptorProto and its types to this pool .
Adds a Descriptor to the pool , non-recursively .
Adds a ServiceDescriptor to the pool .
Adds a FieldDescriptor describing an extension to the pool .
Adds a FileDescriptor to the pool , non-recursively .
Gets a FileDescriptor by file name .
Gets the FileDescriptor for the file containing the specified symbol .
Loads the named descriptor from the pool .
Loads the named enum descriptor from the pool .
Loads the named field descriptor from the pool .
Loads the named service descriptor from the pool .
Finds the file in descriptor DB containing the specified symbol .
Creates a FileDescriptor from a proto or returns a cached copy .
Adds the proto to the pool in the specified package .
Sets all the descriptor's fields's types.
Sets the field 's type , cpp_type , message_type and enum_type .
Creates a enum value descriptor object from a enum value proto .
Make a protobuf ServiceDescriptor given a ServiceDescriptorProto .
Creates a method descriptor from a MethodDescriptorProto .
Pulls out all the symbols from descriptor protos.
finds dependencies for file protos .
Finds a given type name in the current scope .
Returns the maximum number in 'elements ' .; 'ordered ' for comparisons ,
corresponding element in parallel
Copies the content of the specified message into the current message.
Convert a generic tree model to the protobuf spec .
Convert a one-hot-encoder model to the protobuf spec .
Given a model that takes an array of dimension input_dimension , returns
Convert a scalar multiplication from mxnet to coreml .
Convert a dense layer from mxnet to coreml .
Convert a padding layer from mxnet to coreml .
Convert a UpSampling layer from mxnet to coreml .
Convert highly specific ops
embedding layer from mxnet to coreml .
Convert a scalar add layer from mxnet to coreml .
Convert a scalar multiply layer from mxnet to coreml .
Convert an instance norm layer from mxnet to coreml.
Returns the values stored in the AWS credential environment variables .
Inject aws credentials into s3 url as s3://[aws_id]:[aws_key]:[bucket/][objectkey]
Process user input url string with proper normalization
returns True if the path provided is a directory that has an SFrame or SGraph in it .
Returns the contents type for the provided archive path .
containing the crossproduct of all provided options .
Given url where a Turi Create object is persisted , return the Turi
are equal .
specified temporary file location .
Generate a temporary directory that would not live beyond the lifetime of
Generate a temporary file that would not live beyond the lifetime of
obj can be serialized directly into memory ( via cloudpickle ) this
Returns a list of dictionaries, with the following keys:
Implementation of the parameterization decorators.
given file needs fixing .
used as input when pre-processing MPL-containers in their variadic form , need fixing .
Check if files , used as input when pre-processing MPL-containers in their numbered form , need fixing .
used as input when pre-processing MPL-containers , need fixing .
used as input when pre-processing MPL-containers in their variadic form .
used as input when pre-processing MPL-containers in their numbered form .
used as input when pre-processing MPL-containers .
exists or throws an exception .
The main function.
Create a : class : ` ImageSimilarityModel ` model .
load a previously saved ImageClassifier
retrieve the nearest neighbors from the model 's stored
Create a dependency graph from an ast node .
Extract a code object from a binary pyc file .
Compute the size of a varint value .
Compute the size of a signed varint value .
uses the function compute_value_size to compute the size of
is the size
Returns a sizer for a bytes field.
Returns a sizer for a group field.
Returns a sizer for a message field.
Returns a sizer for extensions of MessageSet.
Returns a sizer for a map field.
does not include tag ) .
signed varint value ( does not include
given integer as a varint and return the bytes .
Return a constructor for an encoder for fields of a particular type .
Return a constructor for an encoder for a fixed-width field .
Return a constructor for an encoder for float fields .
Returns an encoder for a boolean field.
Returns an encoder for a string field.
Returns an encoder for a group field.
Returns an encoder for a message field.
Encoder for extensions of MessageSet.
Convert a Caffe model to Core ML format .
returns the spec with the protobuf kernel for that model .
Append a single row to an SFrame .
rows to an SFrame .
< location > is not set , sets it based on the project data .
Given the list of source targets explicitly passed to 'stage ' , returns the
Initialize the logging configuration for the turicreate package .
Returns all the Turi Create configuration variables that can only
Sets the log level .
Returns all the Turi Create configuration variables that can be set
Configures system behavior at runtime.
saved SGraph binary .
Convert a list of vertices into dataframe .
Convert a list of vertices into an SFrame .
Convert a list of edges into dataframe .
Convert a list of edges into an SFrame .
assuming that vertex ids are stored in _VID_COLUMN .
assuming that source and target ids are stored in _SRC_VID_COLUMN , and _DST_VID_COLUMN respectively .
Using vid_field to identify the id
Using src_field and dst_field to
get_vertices(self, ids=list(), fields={}, format='sframe')
get_edges(self, src_ids=list(), dst_ids=list(), fields={}, format='sframe')
be input as a list of
be input as a list of; edges to the SGraph .
Return a new SGraph with only the selected fields .
Apply a transform function to each edge and its associated source and
is saved in binary format , the; Save the SGraph to disk .
Retrieve the graph neighborhood around a set of vertices , ignoring edge
Return the value for the queried field .
Return a dictionary for the class fields description .
Check if the input is of expected type .
Converts audio waveform into an array of examples for VGGish.
Convenience wrapper around waveform_to_examples() for a common WAV format.
given build request by combining all property_sets which do n't
Return the cross-product of all elements of property_sets , less any
Returns non-conflicting combinations of property sets.
is either implicit value , or
taken from ARGV rule )
Format a human-readable error message from a regex
Generate random characters
Enumerate the templates found in path
Returns the nth character of a character- > occurrence map
Returns the C-formatting of the character
Create the file with the given content
Determine the output filename
Convert a BOOST_METAPARSE_STRING mode document into one with
Instantiates the template
Returns the range for N
matching regex and return the match object
Add a protobuf spec or : py : class : ` models.MLModel ` instance to the pipeline .
Gets the version from google/protobuf/__init__.py
Invokes the Protocol Compiler to generate a _pb2.py from the given
Validate a row label column .
Generate a new column name that is guaranteed not to conflict with an
selecting columns of only valid feature types .
check whether the first elements are lists that
Create a summary string for the accessible fields in a model .
is a valid datatype object and false otherwise .
Translates a user specified datatype to an instance of the ones defined above .
defined in an ast node .
Given a list of objects , reorder them so that the constains specified
mention objects not in 'objects ' .
's no constraint in 'constraints ' where
Helper for as_path, below.
overriding any non-free properties
Interpret all path properties in 'properties' as relative to 'path'
values that start with ' @ ' are
is not valid .
'property ' is conditional property , returns
correspond to any of the given features .
are not met
Returns a modified version of properties with all values of the
include all the elements
Returns a property set which include all
Associate value with properties.
Benchmark one command execution
Benchmark one file
Determine the name + version of the compiler
Enumartes the files in path with the given extension
Format a duration
Do the benchmarking
Plot a diagram
Enumerate all configs in src_dir
Join the list of images into the out file
Plot temporary diagrams
Plot one diagram
specified by the configs
was previously saved .
return a get_default_options function .
Trailing parameters should all be; checking toolset parameters .
get the command to invoke some tool .
is found ,
Given an invocation command ,
find tool ( binary ) named 'name ' in PATH and in
be found either in path
be invoked by 'command ' .
sets the following
returns the location of the "program files" directory on a windows
Returns the command needed to set an environment variable on the current
sets a named shell path variable to the given NATIVE
prepends the given paths to the named path variable on
Given a target , as given to a custom tag rule , returns a string formatted
Registers a configuration.
Returns the value of a configuration parameter .
Sets the value of a configuration parameter .
returns a list of dictionaries with just
Returns the environment for unity_server .
Sets the dll load path so that things are resolved correctly .
Dumps a detailed report of the turicreate/sframe directory structure
Take a classpath of the form :
Returns either sframe or turicreate depending on which library
Returns the file name of the config file from which the environment
Imports the environmental configuration settings from the
Writes an environment variable configuration to the current
Constructs the service class .
Calls the method described by a given method descriptor .
Returns the class of the request protocol message .
Returns the class of the response protocol message .
returns a method that can be set for a service methods .
Constructs the stub class .
The body of all service methods in the generated stub class.
text format .
Print a single field value ( not including name ) .
Returns a protobuf message instance.
Parses a text representation of a protocol message into a message .
Skips over contents (value or message) of a field.
Skips over a complete field (name and value/message).
Skips over a field message.
Skips over a field value.
Consumes an integer number from tokenizer.
Parses an integer.
checking size/signedness .
Parse a floating point number .
Parse an enum value.
is a google.protobuf.Any field .
Print a single field name/value pair .
Merges a text representation of a protocol message into a message.
Converts a text representation of a protocol message into a message.
Merges a single protocol message field into a message.
returns the type name .
Merges a single scalar field into a message.
consume a given piece of text .
returns a 2-tuple ( trailing bool , comment str ) .
Consumes protocol message field identifier.
Consumes an integer number.
Consumes a string value.
Consumes a byte array value.
Reads the next meaningful token .
Create a : class : ` ~turicreate.decision_tree_regression.DecisionTreeRegression ` to predict
Compute the value of a composite distance function on two dictionaries ,
Do n't modify the; Check that composite distance function is in valid form .
lists in a composite distance
Convert function names in a composite distance function into function
matching address data .
Builds a dictionary of all the messages available in a set of files.
based on the passed in descriptor .
specified file .
for if statements in list comprehension
Initializes an MpsGraphAPI for object detection.
Create a : class : ` ObjectDetector ` model .
be returned .
Predict object instances in an sframe of images.
Evaluate the model by making predictions and comparing these to ground
Parse a document title as the first line starting in [ A-Za-z0-9 < ]
written function to for error reporting .
wrap the locator
Create a parser context for an HTML in-memory document .
build a tree .
parse an XML file from the filesystem or the network.
Creates a new HTML document
Creates a new HTML document without a DTD node if @ URI and
overwrite existing but
build the associated data structures .
expand CATALOG or
create a new Catalog .
shorten it if necessary
Check whether this name is an predefined entity .
be done by
Creates a parser context for an XML in-memory document .
parse an external subset .
parse an XML external entity out of context and build a
append the char value in the array
Create a parser context for an external entity Automatic
Create a parser context for a file content .
Create a parser context for an XML in-memory document .
Create a parser context for a file or URL content .
Pops the top element name from the name stack
Pushes a new element name on top of the name stack
Pops the top element node from the node stack
Pushes a new element node on top of the node stack
Create a libxml2 input buffer from a Python file
Create a libxml2 output buffer from a Python file
Create a progressive XML parser context to build either an
Create a progressive HTML parser context to build either an
Create a new Node
Create an XML RelaxNGs parse context for that memory buffer
Create an XML RelaxNGs parse context for that file/resource
Builds the QName @prefix:@ncname in @memory if there is
containing a comment .
Creates a new XML document
Creation of a processing instruction element.
Creation of a new text node.
Creation of a new text node with an extra parameter for the
Unescaping routine , but does not check that the string is
based on RFC 3986 URI-reference = [
allows to keep intact the original
fed with the resource at
Create an xmltextReader for an XML in-memory document.
Create an xmltextReader for an XML from a file descriptor.
Parses a regular expression conforming to XML Schemas Part
Create an XML Schemas parse context for that memory buffer
Create an XML Schemas parse context for that file/resource
Create a substring from a given UTF-8 string Note :
Pops the top XPath object from the value stack
Remove a namespace definition from a node .; href is None ,
be called back as
warning handlers for DTD validation .
warning handlers for Schema validation .
warning handlers for RelaxNG validation .
registered with setErrorHandler
Get the namespace of a node
Dumps debug information for the element node, it is
Dumps debug information for the list of element node, it is
Dumps debug information for the element node, it is not
Add a new node to @ parent , at the end of the child ( or
Add a list of node at the end of the child list of the
Append the extra substring to the node content .
Add a new node @ elem as the next sibling of @ cur If the new
Add a new node @ elem as the previous sibling of @ cur
Add a new element @ elem to the list of siblings of @ cur
Do a copy of the node .
Do a recursive copy of the node list .
Do a copy of the attribute .
Do a copy of an attribute list .
Do a copy of the node to a given document .
Set the root element of the document (doc->children is a
Finds the first child node of that element which is a
work on both XML
associated to a node This attribute
associated to a node This function also
Search the last child of a node.
Finds the last child node of that element which is a
contained in the
Build the string equivalent to the text contained in the
added at the end of
Creation of a new Namespace.
Create a new property tagged with a namespace and carried
Create a new property carried by a node .
Finds the first closest next sibling of the node which is
get the value of an attribute associated to a
Finds the first closest previous sibling of the node which
checks that all the namespaces declared
prune the new
registered under a given name space for a
aliasing a given URI .
Replace the content of a node.
update all nodes in the list to point to the right document
Associate a namespace to a node , a posteriori .
reset ) an attribute carried by a node .
reset ) an attribute carried by a node .; @ name has
update all nodes under the tree to point to the right
given string at the end of the existing node
Merge two text nodes into one
Remove an attribute carried by a node .
is of type ID .
is of type Ref .
Does the validation related extra step of the normalization
Implement the XInclude substitution for the given subtree
Validate a branch of a tree , starting with the given @ elem .
w.r.t document order
Evaluate the XPath Location Path in the given context .
Create a new xmlXPathObjectPtr of type NodeSet and
Create a new xmlXPathObjectPtr of type Value Tree ( XSLT )
Traversal function for the "ancestor" direction the
Traversal function for the "ancestor-or-self" direction he
Traversal function for the "attribute" direction TODO:
Traversal function for the "child" direction The child axis
Traversal function for the "descendant" direction the
Traversal function for the "descendant-or-self" direction
Traversal function for the "following" direction The
Traversal function for the "following-sibling" direction
Traversal function for the "namespace" direction the
Traversal function for the "parent" direction The parent
Traversal function for the "preceding" direction the
Traversal function for the "preceding-sibling" direction
Traversal function for the "self" direction The self axis
Create a new xmlXPathObjectPtr of type range using a single
Create a new XPointer context
Create a new xmlXPathObjectPtr of type LocationSet and
Create a new xmlXPathObjectPtr of type range
Create a new xmlXPathObjectPtr of type range using 2 nodes
allows a tag to implicitly close other tags .
Dump an HTML document.
Formating return/spaces are added .
Dump an HTML document to an open FILE.
Dump an HTML node, recursive behaviour,children are printed
is `` - '' the
using a given encoding and
using a given encoding .
Sets the current encoding in the Meta tags NOTE : this will
Check the document for potential content problems , and
Register a new entity for this document.
Register a new entity for this document DTD external subset.
Do an entity lookup in the document entity hash table and
Do an entity lookup in the DTD entity hash table and
remove xmlEncodeEntities , once we are not afraid of
Do a global encoding of a string , replacing the predefined
Create a new entity , this differs from xmlAddDocEntity ( )
Do an entity lookup in the internal and external subsets and
Create an XML RelaxNGs parser context for that document.
Validate a document tree in memory .
Validate a full subtree when
Pop the element end from the RelaxNG validation stack.
Push a new element start on the RelaxNG validation stack .
Do a copy of the document info .
Create the internal subset of a document
Dump an XML document to an open FILE.
Dump an XML/HTML node, recursive behaviour, children are
Get the root element of the document ( doc- > children is a
Get the internal subset of a document
containing a CDATA block .
Creation of a new character reference node.
containing a comment within a
Creation of a new Fragment node.
Creation of a new node element within a document.
Create a new property carried by a document .
Creation of a new text node within a document.
Creation of a new text node with an extra content length
Creation of a new DTD for the external subset.
using PI and without
Creation of a new reference node.
Dump an XML node, recursive behaviour, children are printed
use compression if
converting it to the given encoding
Dump an XML document to an I/O buffer.
Dump an XML document to a file or an URL.
Parse the value string and build the node list associated .
declaring the given ID
Search in the DtDs whether an element accept Mixed content
Remove the given attribute from the ID table maintained
Remove the given attribute from the Ref table maintained
Try to validate the document instance basically it does
Does the final step for the document validation once all
Try to validate the document against the dtd instance
Does the final step for the dtds validation once all the
Try to validate the subtree under an element
given name match a notation declaration .
Try to validate a single attribute for an element basically
Try to validate a single element and it 's attributes ,
Try to validate a single namespace declaration for an
Pop the element end from the validation stack.
Push a new element start on the validation stack .
Try to validate a the root element basically it does the
Implement the XInclude substitution on the XML document @doc
parse a preparsed XML document .
Create an xmltextReader for a preparsed document.
Create an XML Schemas parse context for that document.
Create a new xmlXPathContext
Get the document tree from a parser context .
Applies the options to the parser context
Parse a Chunk of memory
Reset a push parser context
parse a new buffer ; Clears any
is deprecated , we now always process entities
Default handling of defined entities, when should we define
references declarations [ 68 ] EntityRef : :=
[ 30 ]
Takes a entity string content and process to do the
Dumps debug information for the attribute
Dumps debug information for the attribute list
Remove an entry from the catalog
Do a complete resolution lookup of an External Identifier
Try to lookup the catalog local reference associated to a
Try to lookup the catalog resource for a system ID
Do a complete resolution lookup of an URI
Do a copy of the dtd .
Search the DTD for the description of this attribute on
Search the DTD for the description of this element
Search the DTD for the description of this qualified
Save the original error to the new place .
Do a copy of the namespace .
Do a copy of an namespace list .
is optional ( None ) .
Write the content of the array in the output I/O buffer
Write the content of the string in the output I/O buffer
Grow up the content of the input buffer , the old data are
Push the content of the arry in the input buffer This
Refresh the content of the input buffer , the old data are
Setup an XML reader with new options
fed with @ input
Check if the regular expression generates the value
parse a schema definition resource and build an internal
used to pass informations to a parser
validate the document as it is processed .
check the CData parsed for validation in the current stack
validate the document as it
based on the given
schema context to validate the document as it
Sets the options to be used during the validation .
allow access to the parser context of the schema validation
Do a schemas validation of the given resource , it will use
Hacking interface allowing to get the xmlDocPtr
Hacking interface allowing to get the xmlNodePtr
Reads the contents of the current node and the full
Provides the value of the attribute with the specified
Provides the value of the specified attribute
Read the parser internal property .
get the remainder of the buffered XML .
Resolves a namespace prefix in the scope of the current
Moves the position of the current instance to the attribute
parse an XML in-memory document .
parse an XML from a file
tells the XML Reader to preserve the current node .
validate the document as it is
Change the parser processing behaviour by changing some of
Get an interned string from the reader , allows for example
string based on RFC 3986 and fills
Get the doc from an xpathContext
Get the current node from an xpathContext
Set the doc of an xpathContext
Set the current node of an xpathContext
written function to the XPath interpreter
Register a variable with the XPath context
Creates/frees an object cache on the XPath context.
Evaluate the XPath expression in the given context .
Create a new xmlXPathParserContext
Search in the namespace declaration array of the context
is None it unregisters
Search in the Variable array of the context for the given
Get the xpathContext from an xpathParserContext
Implement the compare operation on XPath objects: @arg1 <
Formats an error message.
Appends the suffix appropriate to 'type/property-set ' combination
Traverses the dependency graph of 'target ' and return all targets that will
creates new instance of it
's already registered target , with the same
Creates a virtual target with appropriate name and type from 'file ' .
Appends the suffix appropriate to 'type/property_set ' combination
Adds additional instances of 'VirtualTarget' that this
Generates all the actual targets and sets up build actions for
generating target name , it will override any path; Sets the path .
is root is it directly correspods to some; Sets/gets the 'root ' flag .
sets the subvariant which created this target .
used to distinguish; 'actual_name ' , above .
Given the target name specified in constructor , returns the
Returns the directory for this target .
Generates actual build instructions .
Helper for 'actualize_sources'.
Creates actual jam targets for sources.
referenced by this subvariant ,
specify implicit include paths to
Compare if two nodes are equal .
Creates additional files for the individual MPL-containers .
Creates additional source- and header-files for the numbered sequence MPL-containers.
Adjusts the limits of variadic sequence MPL-containers .
is located in ( if any ) .
throws an exception .
Add an inner product layer to the model.
resize bilinear layer to the model .; resizes the input to a given spatial size using bilinear interpolation .
extracts cropped spatial patches or RoIs ( regions of interest ); resize layer to the model .
Serialize model summary into a dict with ordered lists of sections and section titles
Format a doc-string on the fly.
Finds the only column in ` SFrame ` with a type specified by ` target_type ` .
Finds the only column in ` sframe ` with a type of turicreate.Image .
Finds the only column that can be interpreted as a drawing feature column .
Convert the Json Tree to SGraph
Return a tuple of sections and section titles .
Returns a tuple of the top k values from the positive and
Extract a model summary field value
printed , creates a well-formatted table .
Display a toolkit repr according to some simple rules .
returning value , if it is unity SFrame , SArray , map it
Same as select columns but redirect runtime error to ToolkitError.
Check if a column exists in an SFrame with error message .
Check whether or not the requested option is one of the allowed values .
Provide a proper error; Check if the input is an SArray .
Provide a proper error; Check if the input is an SFrame .
Check if the input is empty .
is within given range
canonicalize training and validation data .
is not specified , a column is; Validate a row label column .
making it easy to do proper
Print a message making it clear to the user what compute resource is used in
Get a proto class from the MessageFactory by name .
Create a Protobuf class whose fields are basic types .
Populate FileDescriptorProto for MessageFactory's DescriptorPool.
making sure information all models should
making sure information all models should have
Converts name to Json name and returns it.
Sets the descriptor 's options
Retrieves descriptor options.
matching proto in descriptor_pb2 .
Returns the string name of an enum value .
Given a target_reference , made in context of 'project ' ,
generate the target given by target reference , which
Registers the specified target as a main target alternatives.
Return the list of sources to use , if main target rule is invoked
Returns the requirement to use when declaring a main target ,
use when declaraing a main target ,
Return the default build value to use when declaring a main target ,
detect cycles in main target references .
Creates a TypedTarget with the specified properties .
Generates all possible targets contained in this project .
returns a list of AbstractTarget instances which
Add 'target' to the list of targets in this project
Add new target alternative.
specified name exists .
corresponding to the 'name ' .
return the target with the specified id , treated
Adds a new constant for this project .
Add a new alternative for this target .
Returns the best viable alternative for this property_set
finding all alternatives
Generates the main target with the given property set
Returns the list of AbstractTargets which are used as sources .
Given build request and requirements , return properties
Returns the alternative condition for this alternative , if
Takes a target reference , which might be either target id
Determines final build properties, generates sources,
Given the set of generated targets , and refined build
Creates a new subvariant-dg instances for 'targets '
Declares a new variant .
declared by this module .
The implementation of the 'lib' rule.
For all virtual targets for the same dependency graph as self,
Create a model that makes recommendations using item popularity .
Transform a string by bracketing it with `` < > '' .; bracketed , does nothing .
Replaces the grist of a string by a new one.
Gets the value of a property , that is , the part following the grist , if any .
Returns the grist of a string .
Returns the value without grist .
Replaces the suffix of name by new_suffix.
Splits an id in the toolset and specific rule parts.
running on windows , whether in cygwin or not .
Validate the main Kmeans dataset .
Validate the initial centers .
Validate the combination of the ` num_clusters ` and ` initial_centers `
Identify the subset of desired ` features ` that are valid for the Kmeans
Create a k-means clustering model .
predicted cluster label for instances in the new 'dataset ' .
is an SArray of strings or an SArray of lists of strings , the
contains the count
Compute the TF-IDF scores for each word in each document .
Remove words that occur below a certain number of times in an SArray .
Tokenize the input SArray of text strings and return the list of tokens .
given query and set of documents , compute the BM25 score for each
Parse a file that 's in libSVM format .
consists of a 3-line header; Parse a file that 's in `` docword '' format .
performing a random split for text data that is already in
given paramaters .
Create a : class : ` ImageClassifier ` model .
Save the model as a dictionary , which can be loaded with the
using the trained logistic
Remove the layer , and reconnect each of its predecessor to each of
represents a fixed frequency datetime index .
Constructs an SArray of size with a const value.
from_sequence(start=0, stop)
Construct an SArray from a json file or glob of json files.
depending on the value
Saves the SArray to file .
contains vectors or lists , this returns a new SArray
returns an SArray with each element sliced accordingly to the
returns an SArray with , for each input string , a dict from the unique ,
see turicreate.text_analytics.count_ngrams ( ) .
given keys .
given range ( inclusive ) .
Create a boolean SArray by checking the keys of an SArray of
apply(fn, dtype=None, skip_na=True, seed=None)
Filter this SArray by a function.
contains a subsample of the current SArray .
Returns an SArray with a hash of each element.
Returns an SArray with random integer values.
Get the index of the minimum numeric value in SArray .
Mean of all the values in the SArray, or mean image.
Create a new SArray with all the values cast to str .
Create a new SArray with all the values cast to datetime .
Create a new SArray with all the values cast to : py : class : ` turicreate.image.Image `
Create a new SArray with all values cast to the given type .
Create a new SArray with each value clipped to be within the given
clipped to the given lower bound .
Get an SArray that contains the last n elements in the SArray .
missing values ( None or NaN ) filled in
are in the top k .
be calculated with one pass over the SArray .
Creates a new SArray with the
Get all unique values in the current SArray .
Visualize the SArray .
Create a Plot object representing the SArray .
Length of each element in the current SArray.
split the rows of an SArray into two SArrays .
multiple columns , return a
Convert a `` wide '' SArray to one or two `` tall '' columns in an SFrame by
dict type to an SFrame with
Sort all values in this SArray.
Calculate a new SArray of the sum of different subsets over this
Calculate a new SArray of the maximum value of different subsets over
Count the number of non-NULL values of different subsets over this
Return the cumulative sum of the elements in the SArray .
Return the cumulative mean of the elements in the SArray .
Return the cumulative minimum value of the elements in the SArray .
Return the cumulative maximum value of the elements in the SArray .
Return the cumulative standard deviation of the elements in the SArray .
Return the cumulative variance of the elements in the SArray .
is an SArray that
Run the doxygen make commands if we 're on the ReadTheDocs server
protobuf message to JSON format .
protobuf message to a JSON dictionary .
Parses a JSON dictionary representation into a message .
Convert a single scalar field value .
Convert an integer.
Convert an floating point number.
Convert a boolean value .
according to Proto3 JSON Specification .
Convert a JSON object into a message .
pairs into regular message .
Convert a JSON representation into Any message .
Convert a JSON representation into Wrapper message .
Convert map field value for a message map field.
Plots the data in ` x ` on the X axis and the data in ` y ` on the Y axis
Plots a columnwise summary of the sframe provided as input ,
Plots a histogram of the sarray provided as input , and returns the
provided as input , and returns the
Parses the input file and returns C code and corresponding header file .
Creates the name inside an enumeration for distinguishing data
add indentation to each entry and prints it .
Prints the tag definitions for a structure.
Builtin approximate quantile aggregator for groupby.
Return a model object with; Compute the K-core decomposition of the graph .
Raise an error if an option is not supported .
Given a list of class labels and a list of output_features , validate the
features into a standard form from a number of different possible forms .
Performs python-side bootstrapping of Boost.Build/Python.
The main function of the utility
were appended either by ` append ` or
be set on targets under certain
is a subset of
Brings all flag definitions from the 'base' toolset into the 'toolset'
Given a rule name and a property set , returns a list of tuples of
Adds a new flag setting with the specified values .
Convert a Logistic Regression model to the protobuf spec .
'path ' is relative , it is rooted at 'root ' .; 's unchanged .
Returns path2 such that `os.path.join(path, path2) == '.
Returns the list of files matching the given pattern in the
Recursive version of GLOB.
glob sall parent directories
walks each thing in val , opening lists and dictionaries ,
ensures that anything; used only by _publish ( ) .
Dispatches arguments to a toolkit function.
is of the form modA.modB.modC.class module_path splits on `` . ''
has already been
registered in unity_server .
Loads a turicreate toolkit module ( a shared library ) into the
Given a toolkit function name , return the argument list
Given a globals dictionary , and a name of the form `` a.b.c.d '' , recursively
fn can be interpreted and handled as a native function : i.e .
have to see if fullname refers to a module we can import .
Print lines of input along with output.
Returns a type checker for a message field of the specified types.
check the provided value and return it .
Generate executable python source code from an ast node.
Escape a bytes string for use in an ascii protocol buffer .
Unescape a text string with C-style escape sequences to UTF-8 bytes.
derived from a 'base-type ' .
have the type 'type ' .
be used for this 'type ' .
'type ' and 'property_set ' .
Returns type and all of its bases, in the order of their distance from type.
derive from it , in the order of their distance from type .
is 'base ' or has 'base ' as its direct or indirect base .
be used when generating target
Change the suffix previously registered for this type/properties
suffix that should be used when generating target of 'type ' ,
file type given it 's name .; are several dots in filename ,
given type on the specified OSes , or on remaining OSes
be the names passed to __init__ ( ... ) as ` column_names `
be readable .
has the same element in each ( H , W ) matrix for each channel
representing the ast .
Pretty print an ast node.
Extract internal data from pd.DataFrame
Get float property from the DMatrix.
Get unsigned integer property from the DMatrix.
Save DMatrix to an XGBoost buffer.
Get the number of rows in the DMatrix .
Get the number of columns ( features ) in the DMatrix .
return a new DMatrix that only contains ` rindex ` .
calculated internally .
Boost the booster for one iteration , with customized gradient statistics .
Evaluate a set of data .
Save the model to a in memory buffer represetation
Dump model into a text file.
Returns the dump the model as a list of strings .
agains the 'pattern '
Replaces occurrences of a match string in a given
given list of strings and returns
Create a topic model from the given data set .
Compute the perplexity of a set of test documents given a set
Get the words associated with a given topic .
Use the model to predict topics for each document .
Estimate the model 's ability to predict new data .
bounding box to center/shape
provided as input to
Create a : class : ` DrawingClassifier ` model .
takes a grayscale; Save the model in Core ML format .
Predict with probabilities.
Predict on an SFrame or SArray of drawings, or on a single drawing.
Takes an mxnet parameter collect (from Block.collect_params()) and
containing a bag of words representation of each column .
Create a model that trains a classifier to classify text from a
are string type .
Convert a Support Vector Regressor ( SVR ) model to the protobuf spec .
Verify that the given extension handle is valid .
defined in this message .
returns a default value for a field .
Re-raise the currently-handled TypeError with the field name added .
Adds an __init__ method to cls .
Returns a field descriptor by field name.
Adds properties for all fields in this protocol message type.
Adds a public property for a protocol message field .
Adds a public property for a `` repeated '' protocol message field .
Adds a public property for a nonrepeated , scalar protocol message field .
Given a ( FieldDescriptor , value ) tuple from _fields , return true if the
Helper for _AddMessageMethods().
returns the unpacked message .
Returns the number of bytes needed to serialize a non-repeated element .
Adds the IsInitialized and FindInitializationError methods to the
Adds implementations of all Message methods to cls.
Adds implementation of private helper methods to cls.
updates the state of the containing oneof in the parent message .
containing the name of an enum value .
Returns the value coresponding to the given enum name .
Return a list of the ( name , value ) pairs of the enum .
Load global singleton of tcmps lib handler.
True if the environment has MPS backend support
be used , else None .
Returns the memory size in bytes that can be effectively allocated on the
Copy the shape from TCMPS as a new numpy ndarray .
Copy the data from TCMPS into a new numpy ndarray
Submits an input batch to the model.
followed by a backward
deserialises the message ,
Runs a function defined by a message object with keys :
decorating a function with @ task , you can just run it directly .
Async task decorator so that running
Given a modular path to a function , import that module
Format the modular task path for a function via inspection.
Get the response from the async table
Create the message object and pass it to the actual sender .
Given a message , directly invoke the lamdba function for this task .
Given a message , publish to this topic .
Parses S3 URL.
returns an int timestamp .
try to discover Django settings files ,
try to discover Flask apps files ,
Given an event_source dictionary , create the object and add the event source .
Given an event_source dictionary , create the object and remove the event source .
Given an event_source dictionary , create the object and get the event source status .
Validate name for AWS Lambda function.
contains .py or .pyc files
lies in the same directory as a .py file with the same name .
is valid according to https : //docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html # bucketnamingrules
Merge the values of headers and multiValueHeaders into a single dict .
Given some event_info via API Gateway ,
Given the WSGI environ and the response ,
Puts the project files from S3 in /tmp and adds to path
read a file from s3 containing a flat json object .
Given a function and event context ,
Get the associated function to execute for a triggered AWS event
given event build ARN and return the configured function
Get the associated function to execute for a cognito trigger
parses specific API Gateway input into a
validate the incoming token
Adds a method to the internal lists of allowed or denied methods .
Adds an API Gateway method ( Http verb + Resource path ) to the list of allowed
Adds an API Gateway method ( Http verb + Resource path ) to the list of denied
Generates the policy document based on the internal lists of allowed and denied
testing and bootleg cloud ) deployments
apply configuration options to boto clients
apply configuration options to boto resources
cached as a parameter .
given package , returns a list of required packages .
are then downloaded .; brings it into a fresh virtualenv-like folder .
Returns the path to the current virtualenv
Create a Lambda-ready zip file of the current virtualenvironment and working directory .
given path .; Assumes the package exists in lambda packages .
cares about .
given package version binary should be copied over from lambda packages .
Downloads a given url in chunks and writes to the provided stream ( can be any io stream ) .
does not exist , the function downloads it .; Gets the locally stored version of a manylinux wheel .
given package name , returns a link to the download URL ,
src file to destination within a bucket .
Given a file name and a bucket , remove it from S3 .
Given a bucket and key ( or a local path ) of a valid Lambda-zip , a function name and a handler , register that Lambda function .
Given a bucket and key ( or a local path ) of a valid Lambda-zip , a function name and a handler , update that Lambda function 's code .
Given an existing function ARN , update the configuration variables .
invoke a named Lambda function with a payload .
Rollback the lambda function code 'versions_back ' number of revisions .
Returns the lambda function ARN , given a name
returns the versions available for a Lambda function , given a function name .
The `zappa deploy` functionality for ALB infrastructure.
The `zappa undeploy` functionality for ALB infrastructure.
Create the API Gateway for this Zappa deployment .
Create Authorizer for API gateway
given API Gateway resource .
Deploy the API Gateway!
Remove binary support
Add Rest API compression
allows to iterate per API keys associated to an api_id and a stage_name .
link it with an api_id and a stage_name
Remove a generated API key for api_id and stage_name
Add api stage to Api key
describes a change of configuration on the given staging .
allows to iterate per every available apis .
Delete a deployed REST API Gateway.
Update CloudWatch metrics configuration.
managed by Zappa .
Build the entire CF stack .
create the CF stack managed by Zappa .
Given a name , describes CloudFront stacks and returns dict of the stack Outputs
Given a lambda_name and stage_name , return a valid API URL .
Given a lambda_name , return the API id .
Creates the API GW domain and returns the resulting DNS name .
following GW domain creation
updates your certificate information for an existing domain ,
Update domain base path mapping on API Gateway if it was changed
handling pagination .
Given our role name , get and set the credentials_arn .
defines the IAM roles and policies necessary for Zappa .
prevent policy from bloating over the limit after repeated updates .
link to an event .
Given a Lambda ARN , name and a list of events , schedule this as CloudWatch Events .
Returns an AWS-valid Lambda event name.
using a digest of the event name , lambda name , and function .
Delete a CWE rule.
associated with a lambda function .
associated with this function .
Given a list of events , unschedule these CloudWatch Events .
Create the SNS-based async topic .
Remove the async SNS topic .
Create the DynamoDB table for async task return values
Fetch the CloudWatch logs for a given Lambda name .
match the name given in log_filter .
Removed all logs that are assigned to a given rest api id .
Get the Hosted Zone ID for a given domain .
is closer matched with domain name .
Remove DNS challenge TXT.
Load AWS credentials.
Main cert installer path.
get public key
Parse certificate signing request for domains
find crypto values from parsed account key ,
Agree to LE TOS
Call LE to get a new signed CA .
is verified , else fail .
Get the new certificate .
encoded cert file .
make signed requests to Boulder
promote our little community .
Main program execution handler.
A shortcut property for settings of a stage.
set by zappa_settings ( for the current stage only )
Main function.
Given a command to execute and stage ,
build the package
build the template file .
Package your project , upload it to S3 , register the Lambda function
update the function code .
Rollsback the currently deploy lambda code to a previous revision .
Tail this function's logs.
existing deployment .
Update any cognito triggers
Given a a list of functions and a schedule to execute them ,
Given a a list of scheduled functions ,
Invoke a remote function.
Formats correctly the string output from the invoke() method,
return a colorized version the invoke
Describe the status of the current deployment.
sure the environment contains only strings
Initialize a new Zappa project by creating a new zappa_settings.json in a guided process .
update a domain certificate for this env .
Spawn a debug shell.
Allows the execution of custom code between creation of the zip file and deployment to AWS .
Print a warning if there 's a new Zappa version available .
Load the local zappa_settings file.
Return zappa_settings path as JSON or YAML (or TOML), as appropriate.
Load our settings file.
be properly configured ,
Remove our local zip file .
Remove the local and S3 zip file after uploading and updating .
Cleanup after the command finishes.
Parse, filter and print logs to the console.
is an HTTP-formatted log string or not .
return a colorized version of a string .
execute the prebuild_script from the zappa_settings .
Given a string , print a warning if this could
're inside a virtualenv .
Route all stdout to null.
Test the deployed endpoint with a GET request .
given string .
return a dict
Get encoding from request headers or page head .
encoding of Response.content.
Returns the json-encoded content of the response , if any .
Returns a PyQuery object of the response's content
be selected by xpath
stored : class : ` HTTPError ` or : class : ` URLError ` , if one occurred .
send process status package back to scheduler .
setting the default kwargs of ` BaseHandler.crawl ` .
been called every minutes or seconds
Catch errors of rabbitmq then reconnect
Reconnect to rabbitmq server
Put a task into task queue
Get a task from queue when bucket available
Mark task done
is in processing
handler the log records to formatted string
Deal one task
create connection to message queue
stack traceback of given stack
return a Process object
is utf8 encoded bytes .
is unicode type , decode with given encoding if it 's not .
is unicode , try to decode with utf8 , or unicode escaped string if failed .
is unicode , try to default with utf8 , or base64 if failed .
is unicode .
is unicode .; encode in base64 .
encoded by ` unicode_string `
unicoded dict/list/tuple encoded by ` unicode_obj `
Load object from module
Return a interactive python console instance with caller 's stack
Start a interactive python console with caller 's stack
XMLRPC service for windmill browser core to communicate with
Handles the HTTP POST request .
create database object by url
Check project update
update one project
load tasks from database
is not in task dict
put task to task queue
dispatch task to fetcher
Check status queue
Check new task queue
cronjob tick , return True when a new tick is sended
fetch & process
Dump counters to file
Dump counters every 60 seconds
Check project delete
Set quit signal
fetcher , once
Start scheduler loop
Start xmlrpc interface
Called when a new request is arrived
Called when a crawled task is arrived
Called when a status pack is arrived
Called when a task is done and success , called by ` on_task_status `
Called when a task is failed , called by ` on_task_status `
Called when a task is selected to fetch & process
interactive mode of select tasks
processing error in interactive mode
Build project script as module
Check if project_name need update
Check projects by last update time
Update one project from database
Load project into self.projects from project info dict
return None if not exists
make cookie python 3 version use this instead of getheaders
Increment the counter by n (default = 1)
refresh one or more index , making all operations
Send fetch result to processor
Do one fetch
used in xmlrpc thread
A fake fetcher for dataurl
Fetch with phantomjs proxy
Run xmlrpc server
Called after task fetched
Dump counters as a dict
Set value of a counter by counter key
Clear not used counters
Load counters to file
A powerful spider system in python.
is allowed .
Run result worker.
phantomjs fetcher if phantomjs is installed .
Run all the components in subprocess or thread
Run Benchmark test.
means all-in-one , it runs every thing in one process over
project from command line
is sys.stdout ] .
Format a Python object into a pretty-printed representation.
returning a string
Called every result
Get the number of tokens in bucket
Migrate tool for pyspider
Encode data to DataURL
Decode DataURL data
Build the actual URL to use .
Quote non-ascii characters
unzips them to the provided path
is the standard LeNet model : from data to the softmax prediction .
Adds an accuracy op to the model
training operators to the model .
adds a few bookkeeping operators that we can inspect later .
Get loss function of VAE.
Trains the agent on the given environment .
Processes an entire step by applying the processor to the observation , reward , and info arguments .
Return current annealing value
Choose an action to perform
Return configurations of LinearAnnealedPolicy
Return the selected action
Return configurations of EpsGreedyQPolicy
Return configurations of BoltzmannQPolicy
Return configurations of MaxBoltzmannQPolicy
Return configurations of BoltzmannGumbelQPolicy
Set environment for each callback in callbackList
Called at beginning of each episode for each callback in callbackList
Called at end of each episode for each callback in callbackList
Called at beginning of each step for each callback in callbackList
Called at end of each step for each callback in callbackList
Called at beginning of each action for each callback in callbackList
Called at end of each action for each callback in callbackList
beginning of training
Print training time at end of training
Reset environment variables at beginning of each episode
Compute and print training statistics of the episode when done
Update statistics of episode after each step
Print metrics if interval is over
Update progression bar at the end of each step
Initialize metrics at the beginning of each episode
Compute and print metrics at the end of each episode
Save metrics in a json file
Save weights at interval steps during training
Return a sample of ( size ) unique elements between low and high
Return an array of zeros with same shape as given observation
Return list of last observations
Return a randomized batch of experiences
Append an observation to the memory
Return configurations of SequentialMemory
Return a randomized batch of params and rewards
Append a reward to the memory
Closes the current episode , sums up rewards and stores the parameters
Create a wrapped , SubprocVecEnv for Gym Environments .
shared by `` local invoke '' and `` local start-api '' commands
is more complex than what Click can handle .
Click Option for template option
creates the tarball of the Docker Context to use for building the image
starts the Local Lambda Invoke service .; block until the service is stopped
function information from the given dictionary of SAM/CloudFormation resources .
Converts a AWS::Serverless::Function resource to a Function configuration usable by the provider.
Extracts the SAM Function CodeUri from the Resource Properties
Extracts the Lambda Function Code from the Resource Properties
Creates a list of Layer objects that are represented by the resources and the list of layers
Resolves the values from different sources and returns a dict of environment variables to use when running
Returns the AWS specific environment variables that should be available in the Lambda runtime .
is a list or dictionary ,; stringifies values of environment variables .
creates the Docker container instance .; Creating the container does * not * run the container .
Removes a container that was created earlier .
be created at the first place to run .; start the container .
Based on the data returned from the Container output , via the iterator , write it to the appropriate streams
\b
separated out for unit testing purposes
Implementation of the ``cli`` method
Parses a swagger document and returns a list of APIs configured in the document .
parse the Lambda Function name from the Integration defined in the method configuration .
given CloudWatch Logs Event dictionary as necessary and returns an iterable that will
convert an event object to string
known Lambda error cases in red :
drawing an underline
is a JSON string , then pretty print the JSON with 2 indents and sort the keys .
Read the template file , parse it as JSON/YAML and return the template as a dictionary .
Move the SAM/CloudFormation template from `` src_template_path `` to `` dest_template_path `` .
contain certain properties whose value is a relative path to a local file/folder .
be present at any part of the template ,
given `` path `` is a relative path , then assume it is relative to `` original_root `` .
is an AWS : :Include data , then parse and return the location of the included file .
fail to retrieve or parse the Swagger; Gets the Swagger document from either of the given locations .
Read the Swagger document from DefinitionBody .; be an inline Swagger dictionary or an
given local or remote location and return it
Download a file from given S3 location , if available .
Parses the given location input as a S3 Location and returns the file 's bucket , key and version as separate
logging if necessary .
creating a new session based on values set in the context .
project using cookiecutter and options given
given date to UTC , if the date contains a timezone .
supports in almost any string formats .; Parse the given string as datetime object .
is provided , this method will return name of
running Lambda functions locally
Returns stream writer for stdout to output Lambda function logs to
Returns stream writer for stderr to output Lambda function errors to
Get the working directory .; is usually relative to the directory that contains the template .
provided a file containing values of environment variables , this method will read the file and
Creates a DebugContext if the InvokeContext is in a debugging mode
multiplexed into one stream of response from the Docker API .
given socket , reads and yields payload of the given size .; do n't receive all data at
Configures --debug option for CLI
Configures --region option for CLI
Configures --profile option for CLI
Creates a Lambda Service ResourceNotFound Response
Creates a Lambda Service InvalidRequestContent Response
Creates a Lambda Service UnsupportedMediaType Response
Creates a Lambda Service Generic ServiceException Response
Creates a Lambda Service Generic PathNotFound Response
Creates a Lambda Service Generic MethodNotAllowed Response
Reads the file ( json and yaml supported ) provided and returns the dictionary representation of the file .
resolved based on current working directory .
defined path to one that is accepted by Flask
defined path to one that is accepted by Api Gateway
is a best effort service which returns None; Gets the name of the function from the Integration URI ARN .
be expressed in various shapes and forms .; normalizes the Integration URI ARN
Given the integration ARN , extract the Lambda function name from the ARN .
tries to resolve; resolve an Integration URI which contains Fn : :Sub intrinsic function .
Is this input data a Fn : :Sub intrinsic function
is provided , read the event from stdin .; Read the event JSON data from the given file .
Normalize all Resources in the template with the Metadata Key on the resource .
Replace a property with an asset on a given resource
is the command; Extract the command name from package name .
method from `` click.MultiCommand `` that returns Click CLI object for given command name , if found .
specified text to the underlying stream
examines contents of the project; Get a workflow config that corresponds to the runtime provided .
Given a workflow config , this method provides a boolean on whether the workflow can run within a container or not .
Finds a configuration by looking for a manifest in the given directories .
parse CloudFormation intrinsics .
Parse a yaml string
reads the encoding type from the event-mapping.json
opens the event json , substitutes the values in , and
Underline the input
add colors to input
given CloudWatch Log Group and yields in the output .
is a long blocking call * *
Parses out the Layer version from the arn
Computes a unique name based on the LayerVersion Arn
makes a temporary directory and yields it name .
Returns the stdout as a byte stream in a Py2/PY3 compatible manner
Returns the stderr as a byte stream in a Py2/PY3 compatible manner
Download a list of layers to the cache
given layer to the local cache .
Fetch the Layer Uri based on the LayerVersion Arn
Create the Cache directory if it does not exist .
is similar to running a; Runs the SAM Translator to determine if the template provided is valid .
Replaces the CodeUri in AWS::Serverless::Function and DefinitionUri in AWS::Serverless::Api to a fake
Updates the 'property_key ' in the 'resource_property_dict ' to the value of 's3_uri_value '
returns a Formatter capable of nicely formatting Lambda function logs
be querying .; generates the name based on the
Parse the time from the given string , convert to UTC , and return the datetime object
Given the LogicalID of a resource , call AWS CloudFormation to get physical ID of the resource within
Given a SAM template dictionary , return a cleaned copy of the template where SAM plugins have been run
given template , apply parameter values to resolve intrinsic functions
based on user-supplied values ,
read default values for template parameters and return it
Change the code_path to be of unix-style if running on windows when supplied with an absolute windows path .
Used by container debug mode to enable certain container
is already configured in the; Returns the entry point for the container .
defined through Serverless Function with an Api Event
reading and parsing Swagger documents .
is defined both in Implicit and Explicit API definitions .
Normalize the APIs to use standard method name
configured for this SAM Function resource .
Given an AWS : :Serverless : :Function Event Dictionary , extract out all 'Api ' events and store within the
Converts a AWS::Serverless::Function's Event Property to an Api configuration usable by the provider.
is a special verb to denote all; allows a Http Methods of ANY .
Stores the given APIs tagged under the given logicalId
Stores the binary media type configuration for the API with given logical ID
Returns the list of APIs in this resource along with other extra configuration such as binary media types ,
is not found , then it returns an; given logical ID .
unzip a file to a temporary directory
given Lambda function locally .
is executing , we setup certain interrupt handlers to stop the execution .
get a path to a directory where the Lambda function code is available .
Build the image if one is not already on the system that matches the runtime and layers
Generate the Docker TAG that will be used to create the image
Builds the image
Generate the Dockerfile contents
starts the local API Gateway service .; block until the service is stopped
configure the Local API Service based on the APIs configured in the template .
is purely for printing purposes .; print the APIs that will be mounted .
returns the path to the directory where static files are to be served from .; static_dir is a
Creates a Flask Application that can be started .
Validates the incoming request
Updates the Flask app with Error Handlers for different Error Codes
is responsible for understanding the incoming
given file into the given directory while preserving file permissions in the process .
extracted file by reading the `` external_attr `` property of given file info .
Download the LayerVersion Zip to the Layer Pkg Cache
Generates the key to the _dict_of_routes based on the list of methods
handling a request is as follows; handle all requests to the host : port .
Get the route ( Route ) based on the current request
Parses the output from the Lambda Container
be decoded from Base64 to Binary
constructs the Event to be passed to Lambda
Constructs an APIGW equivalent query string dictionary
given event to the function and return; given name and invoke it .
invoke configuration to pass to Lambda Runtime to invoke the given function
Returns the environment variables configuration for this function
obtained from the shell environment or given profile
be used in serializing to JSON
is required for us to invoke the function locally; is running .
run a Docker container based on the given configuration .
pull the container image with given name .
Is the container image with given name available ?
Publish the application based on command line inputs .
detailed success message for published applications .
Print link for the application in AWS Serverless Application Repository console.
create a Lambda Failure Response
Constructs a Flask Response for when a Lambda function is not found for an endpoint
Constructs a Flask Response for when a API Route ( path+method ) is not found .
Creates a progressbar
starts up the ( threaded ) Local Server .
Constructs a Flask Response from the body , headers , and status_code .
extract read the given stream and return the response from Lambda function separated out
see if the output from the container is in the form of an Error/Exception from the Lambda invoke
Build the entire application
Given the path to built artifacts , update the template to point appropriate resource CodeUris to the artifacts
Depending on the configuration; Given the function information , this method will build the Lambda function .
is required by the builder
Use this method to convert a list of host paths to a list of equivalent paths within the container
is unused and a Work In Progress
gets the subcommands under the service name
gets the Click Commands underneath a service name
calls for value substitution in the event json and returns the
Generates the lars path for weighted data .
adds features to the model
see explain_instance_with_data to
perturbed data , labels and distances , returns explanation .
is useful for embedding; generate random div ids .
Returns the list of classification labels for which we have any explanations .
Returns the explanation as a list .
Returns the explanation as a pyplot figure .
html explanation in ipython notebook .
html explanation to file .
Returns the explanation as an html page .
Checks for mistakes in 'parameters'
return those in ` fn ` 's arguments .
Maps ids to words or word-position strings.
Adds text with highlighted words to visualization.
id_ ( int ) occurrences
removing the appropriate words .
created by a passed-in tokenizer
appropriate words .
Generates explanations for a prediction.
Generates a neighborhood around a prediction .
feature names .
Shows the current example in a table format .
validate the structure of training data stats
expect 3d arrays , but we are reshaping
Init function.
Generates images and predictions in the neighborhood of this image.
accepts a given keyword argument .
Discretizes the data .
is stored in a way that allows you to have a
Build a DAG and draw it in ASCII .
Draws ASCII canvas on the screen.
Create a point on ASCII canvas .
Create a line on ASCII canvas .
Print a text on ASCII canvas .
Create a box on ASCII canvas .
Refreshes progress bar.
progress bar for a specified target .
Finishes progress bar for a specified target.
Make a progress bar out of info , which looks like :
Extract the content of dvc tree file
Gerenates diff message string output
given node for the given graph .
guess whether the given file is text or binary ,
does n't support Unicode input , so need to use some tricks
Source: https://github.com/python/cpython/blob/
Install package.
was created with ` dvc import ` .
Used mainly for ` dvc remove -- outs ` and : func : ` Stage.reproduce ` .
has been already ran and stored
Launch a ` dvc daemon ` command in a detached process .
need to take into account two cases :
Setup parser for `dvc init`.
delimited text to have same column width .
Tabularize the content according to its type .
Gather all the metric outputs.
Read the content of each metric file and format it .
Generate a graph by using the given stages on the given directory
looking for Dvcfiles ,
Add a new line if progress bar has n't finished
return a stream .
Directory tree generator.
paginate through list objects .
config file location .
getting two repo trees between two git tag commits
has changed .
Ask the user for confirmation about the specified statement .
dvc CLI command .
has a valid value .
Returns global config location.
Returns system config location.
Initializes dvc config.
config from all the config files .
config to config files .
specified option and/or section in the config .
specified option in the config .
Prints option value from the config.
modifies the stage associated
Creates an empty repo on the given directory -- basically a
Generate a version with information about the git repository
has uncommitted changes .
get the ( md5 hexdigest , md5 digest ) of a file
specified keys from a nested dict
Copy file with progress bar
Proxy for `os.walk` directory tree generator.
Returns a message in a specified color.
Put a message inside a box .
Get the the number of columns required to display a string
string according to it 's visual width
See more info at :
Default targets for `dvc repro` and `dvc pipeline`.
corresponds to a repo at the specified
containing common arguments shared among
Parses CLI arguments.
Recursively apply changes from src to dest.
updating target progress
depending on the OS :
specified by path .
Collect analytics report.
info from a CLI command .
Save analytics report to a temporary file.
send analytics for CLI command .
send analytics .
Push data items in a cloud-agnostic way.
Check status of data items in a cloud-agnostic way.
iterates over specified revisions .
Check if file/directory has the expected md5 .
Loads state database.
Saves state database.
Save checksum for the specified path info.
Gets the checksum for the specified path info .
Adds the specified path to the list of links created by dvc .
saved links except the ones that are used .
Acquire lock for dvc repo.
emails from IMAP server and displays them in the Window
run shell command
Kill a process tree ( including grandchildren ) with signal
wish to include
Show a Popup but without any buttons
Popup with colored button and 'Error' as button text
Set the scroll region on the canvas
Configure canvas for a new selection.
Clicked somewhere in the calendar .
Updated calendar to show the previous month .
show the next month .
Return a datetime representing the current selected date .
are a variable number of Elements
Change the window 's transparency
Sets Card's color and escape code.
Generate image data using PIL
Returns either the delay (in ms) or None on timeout.
are returned as tuple
Return a PNG image for a document page number .
Count the number of live neighbours around point (i, j).
Play Conway's Game of Life.
string reprentation of bytes
show a window ( PySimpleGUI form ) that takes user input and sends to the HowDoI web oracle
send the 'Query ' to HowDoI
returns a key of the clicked event .
=============================================================
=========================================
==========
Converts size in characters to size in pixels
Convert from font string/tyuple into a Qt style sheet string
Display popup with text entry field and browse button.
Display Popup with text entry field
Reads the context menu
Shows a balloon above icon in system tray
Updates the menu , tooltip or icon
communicates with the GUI
executes the GUI
Send one ping to the given > destIP < .
Receive the ping from the socket .
show a form on tbe caller 's behalf .
Update the progress meter for a form
need a meter .
return ( does not block )
Display data in a table format
taken from a checkpoint .
attached to the module under the given key , or None .
training images from the file system .
Creates a graph and loads Hub Module into it .
extract the 'bottleneck ' summary layer .
Create a single bottleneck file .
bottleneck values for an image .
testing , and validation bottlenecks are cached .
bottleneck values for cached images .
bottleneck values for training images , after distortions .
Creates the operations to apply the specified distortions .
Attach a lot of summaries to a Tensor ( for TensorBoard visualization ) .
Adds a new softmax and fully-connected layer for training and eval .
Inserts the operations we need to evaluate the accuracy of our results .
Runs a final evaluation on an eval graph using the test data set .
Builds an restored eval session without train operations for exporting.
file , creating a valid quantized one if necessary .
perform JPEG decoding and resizing to the graph ..
Exports model for serving.
logging_level into TensorFlow logging verbosity value
Returns the module 's attached ImageModuleInfo message , or None .
expected [ height , width ] dimensions of an image input .
expected num_channels dimensions of an image input .
Returns a ParsedTensorInfo instance from a TensorInfo proto.
is a SparseTensor or a parsed sparse tensor info .
be feed into ` tensor_info ` .
dict ` values ` in tensors that are compatible with ` targets ` .
feed tensors in ` protomap ` using ` inputs ` .
using ` get_tensor_by_name ` .
Whether two signature inputs/outputs match in dtype, shape and sparsity.
Parses a line of a text embedding file .
Loads a text embedding into memory as a numpy matrix .
perform token to embedding lookups .
performs embedding lookups .
> 0 .
Create a ModuleSpec out of a SavedModel .
be exported under ` export_name ` .
constructed using ` estimator ` and ` serving_input_fn ` .
Creates a ModuleSpec from a function that builds the module 's graph .
Adds a signature to the module definition .
Adds an attached message to the module definition .
do not expect inputs .
hold the state .
Replaces state ops with non state Placeholder ops for the apply graph.
Given a tensor name as node_name : output_number , returns both parts .
Matches a variable to individual parts.
contains PartitionedVariables .
tag list contains each set of tags only once .
uses supported collections .
Register graph ops absent in op_def_registry, if present in c++ registry.
according to input_map .
Returns a dict mapping from pre-import to post-import colocation attrs.
Rewrites colocation constraints in the current default graph.
Returns error message for colocation of state ops, or None if ok.
Returns error message for colocation of signature inputs, or None if ok.
Returns error message for module inputs from ops with multiple outputs.
Internal.
Creates the graph nodes that hold the state of the Module .
Receives a value for the object and some context on its source .
raises ValueError with formatted contexts .
Returns the directory where to cache the module .
Returns the path for storing variables checkpoints .
have format node_name : output_number .
Adds a signature to current graph .
Exports signatures from current graph into a MetaGraphDef.
Adds a ModuleAttachment to the current graph .
Exports ModuleAttachments from the current tf.Graph into `meta_graph`.
Returns the dict of ModuleAttachments stored in ` meta_graph ` .
TypeError if ` node_def ` does not match the expectations .
Merges the ASSETS_KEY collection into the GraphDefs in saved_model_proto.
Creates an ASSETS_KEY collection in the GraphDefs in saved_model_proto .
Reads the savedmodel.pb file containing ` SavedModel ` .
Creates a SavedModelHandler from a SavedModel in ` path ` .
Adds a copy of Graph with the specified set of tags .
Returns a copy of a MetaGraph with the identical set of tags.
Returns a list of set of tags.
Exports to SavedModel directory.
Returns the matching MetaGraphDef or raises KeyError .
register but not create an existing weight .
Helper function to ModuleSpec.export().
Returns a fresh variable/name scope for a module's state.
extra/missing args .
Converts from inputs into dict of input tensors.
yields a function to directly evaluate a Module .
Loads a module from a handle .
Describes the inputs required by a signature .
Describes the outputs provided by a signature .
see there for more .
Exports the module with the variables from the session in `path`.
Returns the list of all tf.Variables created by module instantiation .
Uses a Module to construct a dense representation from a text feature .
is not a text-embedding module .
Uses a Module to get a dense 1-D representation from the pixels of images .
is not usable as image embedding .
Used for variable_scope and naming .
Returns a `Tensor`.
parsing spec as dict .
represent this feature in the input_layer ( ) .
Returns cache directory.
returns the name of directory where to cache a module .
Merge a relative tar file to a destination ( which can be `` gs : // ... '' ) .
Writes a descriptor file about the directory containing a module .
given 'directory ' .
Returns the size of the temp dir pointed to by the given lock file .
Waits for the lock file to disappear.
Returns the path to a Module directory for a given TF-Hub Module handle .
Prints a message about download progress either to the console or TF log.
ongoing module download .
writes to 'dst_path ' .
Streams the content for the 'fileobj' and stores the result in dst_path.
name scope to a name .
shared_name attributes of nodes .
propagate backwards in the graph and mark nodes as used .
prune unused ops given a signature def .
prune the feedmap of nodes which no longer exist .
Writes to ` filename ` atomically .
Builds a path to a new subdirectory within the base directory.
based on the argument but starting with 'temp- ' .
retaining only a given number of the most recent .
Generate a human-readable string representing number of bytes .
Generates a new release announcement entry in the docs .
Generates new docs , release announcements and creates a local tag .
serialize a list of prediction and/or
delete a HorizontalPodAutoscaler
get available resources
update the specified HorizontalPodAutoscaler
partially update status of the specified HorizontalPodAutoscaler
replace the specified HorizontalPodAutoscaler
replace status of the specified HorizontalPodAutoscaler
Sets the ca_bundle of this V1alpha1WebhookClientConfig .
create a PodPreset
delete collection of PodPreset
list or watch objects of kind PodPreset
create a CronJob
delete collection of CronJob
list or watch objects of kind CronJob
read the specified CronJob
read status of the specified CronJob
create a CustomResourceDefinition
delete a CustomResourceDefinition
list or watch objects of kind CustomResourceDefinition
read the specified CustomResourceDefinition
read status of the specified CustomResourceDefinition
delete an Ingress
list or watch objects of kind Ingress
update the specified Ingress
partially update status of the specified Ingress
read the specified Ingress
replace the specified Ingress
replace status of the specified Ingress
proxy of Service
exec of Pod
proxy of Pod
proxy of Node
portforward of Pod
create Endpoints
create an Event
create a Pod
create a ReplicationController
create a ServiceAccount
delete collection of Endpoints
delete collection of Event
delete collection of Pod
delete collection of PodTemplate
delete collection of Secret
delete collection of ServiceAccount
delete a Namespace
delete a ConfigMap
delete a LimitRange
delete a PersistentVolumeClaim
delete a ReplicationController
delete a ResourceQuota
delete a ServiceAccount
delete a Node
list objects of kind ComponentStatus
list or watch objects of kind ConfigMap
list or watch objects of kind Endpoints
list or watch objects of kind LimitRange
list or watch objects of kind Namespace
list or watch objects of kind Event
list or watch objects of kind PersistentVolumeClaim
list or watch objects of kind Pod
list or watch objects of kind PodTemplate
list or watch objects of kind ReplicationController
list or watch objects of kind ResourceQuota
list or watch objects of kind Secret
list or watch objects of kind Service
list or watch objects of kind ServiceAccount
list or watch objects of kind Node
list or watch objects of kind PersistentVolume
update the specified Namespace
partially update status of the specified Namespace
update the specified ConfigMap
update the specified Endpoints
update the specified Event
read status of the specified PersistentVolumeClaim
read the specified PodTemplate
read the specified ResourceQuota
read the specified ServiceAccount
read the specified PersistentVolume
replace status of the specified Namespace
replace the specified LimitRange
replace status of the specified Pod
replace status of the specified ReplicationController
replace the specified Service
replace status of the specified Node
Sets the raw of this RuntimeRawExtension .
Create thread pool on first request
Builds a JSON POST object.
Deserializes dict, list, str into an object.
based on an array of accepts provided .
based on an array of content_types provided .
based on authentication setting .
string to date .
Deserializes list or dict to model.
Sets the debug status .
Sets the logger_format .
set ) .
string ) .
create a Lease
delete collection of Lease
list or watch objects of kind Lease
update the specified Lease
create an AuditSink
delete an AuditSink
list or watch objects of kind AuditSink
update the specified AuditSink
read the specified AuditSink
replace the specified AuditSink
get information of a group
Sets the certificate of this V1beta1CertificateSigningRequestStatus .
create a ClusterRole
create a Role
create a RoleBinding
delete a ClusterRole
delete collection of ClusterRole
delete collection of ClusterRoleBinding
delete collection of Role
delete a Role
delete a RoleBinding
list or watch objects of kind ClusterRole
list or watch objects of kind Role
list or watch objects of kind RoleBinding
update the specified ClusterRole
update the specified ClusterRoleBinding
update the specified Role
read the specified ClusterRole
read the specified ClusterRoleBinding
read the specified Role
replace the specified ClusterRole
replace the specified ClusterRoleBinding
replace the specified Role
Change labels of the "minikube" node:
create a DaemonSet
delete collection of ControllerRevision
delete collection of DaemonSet
delete collection of Deployment
delete a StatefulSet
list or watch objects of kind DaemonSet
list or watch objects of kind Deployment
list or watch objects of kind ControllerRevision
list or watch objects of kind ReplicaSet
list or watch objects of kind StatefulSet
update the specified Deployment
partially update scale of the specified ReplicaSet
partially update status of the specified ReplicaSet
partially update status of the specified StatefulSet
read the specified DaemonSet
read status of the specified DaemonSet
read the specified Deployment
read scale of the specified Deployment
read status of the specified Deployment
read scale of the specified ReplicaSet
read status of the specified StatefulSet
replace the specified DaemonSet
replace the specified Deployment
replace scale of the specified Deployment
replace status of the specified Deployment
replace the specified ReplicaSet
replace scale of the specified ReplicaSet
replace status of the specified ReplicaSet
replace status of the specified StatefulSet
create a NetworkPolicy
create a PodSecurityPolicy
delete collection of PodSecurityPolicy
delete a NetworkPolicy
delete a PodSecurityPolicy
list or watch objects of kind NetworkPolicy
list or watch objects of kind PodSecurityPolicy
update the specified NetworkPolicy
read the specified NetworkPolicy
replace the specified NetworkPolicy
replace scale of the specified ReplicationControllerDummy
create a TokenReview
delete collection of VolumeAttachment
list or watch objects of kind StorageClass
list or watch objects of kind VolumeAttachment
partially update status of the specified VolumeAttachment
read the specified StorageClass
replace the specified StorageClass
replace the specified VolumeAttachment
create a RuntimeClass
delete collection of RuntimeClass
delete a RuntimeClass
list or watch objects of kind RuntimeClass
update the specified RuntimeClass
read the specified RuntimeClass
replace the specified RuntimeClass
create a CSIDriver
create a CSINode
delete collection of CSIDriver
delete collection of CSINode
delete a CSIDriver
delete a CSINode
list or watch objects of kind CSIDriver
list or watch objects of kind CSINode
update the specified CSIDriver
update the specified CSINode
read the specified CSIDriver
read the specified CSINode
replace the specified CSIDriver
replace the specified CSINode
Perform an action from a yaml file .
create a LocalSubjectAccessReview
create a SelfSubjectRulesReview
create a SubjectAccessReview
create a PriorityClass
delete collection of PriorityClass
delete a PriorityClass
list or watch objects of kind PriorityClass
update the specified PriorityClass
read the specified PriorityClass
delete an APIService
delete collection of APIService
partially update status of the specified APIService
replace status of the specified APIService
Sets the request of this V1beta1CertificateSigningRequestSpec .
`application/x-www-form-urlencoded`
create a MutatingWebhookConfiguration
create a ValidatingWebhookConfiguration
delete collection of ValidatingWebhookConfiguration
delete a MutatingWebhookConfiguration
delete a ValidatingWebhookConfiguration
list or watch objects of kind ValidatingWebhookConfiguration
update the specified MutatingWebhookConfiguration
update the specified ValidatingWebhookConfiguration
read the specified ValidatingWebhookConfiguration
replace the specified MutatingWebhookConfiguration
replace the specified ValidatingWebhookConfiguration
Deletes the specified namespace scoped custom object
scoped custom object
watch namespace scoped custom objects
patch the specified namespace scoped custom object
replace scale of the specified cluster scoped custom object
replace status of the specified namespace scoped custom object
create a Job
delete collection of Job
delete a PodDisruptionBudget
list or watch objects of kind PodDisruptionBudget
read the specified PodDisruptionBudget
read status of the specified PodDisruptionBudget
create a CertificateSigningRequest
delete a CertificateSigningRequest
list or watch objects of kind CertificateSigningRequest
update the specified CertificateSigningRequest
partially update status of the specified CertificateSigningRequest
read status of the specified CertificateSigningRequest
replace the specified CertificateSigningRequest
replace approval of the specified CertificateSigningRequest
mutating methods of property container classes
handle special-casing stream events
handle special-casing patch events
Print the Bokeh license to the console .
Return a copy of the default , or a new value if the default
transformed by prepare_value ( ) and the theme overrides .
match values .
Whether the value passes validation
be converted to this property type .
prepared values satisfy given conditions .
generate output saved
generate output in notebook cells
The last path component for the basename of the configured filename.
return an `` expr '' specification for
return a `` field '' specification for
return a `` value '' specification for
Return a list of `` PropertyDescriptor `` instances to install on a
is a string color literal .
given object to the given filename .
decompresses the given filename and returns the created object .
Returns the section heading for the issue , or None if this issue should be ignored .
Returns list of tags for this issue.
was closed after given date .; given , only checks if issue is closed .
is something we should show in the changelog .
relevant closed issues ( closed after a given datetime ) given a list of issues .
closed issues ( closed after a given datetime ) given a list of issues .
given a list of issues .
querying tags .
given by github API .
given URL as JSON and returns data as loaded python object .
Hits the github API for all closed issues after the given date , returns the data .
Given a list of tags , returns the datetime of the tag with the given name ; Otherwise None .
saving that data to a file ; or loads data from a file .
Checks issues for BEP 1 compliance.
given issue .
Prints out changelog.
Return a copy of this color value .
Generate the CSS representation of this RGB color .
Return a corresponding HSL color for this RGB color .
result into a Future by collapsing any futures inside result .
registered callbacks .
Adds a callback to be run on the next tick .
Adds a callback to be run once after timeout_milliseconds .
Adds a callback to be run every period_milliseconds until it is removed .
Link to a Bokeh Github issue.
pointing to appropriate tags
Required Sphinx extension setup function .
Return a link to a Bokeh Github resource .
Dispatch handling of this event to a receiver.
Create a JSON representation of this event suitable for sending
Print a useful report after setuptools output describing where and how
Print information about extra Bokeh-specific command line options.
Build a new BokehJS ( and install it ) or install a previously build
ensure we always build BokehJS when packaging
are installing FROM an sdist , then a pre-built BokehJS is
works around a setuptools deficiency .
build a new BokehJS or install an existing one .
Build BokehJS files (CSS, JS, etc) under the ``bokehjs`` source
built BokehJS files into the Python source tree .
Map axial *(q,r)* coordinates to cartesian *(x,y)* coordinates of
* points to axial * ( q , r ) * coordinates of enclosing
Perform an equal-weight binning of data points into hexagonal tiles .
floating point axial hex coordinates to integer * ( q , r ) *
Try to determine the version from the parent directory name .
TAG[.post.devDISTANCE] -- No -dirty.
need to wrap their values in special containers , etc .
encoded as lists or base64 strings .
recognized timedelta value to floating point absolute
Convert any recognized date, time, or datetime value to floating point
Convert NumPy datetime arrays to arrays to milliseconds since epoch.
Return a new unique ID for a Bokeh object .
Transform a NumPy arrays into serialized format
Transforms a NumPy array into a list of values
Transforms a Pandas series into serialized form
Transforms a NumPy array into serialized form .
traverse an object until a flat list is found .
Transform `` ColumnSourceData `` data to a serialized format
Send a numpy array as an unencoded binary buffer
Decode a base64 encoded array into a NumPy array.
Create a `` CustomJSHover `` instance from a Python functions .
Create a CustomJSHover instance from a CoffeeScript snippet .
is copied from sphinx.directives.code.CodeBlock.run
is largely copied from bokeh.sphinxext.bokeh_plot.run
be used to embed
embed a Bokeh plot .
embeds Bokeh Model or Document objects .
Return a JSON block that can be used to embed standalone Bokeh content .
Create a JSON string describing a patch to be applied as well as
used to wrap the callback with decorators .
Generate a key to cache a custom extension implementation with .
Create a bundle of selected ` models ` .
Returns CustomModels for models with a custom `__implementation__`
Returns the compiled implementation of supplied ` models ` .
Create a JavaScript bundle with selected ` models ` .
Print an error message and exit.
Return a Bokeh application built using a single handler for a script ,
Return a dictionary mapping routes to Bokeh applications built using
help print more informative error messages when a
Distance between (lat1, lon1) and (lat2, lon2).
Bind a socket to a port on an address.
Check a given request host against a whitelist .
Match a host string against a pattern
are 'jupyter ' as well as any names
Configure output to a standalone HTML file.
Save an HTML file with the data for the current document .
are built .
Generate a `` sitemap.txt `` from the collected HTML page links .
Start a Bokeh Server Tornado Application on a given Tornado IOLoop .
Provide a : class : ` ~bokeh.resources.Resources ` that specifies where
Start the Bokeh Server application .
Stop the Bokeh Server application .
Get an active a session by name application path and session ID .
Gets all currently active sessions for an application.
shared implementation to handle both error and warning
yield any that match the
given Bokeh model matches a given selector .
Generate a default filename with a given extension , attempting to use
return the filename of the currently running Python process
given base dir is not accessible or writeable
is on the system exex prefix
specify where output
Given a kwargs dict , a prefix , and a default value , looks for different
Takes a string and returns a corresponding ` Tool ` instance .
Adds tools to the plot object
Create a `` CustomJS `` instance from a Python function .
Write a message to the websocket after obtaining the appropriate
set on resource_attr attribute of all models .
Set the log level for python Bokeh code.
Return the secret_key , converted to bytes and cached .
The absolute path of the BokehJS source code in the installed
The CSS files in the BokehJS directory.
Return a serialized JSON representation of objects , suitable to
Handle special scalars such as (Python, NumPy, or Pandas)
The required ``default`` method for ``JSONEncoder`` subclasses.
Add a handler to the pipeline used to initialize new documents .
using the Application 's handlers .
Invoked to execute code when a new session is created .
Allow flexible selector syntax.
Query this object and all of its references for objects that
~bokeh.models.annotations.Legend ` objects .
Splattable list of :class:`~bokeh.models.tools.HoverTool` objects.
Adds an object to the plot in a specified place .
Adds tools to the plot.
Adds a glyph to the plot with associated data sources and ranges .
Adds new ``TileRenderer`` into ``Plot.renderers``
Create a row of Bokeh Layout objects .
Create a column of Bokeh Layout objects .
Create a column of bokeh widgets with predefined styling .
Create a grid-based arrangement of Bokeh Layout objects .
Create a grid of plots rendered on separate canvases .
create a grid of layoutable objects .
Recursively create grid from input lists.
Yield successive n-sized chunks from list, l.
Wrap a callback function to execute without first obtaining the
Return a script tag that embeds content from a Bokeh server .
Return a script tag that embeds content from a specific existing session on
Produce a canonical Bokeh server URL .
Extract the app path from a Bokeh server URL
add to a Bokeh server URL .
Implement a check_origin policy for Tornado to call.
Initialize a connection to a client .
Perform the specific steps needed to open a connection to a Bokeh session
Process an individual wire protocol fragment.
Send a Bokeh Server protocol message to the connected client .
Override parent write_message with a version that acquires a
is closed .
rendered CSS and JS resources suitable for the given
satisfies a given query predicate
objects contains a plot requesting WebGL
objects contains a TableWidget
objects contains a any Widget
calls this during class creation as it iterates
Produce the value as it should be serialized .
Sets the value of this property from a JSON value .
Get the default value that will be used for a specific instance .
Send a change event notification if the property is set to a
Internal implementation of instance attribute access for the
Internal implementation of instance attribute access for default
set property values , that is used
set property values .
call when a container is mutated `` behind our back ''
send a change event notification for the property .
dealing with units associated units properties
Install a new notebook display hook .
Update Bokeh plots in a Jupyter notebook output cells with new data
Run an installed notebook hook with supplied arguments.
Given a UUID id of a div removed or replaced in the Jupyter
Prepare the IPython notebook for displaying Bokeh plots .
Embed a Bokeh server application in a Jupyter Notebook output cell .
be non-negative , an optionally , less than a
reduce the luminance ) of this color .
increase the luminance ) of this color .
Set up a handler for button state changes (clicks).
Set up a handler for button or menu item clicks.
Set up a JavaScript handler for button or menu item clicks.
import an optional dependency .
Detect if PhantomJS is avaiable in PATH , at a minimum version .
Consume individual protocol message fragments.
Return a driver function that can advance a `` bounced '' sequence
Return a driver function that can advance a sequence of cosine values .
Return a driver function that can advance a sequence of linear values .
Return a driver function that can advance a repeated of values .
Return a driver function that can advance a sequence of sine values .
Collect a duplicate-free list of all other Bokeh models referred to by
Look up a Bokeh model class , given its view model name .
recursing into any
Recurse down Models, HasProps, and Python containers
A Bokeh protocol "reference" to this model, i.e.
using JavaScript .
Attach a `` CustomJS `` callback to an arbitrary BokehJS model event .
Add a callback on this object to trigger when `` attr `` changes .
match a given selector with the specified
encoding the attributes of this object .
Attach a model to a Bokeh |Document| .
Returns a dictionary of the attributes of this object, in
Create a new : class : ` ~bokeh.plotting.gmap.GMap ` for plotting .
Generate the CSS representation of this HSL color .
Return a corresponding : class : ` ~bokeh.colors.rgb.RGB ` color for
Get the location of the server subpackage
dev is True ,; Get the location of the bokehjs source files .
Install the Bokeh Server and its background tasks on a Tornado
Stop the Bokeh Server .
Run the Bokeh Server until shutdown is requested by the user ,
Gets all currently active sessions for applications.
Opens an app in a browser window or tab .
Make a fresh module to run in .
Execute the configured source code in a module and run any post
Execute a blocking loop that runs and executes event callbacks
Pull a document from the server , overwriting the passed-in document
Push a document to the server , overwriting any existing server-side doc .
Ask for information about the server.
Export the `` LayoutDOM `` object or document as a PNG .
Export the SVG-enabled plots within a layout .
Get a screenshot of a `` LayoutDOM `` object .
Crop the border from the layout
Create a `` dict `` of columns from a Pandas `` DataFrame `` ,
Return the Bokeh-appropriate column name for a `` DataFrame `` index
Appends a new column of data to the data source .
Remove a column of data .
update data source columns
Efficiently update data source columns at specific locations
Silence a particular warning on all Bokeh models.
Apply validation and integrity checks to a collection of Bokeh models.
Apply this theme to a model .
create a ( possibly temporary ) Document to use for serializing
check for Python ( event ) callbacks
Generate a `` GraphRenderer `` from a `` networkx.Graph `` object and networkx
Create an |Enumeration| object from a sequence of values.
Generate a random session ID .
Check the signature of a session ID , returning True if it 's valid .
Return a securely generated random string .
display a Bokeh plot in a Jupyter
Create a `` CustomJSTransform `` instance from a pair of Python
Create a `` CustomJSTransform `` instance from a pair of CoffeeScript
mark abstract base classes derived from |HasProps| .
Traverse the class hierarchy and accumulate the special sets of names
Traverse the class hierarchy and accumulate the special dicts
Structural equality of models.
Set a property value on this object from JSON .
Updates the object 's properties from a JSON attributes dictionary .
Collect the names of properties on this class .
Collect a dict mapping property names to their values .
Query the properties values of |HasProps| instances with a
Apply a set of theme values which will be used rather than
given block of text by a specified amount .
Join together sequences of strings into English-friendly phrases using
Convert CamelCase to snake_case.
defined on the Document
add a Message ( and its revision ) to the Protocol index .
Create a session by loading the current server-side document .
Create a session by pushing the given document to the server ,
Open a browser displaying a session document .
Pull the server 's state and set it as session.document .
Push the given document to the server and record it as session.document .
Open a browser displaying this session .
Called by the ClientConnection we are using to notify us of disconnect .
Creates a new message , assembled from JSON fragments .
Associate a buffer header and payload with this message .
Add a buffer header and payload that we read from the socket .
given connection .
Return a message header fragment dict .
Send the message on the given connection .
are present .
undoes the default
formatted deprecation warning .
Execute the `` bokeh '' command line program .
Allow the session to be discarded and do n't get change notifications from it anymore
Sends a PATCH-DOC message , returning a Future that 's completed when it 's written out .
Given a JSON representation of the models in a graph , and new model
Given a JSON representation of all the models in a graph , return a
Given a list of all models in a graph , return JSON representing
Custom JSON decoder for Events.
Generate an inline visual representations of a single color palette.
Create a Create a `` DataSpec `` dict to generate a `` CumSum `` expression
Create a `` DataSpec `` dict that applies a client-side `` Jitter ``
Create a `` DataSpec `` dict that applies a client-side
Create a `` DataSpec `` dict that applyies a client-side
Create a `` DataSpec `` dict that applies a client-side `` LogColorMapper ``
Return a browser controller .
Open a browser to view the specified location .
Create a `` CustomJSFilter `` instance from a Python function .
Return streamlines of a vector flow.
build a suitable CustomJS to display the current event
returns a Python callback to pretty print the events .
Create a new Message instance for the given type .
Create a Message instance assembled from json fragments .
combine a new event with a list of previous events .
be invoked once on the next tick of the event loop .
Add a callback to be invoked on a session periodically .
Add a model as a root of this Document .
be invoked once , after a specified timeout passes .
Apply a JSON patch object and process any resulting events .
Remove all content from the document but do not reset title .
created by this Document when its session is
Load a document from JSON .
Activate a document hold .
apply any collected events .
Provide callbacks to invoke if the document or any Model reachable
Provide callbacks to invoke when the session serving the Document
Remove a model as root model from this Document .
Overwrite everything in this document with the JSON-encoded
match the given selector .
Convert the document to a JSON string.
Perform integrity checks on the modes in this document.
adding session callbacks .
leaving this doc empty .
Called by Model when it changes
Remove a callback added earlier with `` add_periodic_callback `` ,
check callback signature
Remove a callback from this object
Trigger callbacks for ``attr`` on this object.
Download larger data sets for various Bokeh examples.
Execute the Bokeh command .
display a Bokeh object or application .
Render an HTML page from a template and Bokeh render items.
extracts a zip file .
containing directory .
retrieve the value tuple as a tuple of
Execute the configured `` main.py `` or `` main.ipynb `` to modify the
Prints a list of valid marker types for scatter()
Creates a scatter plot of the given x and y items .
Perform a simple equal-weight hexagonal binning .
Generate multiple ``HArea`` renderers for levels stacked left
stacked left to right .
Generate multiple ``Line`` renderers for lines stacked vertically
Generate multiple ``VArea`` renderers for levels stacked bottom
Generate multiple ``VBar`` renderers for levels stacked bottom
Creates a network graph using the given node , edge and layout provider .
Convert an ``ws(s)`` URL for a Bokeh server into the appropriate
Convert an ``http(s)`` URL for a Bokeh server websocket endpoint into
Turn off property validation during update callbacks
Get the correct Jinja2 Environment , also for frozen scripts .
From a dataframe create a parallel coordinate plot
Delegate a received message to the appropriate handler .
adds the necessary locking and post-processing
be called by `` ServerConnection.unsubscribe_session `` or our book-keeping will be broken
Set color scheme for matched parentheses.
Set color scheme of the shell.
Banner for IPython widgets with pylab message
Reset the namespace by removing all names defined by the user .
Create shortcuts for ipyconsole.
increasing the prompt
execute a kernel method and save its reply
returned by silent executions of kernel methods
require the Qt backend , so we try to detect if one is
is trying to change Matplotlib backends with
Reimplement the IPython context menu
let the user decide if he wants a
Refresh the highlighting with the current syntax style by class .
Emit a signal when the prompt is ready .
send focus change notification
Installs a panel on the editor .
Removes the specified panel .
Removes all panel from the CodeEditor.
Gets a specific panel instance.
resize and update margins ) .
Update foating panels.
Update viewport margins.
Compute panel zone sizes.
Locate a module path based on an import line in an python-like file
Find the definition of an object within a source closest to a given line
Return a list of all python-like extensions
Return a list of all editable extensions
Perform completion of filesystem path.
Return a list of ( completion , type ) tuples
Find the definition for an object within a set of source code
Get a formatted calltip and docstring from Fallback
Creates the editor dialog and returns a tuple ( dialog , func ) where func
return the edited copy
Return a sorted list of all the children items of 'item ' .
return the item of the outline explorer under which is located
Reimplemented OneColumnTree method
is disabled : hide all root items except * item *
Bind editor instance
was renamed , updating outline explorer tree
Order the root file items in the Outline Explorer following the
Sort the root file items in alphabetical order if
Generates an outline of the editor 's content and stores the result
has been selected : expanding it and collapsing others
Return the root item of the specified item .
Return a list of all visible items in the treewidget .
Setup the buttons of the outline explorer widget toolbar.
Return outline explorer options
Uninstalls the editor extension from the editor .
py2exe/cx_Freeze distribution object
used by Spyder
is datatype dtype is a number kind
Extract the boundaries from a list of indexes
Array column number
Array row number
Cell content change
Set header data
Create editor widget
Commit and close editor
Set editor widget's data
Resize cells to contents
Setup context menu
Reimplement Qt method
Copy an array portion to a unicode string
Copy text to clipboard
Change display format
Setup ArrayEditor:
is implemented for handling negative values in index for
is plotting over
occured , closing the dialog box
Get a Pygments Lexer given a filename .
Get the keywords for a given lexer .
be used in code completion .
Given a file path , determine the full module path .
are currently in .
Load the user's previously-saved kernel connection settings.
Save user's kernel connection settings.
depending on value type
Return the column separator
Return the row separator
Set if data type conversion
Return a data element
Return a model data element
Parse a type to an other type
Decode the shape of the given text
Put data into table model
Parse to a given type
Open clipboard text as table
Change tab focus
Proceed to a given step
Reduce the alist dimension if needed
processed as data
Process the data from clipboard
Set Spyder breakpoints into a debugging session
Run an IPython magic while debugging.
Refresh Variable Explorer and Editor from a Pdb session,
add a % plot magic .
Handle Key_Up/Key_Down while debugging.
Returns the global maximum and minimum .
Return the corresponding labels taking into account the axis .
Return the number of levels in the labels taking into account the axis .
Return the values of the labels for the header of columns or rows .
Determines the maximum and minimum number in each column .
depending on value .
Return the value of the DataFrame .
Overriding sort method
DataFrame column number
Load more rows and columns to display.
Implement a column sort.
Setup context menu.
changes types of cells .
Get number of rows in the header.
Get more columns or rows ( based on axis ) .
Overriding sort method .
Get the information to put in the header .
Get the data for the header .
Get the text to put in the header of the levels of the indexes .
Get the information of the levels .
Setup DataFrameEditor:
Handle the data change event to enable the save and close button .
Create the QTableView that will hold the level model .
Create the QTableView that will hold the header model .
Create the QTableView that will hold the index model .
Create the QTableView that will hold the data model .
Implement a Index sort.
Update the column width .
Update the row height .
Resize the corresponding column of the index section selected .
Resize the corresponding row of the header section selected .
given table .
Set the model for the data, header/index and level views.
Set current selection.
Resize a column by its contents .
Resize all the colummns to its contents .
catch resize event .
Resize the current column to its contents .
Resize the columns to its contents .
is implementet so column min/max is only active when bgcolor is
modified Dataframe -- this is * not * a copy
Update the column width of the header .
Setup the namespace browser with provided settings.
show in the cog menu .
Add the cog menu button to the toolbar.
Option has changed
Return dict editor view settings
Refresh variable table
Import data from text file.
Apply changes callback
Return page widget
be able to save the widget 's size from the
Return True if all widget contents are valid
Load settings from configuration file
Save settings to configuration file
choices: couples (name, key)
setting plugin font
added in a vertical layout
Prompt the user with a request to restart .
After an application style change, the paintEvent updates the
Update the status and set_status to update the icons to display .
Qt Override.
Register plugin in Spyder's main window
DockWidget visibility has changed
Restoring scrollbar position after main window is visible
Save configuration: tree widget state
Load configuration: tree widget state
Removing obsolete items
is an update available .
Main method of the WorkerUpdates worker
Event filter for search_text widget.
Create shortcuts for this widget
Toggle the 'highlight all results' feature
Overrides Qt Method
Show replace widgets
associated editor/web page :
Find next occurrence
Find previous occurrence
has been edited ( this slot wo n't be triggered when
Highlight found results
Call the find function
Replace and find
find in the current selection
Change number of match and total matches.
Monkey patching rope
Override Qt method.
Return a list of actions related to plugin
Update font from Preferences
Apply configuration file's plugin settings
Set rich text mode font
plain text mode font
Toggle wrap mode
plain text mode
rich text mode
plain text docs
Set rich text
Show text in rich mode
Show text in plain mode
Show the Spyder tutorial in the Help plugin , opening it if needed
object analyzed by Help
Use the help plugin to show docstring dictionary computed
Load history from a text file in user home directory
Save history to a text file in user home directory
Toggle plain text docstring
show source code
sphinxified docstrings or plain ones
Toggle automatic import feature
locked state icon
is currently bound to Help ,
string dictionary to HTML and show it
based on thread result
Display error message on Sphinx rich text failure
is in the decoration .
Select the entire line but starts at the first non whitespace character
Underlines the text.
Underlines text as a spellcheck error.
Highlights text as a syntax error.
Highlights text as a syntax warning.
associated to parent_id
Add thread to queue
Editor's text has changed
Run TODO finder
Set TODO results and update markers in editor
corresponpding ids and tabs .
Remove editors that are not longer open .
Insert the widget (at tab index) in the position i (index).
Remove the widget at the corresponding tab_index .
add it as the latest .
Fill ListWidget with the tabs texts.
selected document and hide this widget .
selected row a number of steps .
Positions the tab switcher in the top-center of the editor.
Reimplement Qt method.
allow cyclic behavior .
close the widget when loosing focus .
Create local shortcuts
Setup editorstack's layout
Upadte file name label.
Overrides QWidget closeEvent().
Clone EditorStack from other instance
Open file list management dialog box
Go to line dialog
Set conditional breakpoint
given slot .
Inspect current object in the Help plugin
is called separately from 'set_oulineexplorer ' to avoid
Return tab title.
Return tab menu title
Setup tab context menu before showing it
Return the self.data index position for the filename .
return the associated editor instance .
is in the editor stack .
Return the position index of a file in the tab bar of the editorstack
is synchronized with the tab bar when
Close file (index=None -> close current file)
opened files ' languages
code editors .
opened to the right
Close all files but the current one
Sort open tabs alphabetically.
Utility function for sort_file_tabs_alphabetically().
Add to last closed file list.
save file if modified .
writing text of editor to file .
Write text of editor to a file.
was just saved in another editorstack , let 's synchronize !
Select a name to save a file .
Save file as...
Save copy of file as...
Save all opened files .
Analyze current script with todos
Synchronize todo results between editorstacks
Stack index has changed
used '' behaviour .
Editor focus has changed
Refresh outline explorer panel
Order the root file items of the outline explorer as in the tabbar
Refreshing statusbar widgets
Check if file has been changed in any way outside Spyder :
Current editor's modification state has changed
Reload file from disk
Revert file from disk
Create a new editor instance
qstr1: obj_text, qstr2: argpspec, qstr3: note, qstr4: doc_text
encoding * and * text *
create an editor instance and return it
Sets the EOL character ( s ) based on the operating system .
Remove trailing spaces
Replace tab characters by spaces
selected text or current line in console .
Run current cell.
Advance to the next cell.
Run the previous cell again .
Run cell code in the console.
attach a new EditorSplitter to the current EditorSplitter .
Return the editor stacks for this splitter and every first child .
Return the layout state for this splitter and its children .
Restore layout state for the splitter panels.
Add toolbars to a menu.
Loads the last visible toolbars from the .ini file .
Return layout state
Restore layout state
was saved in editorstack , this notifies others
was renamed in data in editorstack , this notifies others
Find in files callback
Perform actions before parent main window is closed
Draw the fold region when the mouse is over and non collapsed
Draw the background rectangle using the current style primitive color .
Draw the fold indicator/trigger ( arrow ) .
is not a fold trigger .
Clear scope decorations (on the editor)
Gets the base scope highlight color ( derivated from the editor
Create a decoration and add it to the editor .
Highlights the current fold scope.
highlight the current scope in the
Folds/unfolds the pressed indicator if any.
Show the block previous blank lines
is called by the syntax
Refrehes editor content and scollbars.
Collapses all triggers and makes all blocks with fold level > 0
Clear the folded block decorations.
Expands all fold triggers.
Toggle the current fold trigger.
Highlight the scope of the current caret position.
corner menu .
Move tab.
Add new history tab.
Append an entry to history filename.
emit its results to the LSP server .
allows to map LSP method names to class methods .
Return a list of actions related to plugin .
Return current client with focus, if any
Run script in current or dedicated client
Run cell in current or dedicated client.
Execute code instructions.
Create a new client
Create a client connected to an existing kernel
Connect a client to its kernel
Handle %edit magic petitions.
Generate a Trailets Config instance for shell widgets using our
used by clients
are not defined
Register new client
Close client tab from index or widget (or close current tab)
Return client index from id
are connected to the same kernel as ` client `
related to * client * , except itself
Restart the console
has just stopped at frame ( fname , lineno )
Create a client with its cwd pointing to path .
Create a client to execute code related to a file .
associated with a given file .
elapsed time for slave clients .
Tunnel connections to a kernel via ssh.
Create a kernel spec for our own kernels
Create kernel manager and client.
Restart kernel of current client.
Reset kernel of current client.
Interrupt kernel of current client.
following the execution state of the kernel .
Connect an external kernel to the Variable Explorer and Help, if
have already been moved by the tabwidget )
Generate a file name without ambiguation .
Update the text from the tabs .
Rename client's tab
Rename tabs after a change in name.
Trigger the tab name editor.
Go to error if relevant
intro to IPython help
Show qtconsole help
Show IPython Cheat Sheet
Generate a new connection file
left by previous Spyder instances .
Rotate the kill ring , then yank back the new top .
Kills the text selected by the give cursor .
killed text .
files in external file explorer
fixing case , making absolute and removing symlinks
Create a new Python script
List files and directories
Return True if path has subdirectories
Setup filesystem model
open items .
Set name filters
Toggle 'show all files' state
associated with * index *
Return selected filenames
Setup tree widget
Setup context menu common actions
Edit name filters
Toggle all files mode
Return actions for submenu 'New...
Return file management actions
Return folder management actions
Create context menu actions
Update context menu
Selected item was double-clicked or enter/return was pressed
Drag and Drop - Move event
Reimplement Qt Method - handle drag event
Open files with the appropriate application
Open files with default application
Open file outside Spyder with the appropriate application
Run Python scripts
Remove whole directory tree
Convert an IPython notebook to a Python script in editor
notebooks to Python scripts in editor
Create new folder
Create new file
given file ( s ) /folders ( s ) .
Copy file(s)/folders(s) to clipboard.
Paste file from clipboard into file/project explorer directory.
Create shortcuts for this file explorer.
VCS action (commit, browse)
Set scrollbar positions
Restore scrollbar positions once tree is loaded
Save all items expanded state
Restore directory expanded state
loaded during startup
Restore all items expanded state
Filter the directories to show
Setup proxy model filter parameters
Show tooltip with full path only for the root directory
Setup proxy model
Set root path
associated with filename
Set folder names
Return filename from index
Setup view for projects
show current directory only mode
associated model index
Refresh widget
Go to parent directory
Update browse history
Set directory as working directory
Toggle icon text
Set model data
Return data at table index
Get the list of languages we need to start servers and create
pass to the LSP servers .
Send a new initialize message to each LSP server when the project
given language .
saved in our
Setup config page widgets and options.
Transcode a text string
Return a unicode version of string decoded using the file system encoding .
Return a byte string version of unic encoded using the file
get the coding of a text .
decode a text .
Convert a string to unicode
file ( 'filename ' ) assuming 'encoding ' in an atomic way
file ( 'filename ' ) assuming 'encoding '
Read text from file ( 'filename ' )
Read lines from file ('filename')
supported by Pygments
associated to file extension
Get all file types supported by the Editor
are running in an Ubuntu-based distribution
are running in a Gtk-based desktop
are running in a KDE desktop
Return PYTHONPATH list as relative paths
Set PYTHONPATH list relative paths
is in project 's PYTHONPATH
Remove path from project's PYTHONPATH
Add path to project's PYTHONPATH
Return data files for package *name* with extensions in *extlist*
Return subpackages of package *name*
Return Python documentation path
used by Spyder .
logging with cli options defined by the user .
are intercepted by this handler .
patching sys.exit and eventually setting up ETS
show Spyder 's main window
return toolbar with * title * and * object_name *
Setup main window
be performed only after the main window 's ` show ` method
Set window title.
Show a QMessageBox with a list of missing hard dependencies
Load window layout settings from userconfig-based configuration
Return current window settings
Set window settings
Save current window settings with *prefix* in
Tabify plugin dockwigdets
Setup window layout
Setup default layouts when run for the first time.
Reset window layout to default
Save layout dialog
Layout settings dialog
quick layout number * index *
Update the text displayed in the menu entry .
Saves the name of the visible toolbars in the .ini file .
Collects the visible toolbars .
Handle an invalid active project.
Show action shortcuts in menu
Hide action shortcuts in menu
Get properties of focus widget
Update edit menu
Update search menu
Set splash message
keep track of to the last focused widget
Add QDockWidget and toggleViewAction
Lock/Unlock dockwidgets and toolbars
Shortcut: Ctrl+Alt+Shift+M
widget actions to toolbar
Create About Spyder dialog with general information .
Show Spyder's Dependencies dialog box
sending it to Github
Report a Spyder issue to github , generating body text if needed .
Open external console
Execute lines in IPython console and eventually set focus
Open filename with the appropriate application
be handled either by the Editor or the
Return Spyder PYTHONPATH
Add Spyder path to sys.path
Remove Spyder path from sys.path
Spyder path manager
Projects PYTHONPATH contribution has changed
changed in 'Preferences ' dialog box
Update dockwidgets features settings
Update status bar widgets settings
Edit Spyder preferences
Register QAction or QShortcut to Spyder main application,
shortcuts settings to all widgets/plugins
reset Spyder and then Restart application .
Quit and Restart Spyder application.
Show interactive tour.
Open file list management dialog box.
Add a plugin to the File Switcher .
Called by WorkerUpdates when ready
using a QThread .
Private set method
Save config into the associated .ini file
Defines the name of the configuration file to use .
Create a .ini filename located in user home directory .
Set configuration (not application!)
Load config from the associated .ini file
Read old defaults
Save new defaults
Update defaults after a change in version
are present in the .ini file but not in defaults
Set defaults from the current config
Reset config to Default values
check section and option types
given ( section , option )
Get an option
Set an option
checking previously that it exists .
installed in PATH .
return absolute path
Given a dict , populate kwargs to create a generally
Execute the given shell command .
Run program in a separate process.
Generalized os.startfile for all platforms supported by Qt
return None )
Run Python script in a separate process
using shell-like syntax
Construct Python interpreter arguments
Run Python script in an external system terminal.
Check version string of an active module against a required version.
* module_name * is installed
Check that the python interpreter file has a valid name .
is a python interpreter or not .
Check that the python interpreter has 'pythonw ' .
Check that the python interpreter can execute help .
Draw the given breakpoint pixmap .
Override Qt method
Change visibility and connect/disconnect signal.
Qt/Python2/3 compatibility helper.
illustrates how the workers can be used .
Start the worker ( emits sig_started signal with worker as arg ) .
given method args and kwargs .
Return the encoding/codepage to use .
Set the environment on the QProcess.
Callback for partial output.
Callback for communicate.
Terminate running processes.
Delete periodically workers in workers bag.
check for inactive workers .
Create a new python worker instance .
Create a new process worker instance .
Terminate all worker processes.
Common worker setup.
Convert file data to a string for display.
simulate a recovery use case .
Add label with explanation at top of dialog window.
Add a label to specified cell in table .
Add a cancel button at the bottom of the dialog window .
working directory to parent directory
Breakpoint list has changed
has open quotes .
sig_key_pressed signal .
insert quotes in various situations .
Populate the given `` combobox `` with the class or function names .
Return a list of all the class/function definition ranges .
Adjust the parent stack in-place as the trigger level changes.
Split out classes and methods into two separate lists .
Get the parents at a given linenum .
Update the combobox with the selected item based on the parents .
Update the internal data values .
Move the cursor to the selected definition .
Updates the dropdowns to reflect the current class and function .
Set text editor palette colors:
Set extra selections for a key.
Add extra selections to DecorationsManager.
added through set_extra_selections .
Highlight current line
Highlight current cell
Set wrap mode
selected text as a processed text ,
text block ) is on a block separator
Select cell under cursor
Select cell under cursor in the visible portion of the file
Go to the next cell of lines
Go to the previous cell of lines
Restore cursor selection from position bounds
Duplicate current line or selected text
Move current line or selected text
Go to the end of the current line and create a new line
Extend current selection to complete lines
Delete current line
Unselect read-only parts in shell, like prompt
avoid editing text except between prompt and EOF
Hide calltip when necessary
is first moved at
Reimplemented to handle focus
Reimplemented to emit zoom in/out signals when Ctrl is pressed
Convert options into commands
Set a list of files opened by the project .
Return a list of files opened by the project .
Set project root path.
rename its root path accordingly .
Start pydoc server
Convert text address into QUrl object
autosave files where necessary .
recover files from autosave .
autosave file name for specified file name .
autosave file for specified file .
specified file name .
Autosave a file .
opened files .
returns a temporary CONF element .
Log last error in filename *fname* -- *context*: string (optional)
log all method calls into ` fname ` file .
holds the vertical offset of the scroll flag area
Return the pixel span height of the scrollbar area in which
Return the value span height of the scrollbar
Convert value to position in pixels
Convert position in pixels to value
Make flag QRect
Make slider range QRect
Set scroll flag area painter pen and brush colors
be performed on first plugin registration
Reimplement analyze method
Return physical memory usage (float)
is lighter or darker than the base color .
Gets the list of ParenthesisInfo for specific text block .
restore it when the wrapped
Show a wait cursor while the wrapped function is running .
Request a job execution.
pending requests .
Execute the requested job after the timer has timeout .
specified position .
is collapsed .
Gets the word under cursor using the separators defined by
Selects the word under the * * mouse * * cursor .
Returns the QTextCursor position .; is a tuple made up of
Gets the text of the specified line .
Replace an entire line with ``new_text``.
Removes the last line of the document .
trailing whitespaces and ensure one single blank line at the
Selects an entire line.
Returns the selected lines boundaries ( start line , end line )
Computes line position on Y-Axis (at the center of the line) from line
Returns the line number from the y_pos.
force a full refresh .
Gets the character that is on the right of the text cursor .
Inserts text at the cursor position.
Clears text cursor selection.
Moves the cursor on the right.
Extended selection consists in; extended word selection .
Sets the user state , generally used for syntax highlighting .
Sets the block fold level .
is a fold trigger .
fold trigger flag ( True means the block is a fold
is expanded or collased .
Sets the fold trigger state ( collapsed or expanded ) .
Uses a simplified version of the Perl detection algorithm ,
formatted text value .
stops timers if widget is not visible .
Set timer interval (ms).
Return memory usage.
Print a warning message on the rich text view
Print a usage message on the rich text view
Generate the html_context dictionary for our Sphinx conf file .
outputs the processed documentation .
Generates a Sphinx configuration in ` directory ` .
Update end of line status.
Update encoding of current file.
Update cursor position.
Update vcs status.
Retrieve all Variable Explorer configuration settings.
Change a config option .
Free memory signal.
Register shell with variable explorer.
Import data in current namespace
Return True if string is valid
Reimplemented to avoid formatting actions
Find tasks in source code (TODO, FIXME, XXX, ...)
Check source code with pyflakes
Return checker executable in the form of a list of arguments
defined with * args * ( list )
Check source code with pycodestyle
Return image inside a QLabel object
Return QApplication instance
Select the right file uri scheme according to the operating system
Install Qt translator to the QApplication instance
Extract url list from MIME data
Convert QKeyEvent instance into a tuple
Create a QToolButton
Create a QToolButton directly from a QAction object
Create a QAction
associated with a given action to its tooltip
Add actions to a QMenu or a QToolBar.
Create bookmark action
depending on module installation :
run a program
run a GUI based Python script
Return file type icon
Show all standard Icons
Return a spacing ( int ) or None if we do n't have the appropriate metrics
Returns a layout for a set of controls above a main widget.
show a non-modal dialog and keep reference
Add Spyder dependency
Check if required dependency is installed
Return a status of dependencies
Return the status of missing dependencies ( if any )
Check if dependency is installed
Return dependency status (string)
return the list
loads them .
add it to ` modlist `
Imports `module_name` from `plugin_path`.
Get standard platform icon
Return image inside a QIcon object.
Return the icon depending on the file extension
hide dialog box .
Returns a list of menu actions
Item selection has changed
excluding top level items )
Sorting tree wrt top level items
containing the names of all the modules available in the
fold tree to stdout , for debugging purpose .
Dict --> Dict of lists
Dict of lists --> Dict
Run Windows environment variable editor
Qt override.
Update the column and row numbering in the headers .
Return the entered array in a parseable form .
Construct the text based on the entered content in the widget .
Updates the icon and tip based on the validity of the array content .
selected language setting and save to language configuration file .
Check if path has write access
Get all available project types .
Update text of location.
Set shell styles font
Setup shell context menu
selecting console prompt )
Set input buffer
Return input buffer
Print a new prompt and save its ( line , index ) position
Check if selected text is r/w ,
clipboard ... or keyboard interrupt
Save current history log ( all text in console )
On new input line
Pre-process keypress event:
Post-process keypress event:
Load history from a .py file in user home directory
Simulate stdout and stderr
Flush buffer, write text to console
Insert text at the current cursor position
Drag and Drop - Drop event
Reimplements ShellBaseWidget method
clipboard without prompts
Process keypress event
Action for Backspace key
Action for TAB key
Action for '?
Action for '(
Action for '.
Reimplemented slot to handle multiline paste action
Display the possible completions
Display a completion list based on the current line
Drop path list
Command to start kernels
Env vars for kernels
Toggle horizontal scrollbar
Set the project directory
Start Python interpreter
is available in stdout , let 's empty the queue and write it !
is available in stderr , let 's empty the queue and write it !
raw_input support )
End of wait_input mode
Reimplement PythonShellWidget method
Help on Spyder console
Load file in external Spyder's editor, if available
Edit in an external editor
Reimplement ShellBaseWidget method
Flush keyboard event queue
Simulate keyboard interrupt
Execute a set of lines as multiple command
Execute a command
Run command in interpreter
Is object callable ?
Get func/method argument list
Get object documentation dictionary
Get object source
Return True if object is defined
Set edge line columns values.
used to send the args passed to the Spyder
Start Spyder application.
search for query letters in order .
enriched text ( if a template is provided ) and
return a list of tuples .
is the beginning of the function definition .
Get indent of text.
is located on the first def line .
is located below the last def line .
Get the function body text .
docstring to editor .
docstring to editor at mouse position .
docstring to editor by shortcut of code editor .
Generate a docstring of numpy type .
Get the locations of top-level brackets in a string .
Return the appropriate text for a group of return elements .
Generate the Returns section of a function/method docstring .
is in pairs of brackets or quotes .
Return the start and end position of pairs of quotes .
Return the start and end position of pairs of brackets .
name , type , value .
including multiple arguments to list .
Parse the function definition text .
Parse the function body text .
is not enter key .
Check if a process is running on windows systems based on the pid .
Show message on splash screen.
Animate dots at the end of the splash screen message.
Sets the text in the bottom of the Splash screen .
Launch a message box with a predefined error message .
Add new history tab
Toggle line numbers.
Handle the textDocument/didSave message received from an LSP server .
Overloaded method to handle links ourselves
Apply zoom factor
prevent WebEngine to steal focus
Go to page * address *
Load URL from combo box first item
Initialize plugin: connect signals, setup actions, etc.
Register QAction or QShortcut to Spyder main application.
Register widget shortcuts.
Set a plugin option in configuration file .
Showing message in main window 's status bar .
Clear main window's status bar and restore mouse cursor.
Show compatibility message.
Create options menu.
Action for '('
Reimplement Qt Method - Basic keypress event handler
Return the screen resolution of the primary screen .
Set the currently visible fig_browser in the stack widget , refresh the
Register shell with figure explorer.
Disable empty layout name possibility
execute function , ignoring EINTR error ( interruptions )
socket * sock *
Read data from socket * sock *
Communicate with monitor
return a time in seconds .
Simple test function
string ) and line_number ( int )
Get tree item user data: (filename, line_number)
Clean the tree and view parameters
saved by profile/cProfile module
Find a function without a caller
Populate the tree with profiler data and display it .
processed information about the function 's name and file .
coming from profiler task .
Return a string formatted delta for the values in x .
Formats the data.
create each item ( and associated data ) in the tree .
is a function is a descendant of itself .
Return all items with a level <= `maxlevel`
Change the view depth by expand or collapsing all same-level nodes
Returns a QSS stylesheet with Spyder color scheme settings.
Create a dictionary that saves the given color scheme as a
remove .pyc and .pyo files associated to a Python script
* dest *
Error handler for `shutil.rmtree`.
return a non used port
Return number of source code lines for all filenames in subdirectories
Remove backslashes in *path*
given class ; replace as needed .
Return common path for all paths in pathlist
trade memory for execution speed
is a valid regular expression or
Gets the fold region range ( start and end line ) .
Folds the region.
Unfolds the region.
generates the list of blocks directly under the fold
generates the list of direct child regions .
Return the parent scope .
Get the scope text , with a possible maximum number of lines .
fold level by looking at the block indentation .
Add a extension to the editor .
Remove a extension from the editor .
Remove all extensions from the editor .
Get a extension by name ( or class ) .
print text , taking into account backspaces
Set color scheme (foreground and background).
Set font style with the following attributes:
Set color scheme of the console (foreground and background).
Reimplement TextEditBaseWidget method
Append text to Python shell
Python Shell only
Return widget icon
Run pylint code analysis
used in Spyder .
Enable/disable the Qt style combobox.
Recreates the combobox contents .
Updates the enable status of delete and reset buttons .
Update the color scheme of the preview editor and adds text .
Creates a new color scheme with a custom name .
Edit current scheme.
Deletes the currently selected custom color scheme .
Restore initial values for default color schemes.
Set the current stack by 'scheme_name ' .
Get the values of the last edited color scheme to be used in an instant
Add a stack for a given scheme and connects the CONF values .
Remove stack widget by 'scheme_name'.
Switch to plugin.
update the menu actions .
Update actions of the Projects menu
Edit Spyder active project preferences
Create new project
Create a new project .
Open the project located in ` path `
return to a window without an active
deleting the files in the directory .
Reopen the active project when Spyder was closed last time , if any
Get the list of recent filenames of a project
Set the list of open file names in a project
Get path of the active project
be added to PYTHONPATH
opened projects & tree widget state .
Show the explorer
Check if a directory is a valid Spyder project
Add an entry to recent projetcs
attached to current process .
Return the first installed font family in family list
depending on OS and user options
be removed in Spyder 4.0
Create a Shortcut namedtuple for a widget
Iterate over keyboard shortcuts.
Get syntax color scheme
Set syntax color scheme
Reset color scheme to default values
Check if the font color used in the color scheme is dark .
Filter mouse press events.
called when a tab from a QTabBar has been pressed .
Show the context menu assigned to nontabs section .
capture mouse events in the tabs of a
Load all bookmarks from config.
Load all bookmarks for a specific file from config.
Load all bookmarks but those from a specific file.
Save all bookmarks from specific file to config.
start a LSP server to attend a language .
Register LSP server settings.
Notify all stackeditors about LSP server availability.
create a checkable action .
Handle the toogle of a checkable action .
Called when sig_option_changed is received .
Removing editorstack only if it 's not the last remaining
Setup toolbars and menus for 'New window' instances
* filename * if this file has been opened .
Enable/disable file dependent actions
are files to be saved
warning list menu
Update todo list menu
Opened files list has changed :
Update actions in the warnings menu.
save them .
Load temporary file from a text file in user home directory
Set current script directory as working directory
Add to recent file list
Clone file (*src_editor* widget) in all editorstacks
Create a new file - Untitled
Update recent file menu
Load a text file
Print current file
Print preview for current file
Save *as* the currently edited file
Reopens the last closed tab .
Close file from its name
was removed in project explorer widget
was renamed in file explorer widget or in project explorer
was renamed in file explorer or in project explorer .
debug current file
was just moved : 'go to
Open 'go to line' dialog
Set/Edit conditional breakpoint
Clear breakpoints in all files
Remove a single breakpoint
script inside current interpreter or in a new one
Debug current script
Re-run last script
Save current line and position as bookmark.
bookmarked file and position .
Get the list of open files in the current stack
based on active project .
Open the list of saved files per project .
For raw_input builtin function emulation
For help builtin function emulation
Return thread id
Run filename
Evaluate text and return (obj, valid)
Text has changed
Initialize the logger for this thread .
Restore signal handlers to their original settings.
warning using Tkinter if available
is Spyder properly installed ?
Check Qt binding requirements
Check spyder-kernel requirement .
given name is held or not .
Acquire this lock.
Release this lock.
render a given documentation
selected row .
based on contents .
Handle convention changes.
Adds an external path to the combobox if it exists on the file system .
listed in the combobox .
Returns the path corresponding to the currently selected item
Handles when the current index of the combobox changes.
Sets the project path and disables the project search in the combobox
Used to handle key events on the QListView of the combobox .
see if it is possible to emit the
Reimplemented to handle key events
sorting after search is complete .
Real-time update of search results
show event to start waiting spinner .
stop waiting spinner .
Stop current search thread and clean-up
Current search thread has finished
Get the stored credentials if any .
Store credentials for future use.
Store token for future use.
Get user credentials with the login dialog.
show the specified tip at the current cursor location .
hide the tooltip on leave .
Reimplemented to hide on certain key presses and on text edit focus
Reimplemented to hide the widget when the hide timer fires .
Reimplemented to cancel the hide timer .
Reimplemented to disconnect signal handlers and event filter .
Reimplemented to start the hide timer .
Reimplemented to connect signal handlers and event filter .
Hides the tooltip after some time has passed ( assuming the cursor is
Updates the tip based on user cursor movement .
Detect if text has mixed EOL characters
Use the same eol 's in text
passed string is the name of a Python builtin object
object in * source_code * at * offset *
Split source code into lines
Split source code into python identifier-like tokens
Return the individual components of a given file path
Return the differentiated prefix of the given two iterables .
Get tab title without ambiguation.
Get a list of the path components of the files with the same name .
Add text decorations on a CodeEditor instance.
Removes a text decoration from the editor .
Update editor extra selections with added decorations.
according draw_order and size of selection .
Get signature from inspect reply content
show signatures , using the same
Encode object as json str.
Generate authorize_url.
In callback url: http://host/callback?code=123&state=xyz
send its results via ZMQ .
Return current value
depending on value
Overriding method headerData
Overriding method flags
Decide if showing a warning when the user is trying to view
Overriding method createEditor
Overriding method setEditorData
Overriding method setModelData
Refresh context menu
Set table data
Reimplement Qt methods
Allow user to drag files
Allow user to move files
drop supported files
Toggle min/max display for numpy arrays
use in DataframeEditor .
Import data from string
Import text/data/code from clipboard
Remove values from data
Create new value in data
is a list or a tuple
is a set
Return sequence length
is a numpy array
is a PIL.Image image
is a dictionary
Return array's shape
Return array's ndim
Show item's image
is a PIL image )
Get the value of a variable
Get kernel id
save kernel stderr output .
Get handle to stderr_file.
associated with the client .
Configure shellwidget after kernel is started
handle what to do when the stop button is pressed
kernel initialization errors in infowidget .
Return client name
Return the text widget ( or similar ) to give focus to
Return options menu
Return toolbar buttons list.
widget context menu
Set IPython widget's font
Set IPython color scheme.
Restart the associated kernel .
kernel restarted/died messages .
Resets the namespace by removing all names defined by the user
Show sys.path contents.
Show environment variables.
show in time_label .
show/hide elapsed time label .
Set current info_page.
show while the kernel is starting
is loading .
shown while the kernel is loading .
Read the stderr file of the kernel .
setting the selected Matplotlib backend .
Calculate a global point position ` QPoint ( x , y ) ` , for a given
Update the background stylesheet to make it lighter .
Create HTML template for calltips and tooltips.
Create HTML template for signature.
Show calltip.
Show tooltip.
Set widget end-of-line (EOL) characters from text (analyzes text)
replace '\n '
offset in character for the given subject from the start of
Set cursor position
left or right ( unit : characters )
is on the first line
is on the last line
is before * position *
Move cursor to next *what* ('word' or 'character')
Clear current selection
Extend selection to next *what* ('word' or 'character')
Return text line at line number *line_nb*
Return text between *position_from* and *position_to*
given offset .
Return current word, i.e.
Return current line's text
Return line at *coordinates* (QPoint)
Return word at *coordinates* (QPoint)
Return line indentation (character number)
Return selection bounds (block numbers)
selected by current text cursor , converted in unicode
selected text by * text *
Reimplement QTextDocument's find method
Get the number of matches for the searched text .
Get number of the match for the searched text.
Go to error
Show Pointing Hand Cursor on error messages
has not been restored yet , do it now
docstring in the Help plugin
does n't exist .
Add command to history
Find text 'tocursor' in history, from index 'start_idx
Return a QKeySequence representation of the provided QKeyEvent .
Qt method extension.
Set the filter text.
Setup the ShortcutEditor with the provided arguments.
Qt method override.
Check shortcuts for conflicts.
Check if the first sub-sequence of the new key sequence is valid .
Update the warning label , buttons state and sequence text .
is a convenience method to set the new QKeySequence of the
Set the new sequence to the default value defined in the config .
conflicted shortcuts , and accept the new one
Get the currently selected index in the parent table view .
Update search letters with text input in search box.
Set regular expression for filter.
table model .
Save shortcuts from table model.
display the shortcut editor dialog .
Update the regex text for the shortcut finder .
Reset to default values of the shortcuts making a confirmation .
Get a color scheme from config using its name
inspired from idlelib.ColorDelegator.make_pat
Returns a code cell name from a code cell comment.
inspired from sublime highlighter
generate syntax highlighter for the given filename .
Please do not override , this method .
setting the foreground alpha .
Implement specific highlight for Python.
highlight specific for C/C++ .
highlight specific for Fortran .
highlight specific for Fortran77 .
highlight specific Diff/Patch files .
highlight specific for CSS and HTML .
Parses the complete text and stores format for each character .
highlight the block
Get all submodules of a given module
Get all submodules of the main scientific modules and others of our
is stable , i.e .
Return whether the dev configuration directory should used.
Output debug messages to stdout
Return user home directory
Return the path to a temp clean configuration dir , for tests and safe mode .
specified filename .
Return module *modname* base path
Return module *modname* data path
Return module *modname* source path
Return image absolute path
based on the folders found in the
has a translation available for the locale language , it will
setting from language config file if it exists , otherwise
Return translation callback for module *modname*
Remove all config files
Refresh explorer widget
is type text string , False if it is anything else ,
binary string ( bytes in Python 3 , str in Python 2 )
Return the widget to give focus to when
Synchronize Spyder's path list with PYTHONPATH environment variable
Update path list
Catch clicks outside the object and ESC key press.
Activate the edit tab .
On clean exit, update tab name.
trigger the tab name editor .
Update browse tabs menu
Set tabs corner widgets
Setting Tabs close function
Move tab inside a tabwidget
Move tab from a tabwidget to another
Return script *fname* run configuration
widgets/spacing to dialog vertical layout
add it to the dialog layout
Setup Run Configuration dialog with filename *fname*
Compute and return line number area width
Setup margin settings
Returns a list with line number, definition name, fold and token.
Return a list of icons for oedata of a python file .
Takes a list of paths and tries to `` intelligently '' shorten them all .
goes out of this widget .
Save initial cursors and initial active widget.
Restores initial cursors and initial active editor.
Positions the file switcher dialog.
Get the max size ( width and height ) for the elements of a list of
Adjusts the width and height of the file switcher
based on a number of steps with direction .
Select previous row in list widget.
Select next row in list widget.
Get the real index of the selected item .
Get the data object of the plugin 's current tab manager .
Get the tabwidget of the plugin 's current tab manager .
Get widget by index.
Set the cursor of an editor.
Go to specified line number in current active editor .
List widget item selection change handler.
Setup list widget content for file list display.
Setup list widget content for symbol list display.
Setup list widget content.
Add a plugin to display its files .
is under VCS root
Return VCS root directory path
is a valid VCS repository , run the corresponding VCS tool
located at repopath
Return Git active branch, state, branches (plus tags).
is a Python module/package
hitting tab , it handles if single or double tab
combo box : add a new item if text is not found in
combo box history if valid
Validate entered path
restoring to display the status icon .
focus out event restoring the last valid selected path .
Find available completion options.
If several options available a double tab displays options.
is a single option available one tab completes the option .
be executed when a valid item has been selected
combo box history ( convenient method ) .
Add a tooltip showing the full path of the currently highlighted item
Bind historylog instance to this console
ocurred in the internal console .
Close error dialog.
Run a Python script
give focus to shell
Change external editor path
Toggle automatic code completion
executed when running the script with the -install switch
executed when running the script with the -remove switch
generates a list of tours .
override Qt method
is displayed in each step of the tour .
loses focus and hides the tips .
regains focus and unhides the tips .
has focus .
Reimplemented to handle communications between the figure explorer
Get file language from filename
Line edit's text has changed
fname in the format specified by fmt .
Append a number to `` root '' to form a filename that does not already exist
Setup the figure browser with provided settings.
Setup the toolbar
Create shortcuts for this widget.
Handle when the value of an option has changed
Draw a frame around the figure viewer if state is True .
Bind the shellwidget instance to the figure browser
Copy figure from figviewer to clipboard.
Setup the FigureCanvas.
Set a new figure in the figure canvas .
control the zooming and panning of the figure canvas .
Scale the image up by one scale step.
Scale the image down by one scale step.
Scale the image size.
take into account the zooming of
Setup the main layout of the widget.
contain the FigureThumbnails .
arrow buttons that are placed at the top and
Save all the figures to a file.
Save all figure in dirname.
Save the currently selected figure .
Save the figure to a file .
Remove all thumbnails .
Set the currently selected thumbnail .
Select the thumbnail previous to the currently selected one .
selected one .
selected item of ThumbnailScrollBar .
Scroll the scrollbar of the scrollarea up by a single step.
Setup the toolbar.
Set a colored frame around the FigureCanvas if highlight is True .
is used to send a signal when the figure canvas is
Emit a signal when the toolbutton to save the figure is clicked .
Popup context menu.
Copy figure to clipboard.
Blink figure once.
was painted on the widget .
convert it in
paint a custom image on the Widget .
has been changed
excluded modules name list
Update the list of interpreters used and the current one .
Check that the used custom interpreters are still valid .
Extends :meth:`spyder.api.EditorExtension.on_install` method to set the
using QPalette .
Shows/Hides the panel.
floating panels .
decided to replace pyzmq 's openssh_tunnel method to work around
associated namespace browser widget
Set the namespace view settings
Ask kernel for a value
Set value for a variable
Remove a variable
Copy a variable
Handle internal spyder messages
Reimplemented to handle communications between Spyder
Reimplemented to refresh the namespacebrowser after kernel
is the first time the plugin is shown , perform actions to
Update plugin margins
Update plugin title, i.e.
parent QMainWindow as a dock widget
Create configuration dialog box page widget
Return plugin font option.
Show message in main window's status bar
Associate a toggle view action with each plugin
contains this plugin .
Create a QMainWindow instance containing this plugin .
perform when a plugin is undocked to be moved .
Reimplemented Qt Method to avoid removing the header .
take when pressing the submit button .
Show traceback on its own dialog
is an unmatched brackets in the 'text ' .
Control automatic insertation of brackets in various situations.
---------------------------------------------------------------------------------------------
Adjust the brightness and/or contrast of an image
drawing text with line breaks
drawing vertically & horizontally centered text with line breaks
take the corners from cv2.GoodFeaturesToTrack and return cv2.KeyPoints
is a directory containing a setup.py file .
Retrieve a setting value .
Returns a list of requirements for building, as strings
places it in wheel_directory
places it in sdist_directory
Returns all external incompatibilities in this incompatibility's
Adds an assignment of package as a decision
Adds an assignment of package as a derivation .
Adds an Assignment to _assignments and _positive or _negative .
Resets the current decision level to decision_level , and removes all
Registers an Assignment in _positive or _negative.
Returns the first Assignment in this solution such that the sublist of
Checks the validity of a configuration
Discover subpackages and data.
make it more reproducible .
Retrieve the current shell .
match the given dependency .
match the given VCS dependency .
encapsulate a given package 's dependencies ,
match the root package 's constraints ,
Performs unit propagation on incompatibilities transitively
is almost satisfied by _solution , adds the
Given an incompatibility that 's satisfied by _solution ,
select a version of a required package .
Creates a # SolverResult from the decisions in _solution
Run a command inside the Python environment .
given executable .
help in transforming
Use a fallback method for determining SOABI flags if the needed config
given package matches this dependency .
Set the dependency as optional.
Installs Poetry in $POETRY_HOME.
Packs everything into a single lib/ directory.
update the $ PATH automatically .
satisfies another .
Returns the relationship between the package versions
represents the packages
add to the tarball
Parse the given version string and return either a : class : ` Version ` object
is still up to date with the current hash .
returns a repository of locked packages .
Returns the sha256 hash of the sorted content of the pyproject file .
installed packages .
Given a package name and optional version ,
is , if a ' ' shows up in; sure a link is fully encoded .
Retrieve the release information.
Register a new logger.
Register a package to a repository.
Find packages on the remote server.
Return the package information given its name .
Return the release information given a package name and a version .
Build a wheel in the dist/ directory , and optionally upload it .
'm not sure if using the; is a fallback technique at best .
Prepare the installer for locking only .
Execute a given operation .
required by extras .
Returns the json representation of the dep graph
matching the given `` job_name_prefix `` .
matching the given `` status `` .
Move file atomically.
dest `` , but do n't move it into the `` dest ``
do not exist .
is done by requesting each job and then searching for whether the job
Read in the error file from bsub
Read in the output file
Build a bsub argument that will run lsf_runner.py on the directory we 've specified .
~.UserItemMatrix.data_size ` elements .
Returns the target output for this task .
string to time.struct_time : change datetime.datetime to time , return time.struct_time type
Runs one instance of the API server.
queried columns names .
ordered dictionary , calls _traverse_results ( ) to recursively read into the dictionary depth of data
Helper method for parse_results().
resulting files of a multi-result batch bulk query .
Starts a Salesforce session and determines which SF instance to use for future requests .
Return the result of a Salesforce SOQL query as a dict decoded from the Salesforce response JSON payload .
returned more results
Returns the full set of results for the `query`.
make a direct REST call if you know the path
Creates a new SF job that for doing any operation ( insert , upsert , update , delete , query )
Gets all details for existing job
existing job .; is aborted , no more records are processed .
Closes job
Creates a batch with either a string of data or a file containing data .
is completed or failed .
DEPRECATED: Use `get_batch_result_ids`
has completed processing .
result back from Salesforce as whatever type was originally sent in create_batch ( xml , or csv ) .
Parse "state" column from `qstat` output for given job_id
Submit shell command to SGE queue via `qsub`
Dump instance to file.
Writes data in JSON format into the task 's output target .
Creates the client as specified in the ` luigi.cfg ` configuration .
For internal use only (until further notice)
notification through AWS SES .
notification through AWS SNS .
Decides whether to send notification .; is cancelled if there are
Sends an email to the configured error email , if it 's configured .
related to a luigi.task.Task
Returns true if the path exists and false otherwise.
Has no returnvalue ( just like WebHDFS )
touchz using the web hdfs `` write '' cmd .
Open the target for reading or writing
bogus data and writes it into the : py : meth : ` ~.Streams.output ` target .
This task's dependencies:
based on a MD5 hash generated with the task 's name and its arguments
mark the task as ` done `
provided path exist on S3 ?
Remove a file or directory from S3 .
Returns the object summary at the path
Put an object stored locally to an S3 path .
Put a string to an S3 path .
Put an object stored locally to an S3 path
Copy object(s) from one S3 location to another.
Get an object stored in S3 and write it to a local path .
Get the contents of an object stored in S3 as bytes
Get the contents of an object stored in S3 as string .
Is the parameter S3 path a directory ?
Get an iterable with S3 folder contents .
Return `True` if file or directory at `path` exist, False otherwise.
Remove file or directory at location ``path``.
delete a directory tree on a remote server .
Put file from local filesystem to ( s ) FTP .
Download file from (s)FTP to local filesystem.
Gets an list of the contents of path in (s)FTP
Open the FileSystem target .
targeted mongo collection to query on
Read the target value
Write value to the target
Read the targets value
Write values to the targeted documents
id with missing targeted field
Get logging settings from config file section "logging".
logging via CLI params and config .
logging via CLI options
logging via ini-file from logging_conf_file option .
Setup default logger
is a helper that fetches the configuration value for 'client ' in
is needed to generate temporary location
Rename/move an object from one GCS location to another.
Get an iterable with GCS folder contents .
object URIs matching the given wildcard .
Downloads the object contents to local file system .
Closes and waits for subprocess to exit.
is complete , puts the result to out_queue .
Call `` self._scheduler.add_task `` , but store the values too so we can
Add a Task for the worker to check and possibly schedule and run .
put a response on the result queue .
have to catch three ways a task can be `` done '' :
stay alive given .
True if all scheduled tasks were executed successfully .
is up to date with the codebase .
given task_name and the same parameters as the kwargs .
have been updated .
Return all running/failed/done events.
given record ID .
Returns a dictionary with keyword arguments for use with discovery
Return a credential string for the provided task .
are implemented .
create the schema in the database
provide code for creating the target table .
does n't exist , self.create_table
copying from s3 into redshift .
Determine whether the schema already exists.
Determine whether the table already exists.
Perform pre-copy sql - such as creating table , truncating , or removing data older than x .
cleansing data , inserting into production table ( if copied to temp table ) , etc .
post-copy to fill metadata columns .
copying JSON from s3 into redshift .
representing the inserted dataset .
given database .
Returns a list of dates in this date interval.
returns 24 times more info : one for each hour .
write 4 lines of data on this task 's target output .
Copies the contents of a single file path to dest
does n't have any meaning .; Removes the given mockfile .
Moves a single file from path to dest
does a prefix match of self.get_all_data ( ) , but does n't yet support globs .
Call MockFileSystem's move command
walks `` Mapping `` s and `` list `` s and converts them to `` _FrozenOrderedDict `` and `` tuples `` , respectively .
does n't exist; Loads the default from the config .
Yield the parameter values , with optional deprecation warning as second tuple value .
Parse a list of values from the scheduler .
Parses a date string formatted like `` YYYY-MM-DD `` .
using the : py : attr : ` ~_DateParameterBase.date_format ` .
Add ``months`` months to ``date``.
Clamp dt to every Nth :py:attr:`~_DatetimeParameterBase.interval` starting at
Parses a `` bool `` from the string , matching 'true ' or 'false ' ignoring case .
Parses a : py : class : ` ~luigi.date_interval.DateInterval ` from the input .
Parses a time delta from the input .
Converts datetime.timedelta to a string
Parse an individual value from the input .
~.InputText.output ` targets created by : py : class : ` ~.InputText `
Create a tar archive which will contain the files for the packages listed in packages .
flattens a sequence .
Runs the job by invoking the command from the given arglist .
mechanize to fetch the actual task logs from the task tracker .
Get the MapReduce runner for this job .
is a method which iterates over the output records
Increments a Hadoop counter.
Increments any unflushed counter values.
call the mapper for each item .
call the reducer for each unique key .
Run the mapper on the hadoop node .
Run the reducer on the hadoop node .
uses python eval on each part of a tab separated string .
outputs the python repr for each item .
Mark this update as complete.
Get a psycopg2 connection object to the database where the table is .
corresponding to each row to be inserted .
Applied to each column of every row returned by ` rows ` .
generated by rows ( ) into target table .
Get configs singleton for parser
add path into parser .
compresses and uploads packages to the cluster
Run either the mapper, combiner, or reducer from the class instance in the file "job-instance.pickle".
luigi with command line parsing , but raise `` SystemExit `` with the configured exit code .
provided CLI task
Returns a task's output as a string
cover paths and not too much extra .
Wanted functionality from Counters ( new in Python 2.7 ) .
listing existing output paths .
Yields a (filesystem, glob) tuple per every output location of task.
existing paths .; Get all the paths that do in fact exist .
determines missing datetimes by filesystem listing .
be deleted soon .
consider the entire range , but
do bulk checks .
be deleted eventually ( stated on Dec 2015 )
Given a dictionary of parameters , will extract the ranged task parameter value
returns the points in time that correspond to turn of day .
returns the points in time that correspond to whole hours .
returns the points in time that correspond to a whole number of minutes intervals .
returns the points in time that correspond to turn of month .
Create a SQL Server connection and return a connection object
Retrieve an opener for the given protocol
Adds an opener to the registry
Open target uri.
Converts the query string from a target uri, uses
use values from the parsed uri to initialize
return a detailed response of type LuigiRunResult
Please dont use.
bypassing the cmdline parsing .
use temporary files when used for output .
Get name of first active job queue
Retrieve the first job ID matching the given name
Retrieve task statuses from ECS API
Retrieve log stream from CloudWatch
Wrap submit_job with useful defaults
Poll task status until STOPPED
using a JSON
Run the work ( ) method from the class instance in the file `` job-instance.pickle '' .
picks an object in input and reads the Avro schema from it .
Return a string representation of the tasks , their statuses/parameters in a dependency tree format
base URLs like urllib.parse.urljoin but support
given dataset exists .
given table exists .
Creates a new dataset with the default permissions .
Deletes a dataset ( and optionally any tables in it ) , if it exists .
Deletes a table , if it exists .
Returns the list of datasets in a given project .
Returns the list of tables in a given dataset .
Returns the SQL query for a view , or None if it does n't exist or is not a view .
Updates the SQL query for a view .
Runs a BigQuery `` job '' .; See the documentation for the format of body .
appends ) a table to another table .
takes a : py : class : ` BQTable ` .
The fully-qualified URIs that point to your data in Google Cloud Storage.
The fully-qualified URIs that point to your data in Google Cloud
Execute a shell command remotely and return the output .
Open a tunnel between localhost : local_port and remote_host : remote_port via the host specified by this context .
Return `True` if directory at `path` exist, False otherwise.
Remove file or directory at location `path`.
Returns command of process.
sure the process is only run once at the same time with the same name .
Add a failure event with the current timestamp .
Return the number of failures in the window .
's not an assistant having only tasks that are without
are PENDING + RUNNING .
given task .
add task identified by task_id if it does n't exist
Returns the dependency graph rooted at task_id
Query for a subset of tasks by status.
Resources usage info and their consumers (tasks).
get total resources and available ones
Query for a subset of tasks by task_id.
exists ; `` False `` otherwise .
enables a reasonably short , general and
Generate an id for the indicator document.
has been run .
keep track of the tasks if necessary .
Shrink the history of updates for
yield documents that do not explicitly contain ` _index ` or ` _type ` ,
provide code for creating the target index .
Delete the index, if it exists.
Run task, namely:
Gets the value of the section/option using method .
Poll job status while active
has successfully started
Return the Kubernetes job status
creating it if it does n't exist .
does the actual insertion of the rows of data given by ins_rows into the
change self.path .
Alias for ``rename()``
works with hadoopcli
Takes a worker and sorts out tasks based on their status .
Add the `` upstream_ * '' and `` not_run '' statuses my mutating set_tasks .
checks why tasks are still pending .
returns a string for each status
is a continuous range
Get the human readable comments and quantities for the task types .
returns a set of the tasks that are being run by other worker
returns a dict with a set of tasks for all of the other workers
Takes a dictionary with sets of tasks grouped by their status and
Given a grouped set of tasks , returns a LuigiStatusCode
used to identify a particular task
both pass an
Maps all Tasks in a structured data object to their .output().
Creates a flat list of all all items in structured output ( dicts , lists , items ) :
walking output-less ( wrapper ) tasks .
has forked , we have a different PID , and need to reconnect .
Use snakebite.rename, if available.
Use snakebite.rename_dont_move, if available.
Use snakebite.delete, if available.
Use snakebite.chmod, if available.
Use snakebite.chown/chgrp, if available.
Use snakebite.count, if available.
Use snakebite.copyToLocal, if available.
Using snakebite getmerge to implement this .
Use snakebite.mkdir, if available.
get the list of items in a directory .
given a module and a task name .
Return all of the registered classes.
writing complement of _get_reg
raises an exception .
Compiles and returns all parameters for all :py:class:`Task`.
Simple unweighted Levenshtein distance
>>> list(Register._module_parents('a.b'))
perform custom queries .
are found in task_cls .
Lets a task call methods on subtask ( s ) .
Return a previous Task of the same family .
Given that we want one of the hadoop cli clients ( unlike snakebite ) ,
-stat `` to check file existence .
creates parent directories .
Runs the ` hive ` from the command line , passing in the given args , and
Runs the contents of the given script in hive and returns stdout .
be passed along
Called before job is started .
Returns the path to this table in HDFS .
be used as a context manager .
Get the local task arguments as a dictionary .
Check if the user passed -- help [ -all ] , if so , print a message and exit .
given current file and relative path .
pass to the job .
Adds an event to the event file .
given bytes to be written asychronously
enqueued bytestring before this flush call to disk .
Closes the underlying writer , flushing any pending writes first .
carrying tensor value .
Create a dict ( ) as the outgoing data in the tensor data comm route .
Add a GraphDef .
Get the runtime GraphDef protos associated with a run key .
Get the runtime GraphDef proto associated with a run key and a device .
Obtain possibly base-expanded node name.
Implementation of the core metadata-carrying Event proto callback.
Implementation of the GraphDef-carrying Event proto callback.
based on an updated message from the debugger .
Add a DebuggedSourceFile proto .
Get the traceback of an op in the latest version of the TF graph .
Get the lists of ops created at lines of a specified source file .
given debugged tensor value .
Construct a werkzeug Response.
Finds the longest `` parent-path '' of 'path ' in 'path_set ' .
Returns the type of the google.protobuf.Value message as an api.DataType .
given google.protobuf.Value message .
Finds the experiment associcated with the metadata.EXPERIMENT_TAG tag .
Computes a minimal Experiment protocol buffer by scanning the runs .
Computes a list of api_pb2.HParamInfo from the current run , tag info .
Builds an HParamInfo message from the hparam name and list of values.
Computes the list of metric names from all the scalar ( run , tag ) pairs .
Handles the request specified on construction .
Creates a summary that defines a hyperparameter-tuning experiment .
Constructs a SessionStartInfo protobuffer .
Constructs a SessionEndInfo protobuffer .
holding the given HParamsPluginData message .
returns if parent/item is a directory .
have registered assets in logdir .
are available for given plugin in a logdir .
Retrieve a particular plugin asset from a logdir .
Result of the form `(body, mime_type)`, or `ValueError`.
Given a tag and single run , return an array of compressed histograms .
Loads new values.
Internal implementation of Load().
Sets the current path to watch for new events .
Gets the next path to load from .
has had an out-of-order write .
Returns a number of examples from the provided path .
Send an RPC request to the Servomatic prediction service .
Convert `value` to a new-style value, if necessary and possible.
Obtains a mapping between routes and handlers .; Stores the logdir .
Returns JSON of the specified examples.
Updates the specified example .
Duplicates the specified example .
Deletes the specified example .
comma separated request arguments
Returns JSON for the `vz-line-chart`s for a feature.
Returns a list of JSON objects for each feature in the example.
Serves a pre-gzipped static asset from the zip file .
Serve a JSON object containing some base properties used by the frontend .
Serve a JSON array of run names , ordered by run started time .
are ordered by experiment; Serve a JSON array of experiments .
Serve a JSON runs of an experiment , specified with query param
Adds standard TensorBoard CLI flags to parser.
Fixes standard TensorBoard CLI flags to parser.
Put a message into the outgoing message stack .
Get message(s) from the outgoing message stack.
generate event files .
Returns the plugin , if possible .
Stores a config file used by the embedding projector .
's define functions so tf.flags accepts old names .
storing the evaluations of the specified metric .
Returns the last evaluations of the given metric at the given session .
Return {runName: {tagName: {displayName: ..., description: ...}}}.
Result of the form `(body, mime_type)`.
given blob and dtype enum .
Given a tag and single run , return array of ScalarEvents .
Add a run to the multiplexer .
runs from a directory ; recursively walks subdirectories .
Call ` Reload ` on every ` EventAccumulator ` .
given plugin .
Return the contents for a specific plugin asset from a run .
Retrieve the scalar events associated with a run and tag .
Get the session.run ( ) metadata associated with a TensorFlow run and tag .
Retrieve the audio events associated with a run and tag .
Retrieve the tensor events associated with a run and tag .
run : { tag : content } } .
Return the summary metadata for the given tag on the given run .
Return all the run names in the `EventMultiplexer`.
Write a text summary .
Create a text tf.Summary protobuf .
Initializes flags and calls main().
Create a ` summary_pb2.SummaryMetadata ` proto for image plugin data .
Create a legacy audio summary op for use in a TensorFlow graph .
Create a legacy audio summary protobuf .
Create a PR curve summary op for a single binary classifier .
Create a PR curves summary protobuf .
Computes a precision-recall curve summary across batches of data .
collects data for visualizing PR curves .
Create a PR curves summary protobuf from raw data values .
generating a tensor summary .
Executes the request .
is active iff any run has at least one histograms tag .
Given a tag and single run , return array of histogram values .
Initialize the graph and session , if this has not yet been done .
get the scalars plugin .
is active if 2 conditions hold .
downloading scalars data for a data series .
Given a tag regex and single run , return ScalarEvents .
specified by the config file in the logdir .
Given an iterable of string contents , make a table row .
Given a numpy ndarray of strings , concatenate them into a html table .
Given a np.npdarray with nDims > 2 , reduce it to 2d .
Take a numpy.ndarray containing strings , and convert it into html .
Convert a TensorEvent into a JSON-compatible response .
Determines whether this plugin is active .
launch a thread to compute index_impl ( ) .
index_impl ( ) asynchronously on a separate thread .
Create a ` summary_pb2.SummaryMetadata ` proto for pr_curves plugin data .
Parse summary metadata to a Python object.
Return a field to ` Observations ` dict for the event generator .
query over .
Prints a shallow dict to console.
Transform the field-to-obs mapping into a printable dictionary .
break the monotonically non-decreasing trend .
Returns a list of event generators for subdirectories with event files.
objects given either logdir or event_file .
prints out a digest of event files .
Adds DebuggerPlugin CLI flags to parser.
Returns the debugger plugin , if possible .
Returns a summary metadata for the HParams plugin.
Returns a data oneof's field from plugin_data.content.
Writes an event proto to disk .
making it no longer usable .
Creates a new events writer .
Obtains the names of debugger-related events files within the directory .
Re-export all symbols from the original tf.summary.
PNG on ` thread_count ` threads in parallel .
Generate a square RGB test image of the given side length .
Format a line of a table.
Extract all nodes with gated-gRPC debug ops attached.
Expand the base name if there are node names nested under the node .
detected run .
Returns a batched event iterator over the run directory event files.
Processes a single tf.Event and records it in tagged_data .
Create a TensorFlow op to group data into histogram buckets .
Create a legacy histogram summary op .
Create a legacy histogram summary protobuf .
Add a tensor the watch store .
Get number of values in memory.
Get the number of values discarded due to exceeding both limits .
given time indices .
Add a tensor value .
given watch_key .
listening on the given gRPC port .
wrapped ) werkzeug handler for serving health pills .
Obtains the health pills for a run sampled by the event multiplexer .
Converts an event_accumulator.TensorEvent to a HealthPillEvent.
obtain the health pills for a run at a specific step .
Creates health pills out of data in an event .
Creates a HealthPillEvent containing various properties of a health pill .
wrapped ) werkzeug handler for serving numerics alert report .
Convert a ` TensorBoardInfo ` to string form to be stored on disk .
Parse a ` TensorBoardInfo ` object from its string representation .
Compute a ` TensorBoardInfo.cache_key ` field .
store info files .
Write TensorBoardInfo to the current process's info file.
Remove the current process 's TensorBoardInfo file , if it exists .
running TensorBoard processes .
Start a new TensorBoard instance , or reuse a compatible one .
running TensorBoard instance compatible with the cache key .
Read the given file , if it exists .
returns the UI data .
is active and has any profile data to show .
maps a frontend run name to a profile `` run '' directory .
run name '' and a list of tools for that run .
Returns available hosts for the run and tool in the log directory.
processes the tool data for a run and a host .
Run a temperature simulation .
Run simulations on a reasonable set of parameters.
Makes Python object appropriate for JSON serialization.
Create a legacy text summary op .
Create a legacy text summary protobuf .
Return the string message associated with TensorBoard purges .
given path string .
Convert the string file_version in event.proto into a float.
added since the last call to ` Reload ` .
Return the contents of a given plugin asset .
Returns the timestamp in seconds of the first event .
content specific to that plugin .
found in the value stream .
Return the graph definition , if there is one .
Return the metagraph definition , if there is one .
Given a tag , return the associated session.run ( ) metadata .
orphaned data due to a TensorFlow crash .
expired events using SessionLog.START .
Check for out-of-order event.step and discard expired events for tags .
Processes a proto histogram by adding it to accumulated state .
Callback for _ProcessHistogram.
Processes an image by adding it to accumulated state .
Processes a audio by adding it to accumulated state .
Processes a simple value by adding it to accumulated state .
have occurred after the given event.step .
serialized proto bytestrings .
Loads all new events from disk.
input: unscaled sections.
Parses the session_run_index value from the event proto .
fixed size histogram by adding compression to accumulated state .
Affinely map from [x0, x1] onto [y0, y1].
plugin is active iff any run has at least one relevant tag .
Given a tag and list of runs , serve a list of metadata for images .
Builds a JSON-serializable object with information about images.
Returns the actual image bytes for a given image .
Serves an individual image .
Generate a PR curve with precision and recall evenly weighted .
Generate PR curve summaries.
Write an image summary.
Sets the examples to be displayed in WIT .
Sets the model for inference as a TF Estimator .
Sets a second model for inference as a TF Estimator.
Sets a custom function for inference.
Sets a second custom function for inference.
Reshape a rank 4 array to be rank 2 , where each column of block_width is
Reshapes arrays of ranks not in {1, 2, 4}
input: unprocessed numpy arrays.
Computes the variance of corresponding sections over time .
Clears the deque if certain parts of the config have changed .
define a function that lazily loads the module 'name ' .
Memoizing decorator for f , which must have exactly 1 hashable argument .
Provide the root module of a TF-like API for use within TensorBoard .
Provide the root module of a TF-2.0 API for use within TensorBoard .
Provide pywrap_tensorflow access in TensorBoard.
holding this experiment .
Runs a temperature simulation .
Return the registered filesystem for the given file .
Recursive directory tree generator for directories.
Reads contents of a file to a string.
match the given pattern ( s ) .
contained within a directory .
file statistics for a given path .
Split an S3-prefixed URL into bucket and path.
exists or not .
is a directory or not .
Determine the most specific context that we 're in .
display a TensorBoard instance as if at the command line .
given TensorBoardInfo .
Display a TensorBoard instance already running on this machine .
Internal version of `display`.
Display a TensorBoard instance in a Colab output frame .
Print a listing of known running TensorBoard instances .
Check the path name to see if it is probably a TF Events file .
given directory .; are absolute .
Escapes the glob characters in a path.
lists all files within the directory .
yielding ( dir_path , file_paths ) tuples .
Obtains all subdirectories with events files.
Write an audio summary.
contains bad values .
Obtain the first timestamp.
Obtain the last timestamp.
Creates a JSON-able representation of this object .
alerting numeric event .
Get a report of offending device/tensor names .
given form .
Emit a sine wave at the given frequency .
Emit a triangle wave at the given frequency .
Emit two sine waves, in stereo at different octaves.
left and right .
defined above .
be served to the front-end .
Returns a dict of all runs and tags and their data availabilities.
Result of the form `(body, mime_type)`, or `None` if no graph exists.
Result of the form `(body, mime_type)`, or `None` if no data exists.
Given a single run , return the graph definition in protobuf format .
Given a tag and a run , return the session.run ( ) metadata .
Create a Keras model with the given hyperparameters .
Run a training/validation session .
normalize data .
Perform random search over the hyperparameter space.
Sample a value uniformly from a domain .
returns a JSON mapping between runs and PR curve data .
Creates the JSON object for the PR curves response for a run-tag combo .
Creates the JSON object for the tags route response .
Creates the JSON object for the available time entries route response .
encapsulates information on it .
corresponds to 1 step .; Creates an entry for PR curve data .
Normalize a dict keyed by ` HParam ` s and/or raw strings .
Create a top-level experiment summary describing this experiment .
Generate a bunch of histogram data , and write it to logdir .
asserts a positive ( > 0 ) integer query parameter .
Adds a named column of metadata values .
ProjectorConfig ` protos .
Retrieve the histogram events associated with a run and tag .
Retrieve the compressed histogram events associated with a run and tag .
Retrieve the image events associated with a run and tag .
Write a histogram summary .
Create a histogram summary protobuf .
recommended modifications to the environment .
web assets collection .
Create a server factory that performs port scanning .
Configures TensorBoard behavior via flags.
Blocking main function for TensorBoard .
launching TensorBoard .
Write a TensorBoardInfo file and arrange for its cleanup .
Set a signal handler to gracefully exit on the given signal .
Constructs the TensorBoard WSGI app and instantiates the server .
Returns a wildcard address for the port in question.
enable IPV4 mapping for IPV6 sockets when desired .
get rid of noisy EPIPE errors .
Iterator over all catapult trace events, as python values.
Converts a TraceEvent proto into a catapult trace event python value.
Create a legacy scalar summary op .
Create a legacy scalar summary protobuf .
temp symlink tree , runs program , and copies back outputs .
run function using a JSON file config .
Initializes the TensorBoard sqlite schema using the given connection .
created DB-wide unique ID .
Returns the ID for the current user , creating the row if needed .
Returns the ID for the given experiment , creating the row if needed .
Returns the ID for the given run , creating the row if needed .
given tags , creating rows if needed .
writes the given tagged summary data to the DB .
Get the raw encoded image data , downloading it if necessary .
Perform a 2D pixel convolution on the given image .
Get the image as a TensorFlow variable .
Run a box-blur-to-Gaussian-blur demonstration .
Run a Sobel edge detection demonstration .
Get the value of a feature from Example regardless of feature type .
Returns an `OriginalFeatureList` for the specified feature_name.
packaged inference results from the provided proto .
Returns a list of feature names for float and int64 type features.
Returns a list of feature names for byte type features.
Returns numerical features and their observed ranges.
Returns categorical features and a sampling of their most-common values.
Return a list of ` MutantFeatureValue ` s that are variants of original .
Return a list of ` MutantFeatureValue ` s and a list of mutant Examples .
formatted for rendering all charts for a feature .
formatted for a single mutant chart .
Returns the non-sequence features from the provided example .
wraps the inference results .
Returns a list of JSON objects for each feature in the examples.
loaded from the provided path .
Returns an encoded sprite image for use in Facets Dive.
given model information
associated with given key .
Add a new item to the Reservoir with the given tag .
using a filtering function .
replacing an old item if necessary .
Returns the inferred dense dimensions of a list of lists .
Create a TensorProto .
Create a numpy ndarray from a tensor .
Creates a summary that contains a layout .
is convertible with this Dimension .
combines the information in ` self ` and ` other ` .
Returns the rank of this shape , or None if it is unspecified .
Returns the total number of elements , or none for incomplete shapes .
combining the information in ` self ` and ` other ` .
Returns the concatenation of the dimension in ` self ` and ` other ` .
Raises an exception if ` self ` and ` other ` do not have convertible ranks .
based on ` self ` with the given rank .
based on ` self ` with at least the given rank .
based on ` self ` with at most the given rank .
is convertible with ` other ` .
Returns the most specific TensorShape convertible with ` self ` and ` other ` .
is fully defined in every dimension .
Returns a list of integers or `None` for each dimension.
Returns this shape as a `TensorShapeProto`.
Converts a PredictResponse to ClassificationResponse or RegressionResponse.
Converts tensor values into ClassificationResponse or RegressionResponse.
Called whenever an event is loaded .
Walks the nested keras layer configuration in preorder.
input_to_in_layer , model_name_to_output , and prev_node_name
Returns a GraphDef representation of the Keras model in a dict form.
True if the hparams plugin is active .
's safe to splice into the DOM .
Converts the given ` type_value ` to a ` DType ` .
Returns the dtype correspond to this dtype 's real part .
is a ( non-quantized ) integer type .
is a ( non-quantized , real ) floating point type .
Returns the minimum representable value in this data type .
Return intensity limits, i.e.
True if the ` other ` DType will be converted to this DType .
Obtains a mapping between routes and handlers .
is active iff any run has at least one relevant tag .
Return information about the tags in each run.
Given a tag and list of runs , serve a list of metadata for audio .
Builds a JSON-serializable object with information about audio.
accessing the specified audio .
encoded audio data .
Writes __main__ 's docstring to stdout with some help text .
Runs the program with an optional 'main ' function and 'argv ' list .
Create a legacy image summary op for use in a TensorFlow graph .
Create a legacy image summary protobuf .
Apply user per-summary size guidance overrides.
Construct a TensorBoardWSGIApp with standard plugins and multiplexer.
Constructs the TensorBoard application .
run group names .
reloading the given multiplexer .
relating to SQL database .
returns SQLite Connection objects .
is enabled .
Parse a string as time indices .
Process a buffer for human-readable display.
View a slice or the entirety of an ndarray .
Convert an array into base64-enoded PNG image.
Safely merge values from `src_proto_list` into `dst_proto_list`.
adding nodes from from_proto into to_proto .
Write a scalar summary .
Create a scalar summary_pb2.Summary protobuf .
Dumps plugin data to the log directory.
Calculate health pill of a tensor.
Reads the config file from disk or creates a new one .
Writes the frame to disk as a tensor summary .
limiting how often frames are computed .
Adds a frame to the current video output .
Creates a frame and writes it to disk .
get the gradients out at each step .
be used in list.sort ( ) .
extract properties corresponding to 'col_params ' .
extracts a metric from a session group or a session .
Returns the metric_value for a given metric in a session or session group .
extracts an hparam from a session group .
Creates filters for the given col_params .
Creates a filter for the given col_param and extractor .
based on a regular exp .
checkes whether a number belongs to an interval .
Converts a google.protobuf.Value to a native Python object.
Sets the metrics for the group to be the average of its sessions .
Sets the metrics for session_group to those of its `` median session '' .
Sets the metrics for session_group to those of its `` extremum session '' .
A generator for the values of the metric across the sessions in the group.
Returns a list of SessionGroups protobuffers from the summary data.
Adds a new Session protobuffer to the 'groups_by_name ' dictionary .
Builds a session object.
Builds the session metric values.
Sets the metrics of the group based on aggregation_type .
according to _request.col_params .
recordAnalyzeAudio(duration, outputWavFile, midTermBufferSizeSec, modelName, modelType)
Break an audio stream to segments of interest,
required , the output names of the WAV files are based on MP3 tags , otherwise the same names are used .; converts the MP3 files stored in a folder to WAV .
converts the WAV files stored in a folder to WAV using a different sampling freq and number of channels .
returns a numpy array that stores the audio samples of a specified WAV of AIFF file
converts the input signal
computes the self-similarity matrix for a sequence
ARGUMENTS:
converts segment endpoints and respective segment
computes the precision , recall and f1 measures ,
reads a segmentation ground truth file , following a simple CSV format with the following columns :
produced either by the fix-sized supervised method or the HMM method .
computes the statistics used to train an HMM joint segmentation-classification model
trains a HMM model for segmentation-classification using a single annotated audio file
trains a HMM model for segmentation-classification using
performs mid-term classification of an audio stream .
Event Detection (silence removal)
prints the cluster purity and speaker purity for
detects instances of the most representative part of a
generates a 256 jet colormap of HTML-like
Distance between two strings
Generates a list of colors based on a list of names ( strings ) .
Generates a d3js chordial diagram that illustrates similarites
generates a chordial visualization for the recordings of the provided path .
crossing rate of frame
Computes entropy of energy
given abs ( FFT ) )
Computes the spectral entropy
Computes the spectral flux feature of the current frame
Computes spectral roll-off
Computes harmonic ratio and pitch
Computes the triangular filterbank for MFCC computation
Computes the MFCCs of a frame , given the fft mag
initializes the chroma matrices used in the calculation of the chroma features
Short-term FFT mag for spectogram estimation:
extracts an estimate of the beat rate for a musical signal .
is extracted .; implements the shor-term windowing process .
Mid-term feature extraction
extracts the mid-term features of the WAVE files of a particular folder .
Same as dirWavFeatureExtraction, but instead of a single dir it
extracts the mid-term features of the WAVE
is used as a wrapper to :




order_book_id
[float] [0, 1]


[str] Industry 
[str] MainBoard - ,GEM - 
[str] Active - , Delisted - , TemporarySuspended - ,
[str] Normal - , ST - ST, StarST - *ST,
  null  
[datetime]  datetime(2999, 12, 31)
[str] CashSettlementRequired - , PhysicalSettlementRequired - 


WARNING:  bar_status lazy_compute

e.g.PSbarupdate_universeoverwrite['000001.XSHE', '000024.XSHE']update_universe(['000030.XSHE'])000030.XSHE000030.XSHE
handle_barbar


init API



/market order
/market order
//100AA1100API
/11001<=100%0.550%.API
/
/

Sync Data Bundle
run a strategy
Generate example strategies to target folder
Generate default config file
Mod management command
sys_analyser ] draw result DataFrame
sys_analyser ] Generate report from backtest output file

`<Clear SQL cursor>`_
`<Execute SQL>`_
`<Translate SQL into Elasticsearch queries>`_
Log a successful API call .
Log an unsuccessful API call.
raise it .
transform hosts argument to
True if the cluster is up , False otherwise .
based on an index , type ( optional ) and ids .
Update a document based on a script or partial data provided .
Execute a search query and get back search hits that match the query .
Reindex all documents from one index to another.
Change the value of `` requests_per_second `` of a running `` reindex `` task .
api returns the indices and shards that a search
accepts a query template and a map of key/value pairs to
created by specifying the scroll parameter .
allows to get multiple termvectors based on an
Create a script in given language with specified ID .
`<http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html>`_
allows to use the mustache language to
allows to retrieve the capabilities of fields among multiple indices .
`<http://www.elastic.co/guide/en/monitoring/current/appendix-api-bulk.html>`_
Create a new : class : ` ~elasticsearch.Connection ` instance and add it to the pool .
Retreive a : class : ` ~elasticsearch.Connection ` instance from the
Obtain a list of nodes from the cluster and create a new connection
failed ) in the connection pool .
Perform the actual request .; Retrieve a connection from the connection
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-change-password.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-role-cache.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html>`_
`<TODO>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delete-user.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role-mapping.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-token.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-has-privileges.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-api-key.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-token.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-user.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-delete-lifecycle.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-explain-lifecycle.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-lifecycle.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-move-to-step.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-put-lifecycle.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-remove-policy.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-retry-policy.html>`_
`<https://www.elastic.co/guide/en/x-pack/current/license-management.html>`_
generate a document per commit
Parse a git repository with all it 's commits and load it into elasticsearch
`<>`_
`<http://www.elastic.co/guide/en/migration/current/migration-api-deprecation.html>`_
info API allows to retrieve one or more ( or all ) of
stats API allows to retrieve one or more ( or all ) of
allowing to get the current hot threads on each node in the cluster .
allows to retrieve information on the usage
serialize them into strings in
Send a bulk request to elasticsearch and process the output .
Streaming bulk consumes actions from the iterable passed in and yields
Parallel version of the bulk helper run in multiple threads at once.
Simple abstraction on top of the
satisfy a given query
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform-stats.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/preview-data-frame-transform.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/put-data-frame-transform.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-data-frame-transform.html>`_
provides a snapshot of how shards have located around the
provides quick access to the document count of the entire cluster ,
command provides a cross-section of each index .
is a view of shard replication .
is the detailed view of what nodes contain which shards .
is the detailed view of Lucene segments per index .
Get information about thread pools .
loaded fielddata on a per-node basis .
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-templates.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-ack-watch.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-activate-watch.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-delete-watch.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html>`_
Get a very simple status on the health of the cluster .
Get a comprehensive state information of the whole cluster .
allows to retrieve statistics from a cluster wide
execute a cluster reroute allocation command including specific commands .
Update cluster wide specific settings.
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-allocation-explain.html>`_
Return a connection from the pool using the ` ConnectionSelector `
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-follow.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-info.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-follow-stats.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-get-auto-follow-pattern.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-put-auto-follow-pattern.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-post-resume-follow.html>`_
`<https://www.elastic.co/guide/en/elasticsearch/plugins/current/ingest.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-forecast.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-job.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-snapshot.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-file-structure.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-flush-job.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-bucket.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-category.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed-stats.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-datafeed.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job-stats.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-job.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-post-data.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-preview-datafeed.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-start-datafeed.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-datafeed.html>`_
`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-snapshot.html>`_
Perform the analysis process on a text and return the tokens breakdown of the text .
refresh one or more index , making all operations performed
Explicitly flush one or more indices.
allows to retrieve information about one or more indexes .
Check if a type/types exists in an index/indices .
mapping definition of index or index/type .
mapping definition of a specific field .
Create an alias for a specific index/indices.
Return a boolean indicating whether given alias exists .
Retrieve a specified alias .
specified aliases .
Delete specific alias.
Retrieve an index template by its name.
Retrieve settings for one or more (or all) indices.
happening on an index .
cached associated with one ore more indices .
Upgrade one or more indices to the latest format through an API .
is upgraded .
Perform a normal flush , then add a generated unique marker ( sync_id ) to all shards .
Provides store information for shard copies of indices.
allows to force merging of one or more indices
allows you to shrink an existing index into a new
rolls an alias over to a new index when the
Retrieve information about a snapshot.
Removes a shared file system repository .
Return information about registered repositories.
shared file system repository .
Restore a snapshot .
running snapshots .
is a list
Create a URL string from parts , omit all ` None ` values and empty strings .
pops all accepted parameters from method 's kwargs and puts
Retrieve information for a particular task.
print results of a search query .
incoming requests .
Expect observation with known target
Probe target of x
Most promissing point to probe next
's something in the queue at the very beginning .
Mazimize your function
Append a point and its target value to the known data .
Evaulates a single point x , to obtain the value y and then records them
random points within the bounds of the space .
found and corresponding parametes .
Get all target values found and corresponding parametes .
allows changing the lower and upper searching bounds
Synthetic binary classification dataset.
SVC cross validation.
Random Forest cross validation.
Apply Bayesian Optimization to SVC parameters.
Apply Bayesian Optimization to Random Forest parameters.
find the maximum of the acquisition function
Load previous ...
Creates a random number generator based on an optional seed .
Expand abbreviations in a template name.
contains a ` cookiecutter.json ` file .
Locate the repository directory from a template reference .
child directory of ` repo_dir ` is the project template .
Check whether the given ` path ` should only be copied and not rendered .
Modify the given context in place based on the overwrite_context .
Generate the context for a Cookiecutter project template .
handle infile correctly .
create the directory , return its path .
Run hook from repo directory, clean project directory if hook fails.
Render the templates and saves them to files .
update a dict with the key/value pair of another .
Retrieve the config from the specified path , returning a config dict .
Return the user config as a dict .
rm -rf ` .
Ensure that a directory exists.
Context manager version of os.chdir.
Make `script_path` executable.
's okay to delete the previously-downloaded file/directory .
unpack a zipfile at a given URI .
using it from the command line .
Prompt the user to reply with 'yes ' or 'no ' ( or equivalent values ) .
Prompt the user to choose from several options for the given variable .
Prompt the user to provide a dictionary of data .
prompting taken from the cookiecutter.json file , this renders
Prompt the user which option to choose from the given .
Prompts the user to enter new config , using context as a source for the
be passed on to the Jinja2 env .
logging for cookiecutter .
be treated as a URL to a git or hg repo .
Clone a repo to the current directory.
is valid .
Return a dict of all hook scripts provided .
Execute a script from a working directory .
Execute a script after rendering it with Jinja .
Try to find and execute a hook from the specified project directory .
Return the Cookiecutter version , location and Python powering it .
Validate extra context.
Create a project from a Cookiecutter project template ( TEMPLATE ) .
Mount the UnmountedType instance
Creates a customized copy of the Parameter .
given python function
Creates a customized copy of the Signature .
Do n't use directly .
Get a BoundArguments object , that partially maps the
Check if the given objecttype has Node as an interface
is non-empty ,
Execute a on_resolve function once the thenable is resolved ,
Get type mounted
given attributes ( dict )
return the attribute/class designated by the
used to iterate over all possible bucket combinations of
construct the suggestion input and weight by taking all
Get all the fields defined for our class , if we have an Index , try
Return a clone of the current search request .
used for the response .
Apply options from a serialized body to the current instance .
Define update action to take:
Serialize the search into the dictionary that will be sent over as the
Execute the search and return an instance of `` Response `` wrapping all
including multi fields .
Create a copy of the instance with another name or connection alias .
Associate a : class : ` ~elasticsearch_dsl.Document ` subclass with an index .
add an analyzer to an index .
Return a : class : ` ~elasticsearch_dsl.Search ` object searching over the
Return a : class : ` ~elasticsearch_dsl.UpdateByQuery ` object searching over the index
Creates the index in elasticsearch .
creating the index if it
Perform the analysis process on a text and return the tokens breakdown
Preforms a refresh operation on the index.
Preforms a flush operation on the index.
allows to retrieve information about the index .
Opens the index in elasticsearch .
Closes the index in elasticsearch .
Deletes the index in elasticsearch .
exists in elasticsearch .
Check if a type/types exists in the index .
mapping definition for a specific type .
Create an alias for the index.
Return a boolean indicating whether given alias exists for this index .
Retrieve settings for the index.
Change specific index level settings in real time.
happening on the index .
Provide low level segments information that a Lucene index (shard
Validate a potentially expensive query without executing it .
cached associated with the index .
provides insight into on-going shard
Upgrade the index to the latest format.
Perform a normal flush , then add a generated unique marker ( sync_id ) to
Provides store information for shard copies of the index.
allows to force merging of the index through an
passing in config
Raises `` KeyError `` if connection; Remove connection from the registry .
Construct an instance of ``elasticsearch.Elasticsearch`` and register
Retrieve a connection , construct it if necessary ( only configuration
specifying the mappings and any
creates a new index for the data .
Use the Analyze API of elasticsearch to test the outcome of this analyzer .
Get answers either from inner_hits already present or by searching
Return the aggregation object .
Construct a filter.
Turn the raw bucket data into a list of tuples containing the key ,
Add a filter for a facet .
Returns the base Search object to which the facets are added .
search `` .
representing the facets selected , including potential
Add a `` post_filter `` to the search request narrowing the results based
highlighting for all the fields
sorting information to the request .
Construct the `` Search `` object .
Execute the search and return the response .
be used when executing the search .
called empty it will remove all information .
Set the type to search through .; supply a single value or
Associate the search request with an elasticsearch client.
Add extra keys to the request body.
be calculated on hits .
control how the _source field is returned .
sorting information to the search request .
Update the global highlighting options used for this request .
passed in will be
Add a suggestions request to the search .
Return the number of hits matching the query and filters .
Turn the search into a scan search and return a generator that will
executes the query by delegating to delete_by_query ( )
Adds a new : class : ` ~elasticsearch_dsl.Search ` object to the request : :
Execute the multi search request and return a list of search results .
Create a tar file from all the files under 'base_dir ' .
Start the GSS-API / SSPI Authenticated Diffie-Hellman Key Exchange .
Parse the next packet .
Parse the SSH2_MSG_KEXGSS_HOSTKEY message ( client mode ) .
Parse the SSH2_MSG_KEXGSS_CONTINUE message .
Parse the SSH2_MSG_KEXGSS_COMPLETE message ( client mode ) .
Parse the SSH2_MSG_KEXGSS_INIT message ( server mode ) .
Start the GSS-API / SSPI Authenticated Diffie-Hellman Group Exchange
Parse the SSH2_MSG_KEXGSS_GROUP message ( client mode ) .
Parse the SSH2_MSG_KEXGSS_ERROR message ( client mode ) .
object from an existing `` stat `` object ( an
be used to; Generate a new private RSA key .
is present in the prefetch buffers , at the given
Set the file 's current position .
Retrieve information about this file from the remote system.
Change the size of this file .
Ask the server for a hash of a section of this file .
Pre-fetch the remaining contents of this file in anticipation of future
Read a set of blocks from the file by ( offset , length ) .
's a saved exception , raise & clear it
performs an openness check .
Execute a command on the server .; allows it , the channel
be used to change the width and; Resize the pseudo-terminal .
Updates this channel's remote shell environment.
Set the value of an environment variable.
Return the exit status from the process on the server .
allows it ,
Request for a forward SSH Agent on this channel.
be combined into stdout on this channel .
Close the channel.
be written to this channel without blocking .
Returns the number of bytes sent , or 0 if
Send data to the channel on the "stderr" stream.
allowing partial results .
Send data to the channel's "stderr" stream, without allowing partial
be used for polling , but
is 0 ,
are already holding the lock . )
Convert an errno value (as from an ``OSError`` or ``IOError``) into a
flags to Python 's os.open ( ) flags
is ready to be read ( or the
is assumed to be called
is buffered and ready to be read from this
is a string representing; Read data from the pipe .
return all data that was in it .
calls to ` read ` after the buffer
Read an OpenSSH config from the given file object .
Return a dict ( ` SSHConfigDict ` ) of config options for a given hostname .
Return the set of literal hostnames defined in the SSH config ( both
Return a dict of config options with expanded substitutions
Return a list of host_names from host value .
given key 's value as a boolean type .
Authenticate the given user to the server if he is a valid krb5
be provided to the client on
Add a prompt to this query .; be a ( reasonably short )
Provide SSH2 GSS-API / SSPI authentication.
returns a single OID , because we only support the
Check if the given OID is the Kerberos V5 OID ( server mode ) .
Create the SSH2 MIC filed for gssapi-with-mic .
Initialize a GSS-API context .
Accept a GSS-API context (server mode).
Initialize a SSPI context .
Create the MIC token for a SSH2 message .
Accept a SSPI context (server mode).
Verify the MIC token for a SSH2 message .
are delegated ( server mode ) .
Create an SFTP client channel from an open `.Transport`.
Generator version of `.listdir_attr`.
Rename a file or folder from `` oldpath `` to `` newpath `` .
Rename a file or folder from `` oldpath `` to `` newpath `` , following
Create a folder ( directory ) named `` path `` with numeric mode `` mode `` .
Create a symbolic link to the `` source `` path at `` destination `` .
Change the owner ( `` uid `` ) and group ( `` gid `` ) of a file .
modified times of the file specified by `` path `` .
Change the size of the file specified by `` path `` .
Copy the contents of an open file object ( `` fl `` ) to the SFTP server as
Copy a local file ( `` localpath `` ) to the SFTP server as `` remotepath `` .
Copy a remote file ( `` remotepath `` ) from the SFTP server and write to
Copy a remote file ( `` remotepath `` ) from the SFTP server to the local
does nothing .
adjusted path if we 're emulating a `` current working
Execute all tests (normal and slow) with coverage enabled.
watch for changes , re-running .
add baked-in docs folder .
Normalize/canonicalize `` self.gss_host `` depending on various factors .
is the first step after; Negotiate a new SSH2 session as a client .
(optional)
are tied to it .
Request a new channel to the server , of type `` `` session '' `` .
Request a new channel to the server .
Ask the server to forward TCP connections from a listening port on
Ask the server to cancel a previous port-forwarding request .
is sometimes used; Send a junk packet across the encrypted link .
Return the next channel opened by the client over this transport , in
verify the server 's host key
happened during the last server request .
Set the handler class for a subsystem in server mode.
using a password .
is used to; using a private key .
is used to answer
Autenticate to the server interactively but dumber.
using GSS-API / SSPI .
Set the channel for this transport's logging.
are holding the lock
is ' A ' - ' F ' for the various keys used by ssh
Checks message type against current auth state.
negotiated encryption parameters for
do nothing if write; Write out any data in the write buffer .
Read up to `` len ( buff ) `` bytes into `` bytearray `` * buff * and return the
Read all remaining lines using ` readline ` and return them as a list .
is on ( `` bufsize `` was
existing entry for a; Add a host key entry to the table .
Read a file of known SSH host keys , in the format used by OpenSSH .
is found ,; given hostname or IP .
given SubDict `` entry `` .
Parses the given line of text to find the names for the host ,
is done through a shared
Return the bytes ( as a ` str ` ) of this message that have n't already been
Returns the ` str ` bytes of this message that have been parsed and
Return the next `` n `` bytes of the message ( as a ` str ` ) , without
Add a boolean value to the stream .
Add an integer to the stream.
Add a 64-bit int to the stream .
returns a random # from 0 to N-1
passed from any file operations that fail .
be used to; Generate a new private ECDSA key .
Create a key object by reading a private key file .
Create a key object by reading a private key from a file ( or file-like )
be read by
Perform message type-checking & optional certificate loading.
loaded from an OpenSSH
Create a public blob from a `` -cert.pub `` -style file on disk .
Create a public blob from a `` -cert.pub `` -style string .
Create a public blob from a network ` .Message ` .
are `` or '' d together to
Return a pair of socket object and string address .
terminate the agent
Terminate the agent , clean the files , close connections
Switch outbound data cipher.
Switch inbound data cipher.
Turn on/off the callback keepalive.
Tells `Packetizer` that the handshake process started.
has timed out .
has completed .
closes a file , this method is called on the handle .
Used by the SFTP server code to retrieve a cached directory
Given a password , passphrase , or other human-source key , scramble it
send paramiko logs to a logfile ,
received from the SSH client to the standard
Read from the standard output of the forked program .
Call FormatMessage with a system error number to retrieve
Given a token , get the token information for it .
Return a TOKEN_USER for the owner of this process .
be used to; Generate a new private DSS key .
Load host keys from a system (read-only) file.
read with this
use when connecting to servers without a known host key .
try for connecting .
Connect to an SSH server and authenticate to it.
Close this SSHClient and its underlying `.Transport`.
is opened and; Execute a command on the SSH server .
Start an interactive shell session on the SSH server.
derive a ` .PKey ` from given string path `` filename `` :
Try, in order:
Sets the language of this KernelPushRequest .
Sets the kernel_type of this KernelPushRequest .
Download competition leaderboard  # noqa: E501
VIew competition leaderboard  # noqa: E501
Download competition data file  # noqa: E501
List competition data files  # noqa: E501
List competitions  # noqa: E501
List competition submissions  # noqa: E501
Submit to competition  # noqa: E501
Upload competition submission file  # noqa: E501
Generate competition submission URL  # noqa: E501
Create a new dataset # noqa : E501
Create a new dataset version # noqa : E501
Create a new dataset version by id # noqa : E501
Download dataset file  # noqa: E501
List datasets  # noqa: E501
List dataset files  # noqa: E501
dataset creation status # noqa : E501
token to start uploading a data file # noqa : E501
Show details about a dataset  # noqa: E501
Download the latest output from a kernel  # noqa: E501
Pull the latest code from a kernel # noqa : E501
be used to create a new kernel and update an existing one .; Push a new kernel version .
Get the status of the latest kernel version # noqa : E501
List kernels  # noqa: E501
authenticate the user with the Kaggle API .
is the second effort to get a username
load the values
is the first effort to get a username
defined at self.config
write config data to file.
set a configuration value , meaning
unset a configuration value
defined , return default; Get the download path for a file .
based on a prefix and separator
print_config_value to print all configuration values
make call to list competitions , format the response , and return
a wrapper for competitions_list for the client.
submit a competition !
are same as for; submit a competition using the client .
get the list of Submission for a particular competition
return either json or csv
list files for competition
List files for a competition, if it exists
designated location , or use
competition_download_file to download all competition
a wrapper to competition_download_files, but first will parse input
Download competition leaderboards
based on a competition name
a wrapper for competition_leaderbord_view that will print the
return a list of datasets !
datasets_list for the client .
view metadata for a dataset.
list files for a dataset
a wrapper to dataset_list_files for the client
get the status of a dataset from the API
wrapper for client for dataset_status, with additional
download a single file for a dataset
download all files for a dataset
client wrapper for dataset_download_files and download dataset file,
upload a dataset file
create a version of a dataset
creating a version of a dataset
initialize a folder with a a dataset configuration ( metadata ) file
create a new dataset , meaning the same as creating a version but
creating a new dataset
based on a chunk size
based on a set of search criteria
see this function for arguments .
create a new kernel in a specified folder from template , including
takes same arguments but
read the metadata file and kernel files from a notebook , validate
client wrapper for kernels_push, with same arguments.
pull a kernel , including a metadata file ( if metadata is True )
client wrapper for kernels_pull
retrieve output for a specified kernel
client wrapper for kernels_output, with same arguments.
get the status of a kernel .
client wrapper for kernel_status
is needed based on timestamp .
print a table of items, for a set of fields defined
using a csv.writer
check the API version against
is up to date
upload files in a folder
upload a single file
return the processed
complete an upload to retrieve a path from a url
is valid , meaning it is in the format
is a wrapper to validate the existence of files
ensure that one or more resource files exist in a folder
ensure that the user has not provided duplicate paths in
convert a set of file_data to a metadata file at path
read the buffer , passing named and non named arguments to the
formatting collections .
Builds form parameters.
Deserializes body to file
string to primitive type .
The logger file.
Sets the license_name of this DatasetNewRequest .
Train textCNN model for sentiment analysis.
Get tokens, tokens embedding
prepare the input sentences .
handle oov .
pretrained model .
Construct a decoder for the next sentence prediction task
Construct a decoder for the masked language model task
Construct an embedding block.
Construct pooler.
Generate the representation given the input sequences .
Generate unnormalized prediction for the masked language model task.
Extracts n-grams from an input segment.
Convert a sequence of bpe words into sentence .
Tokenize a string following following the international tokenizer in mteval-v14a.pl .
bleu score of translation against references .
Compute ngram precision.
Calculate brevity penalty.
has the closest length to the translation .
Compute the smoothed precision for all the orders .
preprocessing helper .
Wikipedia dump helper.
Transform a DataStream of coded DataSets to a DataStream of batches .
Create a batch for CBOW training objective with subwords .
Create a batch for SG training objective with subwords .
Create a batch for CBOW training objective .
Create a batch for SG training objective .
Get a sparse COO array of words and subwords for SkipGram .
Get a sparse COO array of words and subwords for CBOW .
Source Vocabulary of the Dataset.
Target Vocabulary of the Dataset.
given the data loader
Returns a cache model using a pre-trained language model .
Return the dataset corresponds to the provided key .
Get prediction results
Calculate the F1 and EM scores of the predicted results .
Data preparation function.
Evaluate the model on validation dataset .
Generate and print out the log message for training.
Generate and print out the log message for inference.
creating NDArray version of data
returning start_index and end_index
Provides word level vocabulary
Performs invalid character removal and whitespace cleanup on text.
is a control character .
Splits punctuation on a piece of text.
is a whitespace character .
Truncates a sequence pair in place to the maximum length .
Construct the argument parser .
provided arguments and act on -- help .
Load a TokenEmbedding .
Calculate the 2-norm of gradients of parameters , and how much they should be scaled down
backward propagation with loss
detect inf and nan
dynamically update loss scale
Return a string representing the statistics of the bucketing sampler .
Training loop for language model .
Evaluate loop for the trained model
Load sentiment dataset.
storing datasets/models/pre-trained word embeddings
Read dataset from tokenized files.
given a dataset .
build data loader .
gpu available return gpu , else cpu
Initialize a logger
Build a standard LSTM cell , with variational dropout ,
Feature extraction through BiLSTM
Do xWy
adopted from Timothy Dozat https : //github.com/tdozat/Parser/blob/master/lib/linalg.py
MST
Fix the relation prediction by heuristic rules
missing Fortran reshape for mx.NDArray
Updates the progress bar .
Get mini-batches of the dataset.
Evaluate the model on the dataset .
Training loop for awd language model .
embedding evaluation function .
Creates an instance of a registered word embedding evaluation function .
embedding functions names .
Predict the similarity of words1 and words2.
given question words .
Evaluate the model on the dataset with cache model .
Returns a pre-defined model by name.
Static BERT BASE model.
Generate the representation given the inputs .
Defines the forward computation for cache cell .
Assign input `x` to an available worker and invoke
object from json string .
SkipGram forward pass.
Evaluate network on the specified dataset
r"""Hybrid forward computation for Long-Short Term Memory Projected network cell
forward backward implementation
Print statistical information via the provided logger
embedding file for extending vocabulary
Read pre-trained embedding file
initialized embeddings when pre-trained embeddings are used , otherwise zero vectors
Randomly initialize embeddings for tag
Map word(s) to its id(s)
Map id(s) to word(s)
id ( s )
Map id(s) to relation(s)
enumerating data set from batches .
Get batch iterator
Extract a set of n-grams from a list of integers .
appending n-grams values .
is used for evaluating accuracy of
get training data
Parse command line arguments.
Create the mapping from label to numeric label
takes a dataset and converts
prepare a dataset
Construct the DataLoader .
Training function that orchestrates the Classification !
Generate the unnormalized score for the given the input sequences .
Encode the input sequence.
given the input sequence .
One step decoding of the translation model.
Generate the prediction given the src_seq and tgt_seq .
Creates an instance of a subword function .
Attaches one or more embeddings to the indexed text tokens.
object to json string .
Inner Implementation of the Pad batchify
Train a deep biaffine dependency parser
Load from disk
Run evaluation on test set
Parse raw sentence into ConllSentence
weight drop to the parameter of a block .
create rnn cell given specs
create rnn layer given specs
Flatten the structure of a nested container to a list.
Reconstruct the flattened list back to ( possibly ) nested structure .
have batch_size * beam_size on the batch axis .
Sample by beam search.
r"""ELMo 2-layer BiLSTM with 1024 hidden units, 128 projection size, 1 highway layer.
tied weights .
r"""Standard 2-layer LSTM language model with tied embedding and output weights.
r"""Big 1-layer LSTMP language model.
Implement forward computation.
Get the object type of the cell by parsing the input
Compute the context with respect to a center word in a sentence .
Construct the model .
multiprocessing to perform transform for dataset .
calculating the softmax
Read from the value matrix given the attention weights .
Get the translation result given the input sentence .
Evaluate parser on a data set
initialized according to a numpy tensor
given name , shape and initiator
Run decoding
Save model
processing data .
embedding vectors from a pre-trained token embedding file .
Check that tokens and embedding are in the format for __setitem__ .
embedding source name is valid .
Creates a user-defined token embedding from a pre-trained embedding file .
Serializes the TokenEmbedding to a file specified by file_path .
Create a new TokenEmbedding from a serialized one .
Evaluate the model on a mini-batch .
Registers a dataset with segment specific hyperparameters.
Creates an instance of a registered dataset .
registered parameters .
Compute the vocabulary .
Add evaluation specific parameters to parser.
Generator over all similarity evaluation datasets.
Generator over all analogy evaluation datasets.
occurring the evaluation datasets .
occuring the evaluation datasets .
Evaluate on specified similarity datasets.
Evaluate on specified analogy datasets.
Log a similarity evaluation result dictionary as TSV to logfile .
Get model for pre-training.
create dataset for pretraining.
Return a dummy data loader which returns a fixed data batch of target shape
Save the model parameter , marked by step_num .
Log training progress.
split and load arrays to a list of contexts
forward computation for evaluation
dataset the dataset into a npz
Load translation dataset
Create data loaders for training/validation/test.
representing the process  s activity .
Compute embedding of words in batch.
Create an instance of the class and load weights.
Config the logging.
get training data .
Log to a file.
Updates the internal evaluation result .
Predict the relation of two sentences.
given embedded words .
Forward of Decomposable Attention layer
Counts tokens in the specified string .
Slice a flat sequence of tokens into sequences tokens, with each
Calculate the padding length needed for sliced samples in order not to discard data .
Split the dataset into training and validation sets.
accompanying vocabulary object for pre-trained model .
Extract archive file
Discards tokens with frequency below min_frequency and represents them
Read tokens from the provided parse tree in the SNLI dataset .
one iteration of k-means
Index every sentence into a cluster
Build a pair of GNMT encoder/decoder
Initialize the state from the encoder outputs .
is only used for training .
inputs for MLM and NSP .
numpy files from ` TrainingInstance ` s .
Create IndexedRecordIO files from `TrainingInstance`s.
Create `TrainingInstance`s from raw text.
Creates `TrainingInstance`s for a single document.
Creates the predictions for the masked LM objective .
Truncates a pair of sequences to a maximum sequence length .
convert the original vocabulary to nlp.vocab.BERTVocab .
read tensorflow checkpoint
profile the program between [start_step, end_step).
Loads a vocabulary file into a dictionary .
Init the sinusoid position encoding table
Build a pair of Parallel Transformer encoder/decoder
based on the name .
Position-wise encoding of the inputs.
Transformer Encoder Attention Cell.
Transformer Decoder Attention Cell.
Perform forward and backward computation for a batch of src seq and dst seq
validate/save every epoch .
Entry point: train or test.
sampled candidates .
Add a handler sending log messages to a sink adequately configured .
Remove a previously added handler and stop sending logs to its sink .
Return a decorator to automatically log possibly caught error in wrapped function .
Parametrize a logging call to slightly change generated log message .
attributes to the `` extra `` dict of each logged message record .
retrieve a logging level .
Configure the core logger .
extract each entry as a |dict| .
r"""Log ``_message.format(*args, **kwargs)`` with severity ``_level``.
Deprecated function to |add| a new handler .
Deprecated function to |remove| an existing handler .
Converts a django rest frameworks field to a graphql field
given setting is a string import notation ,
Create a filterset for the given model using the provided meta data
Convert the filter value to a primary key before filtering
Inspect a FilterSet and produce the arguments to pass to
Map per-topic results to per-topic futures in futmap.
Map per-resource results to per-resource futures in futmap.
Create futures and a futuremap for the keys in futmap_keys,
Create new topics in cluster.
Delete topics.
given topics .
Get configuration for the specified resources.
Parse a schema given a schema string
sends message to Kafka by encoding with specified or default avro schema .
is an overriden method from confluent_kafka.Consumer class .
Given a parsed avro schema , encode a record for the given topic .
Encode a record with a given schema id .
has been encoded for use with
called ( from flush ( ) ) on successful or failed delivery of the message .
Alter configs atomically, replacing non-specified
requires all configuration to be passed ,
list topics and cluster metadata
embedded plugins from the wheel 's library directory .
served from producer.poll .
Produce User records
Consume User records
POST /subjects/(string: subject)/versions
DELETE /subjects/(string: subject)
GET /schemas/ids/{int: id}
POST /subjects/(string: subject)
PUT /config/ ( string : subject )
GET /config
Download artifact from S3 and store in dirpath directory.
Collect single S3 artifact
download build-artifacts from S3 based on git reference
Collect artifacts from a local directory possibly previously
Return a value for 'name ' from command line args then config file options .
replace , or update the A and PTR ( reverse ) records for a host .
Delete the forward and reverse records for a host.
replace , or update a DNS record .
Set all Ansible modules callables
Display help on Ansible standard module.
Run Ansible Playbooks
installed Ansible modules
Introspect Ansible module.
Return module map references.
Call an Ansible module by invoking it .
Clean out a template temp file
does NOT do any diffing , it just checks the old and new files
breaking at line boundaries and
Convert the group id to the group name on this system
Convert the group to the gid on this system
Return the id of the group that owns a given file
owns a given file
Convert user name to a uid
Return the id of the user that owns a given file
Return the mode of a file
Set the mode of a file
Chown a file , pass the file the desired user and group without following
Chown a file , pass the file the desired user and group
Change the group of a file
versionadded : : 2018.3.0
Return the checksum for the given file .
Get the hash sum of a file
versionadded : : 2016.11.0
Check if a file matches the given hash string
find ( 1 ) `` command and return a list of paths that
Escape single quotes and forward slashes
deprecated : : 0.17.0
Does the actual work for file.psed , so that single lines can be passed in
Return an integer appropriate for use as a flag for the re module from a
flags `` and `` new_flags ``
Create a temp file and move/copy the contents of `` path `` to the temp file .
True if src and probe at least matches at the beginning till some point .
Expand regular expression to static match.
Raise an exception , if there are different amount of specified occurrences in src .
Indent the line with the source line.
Add line ending
versionadded : : 2015.8.0
versionadded : : 0.17.0
versionadded : : 2014.1.0
versionadded : : 0.10.4
versionadded : : 0.9.5
versionadded : : 2014.7.0
versionadded : : Neon
Create a symbolic link ( symlink , soft link ) to a file
Rename a file or directory
Copy a file or directory from source to dst
versionadded : : 2017.7.0
Return a dict containing the stats for a given file
Remove the named file .; is supplied , it will be recursively
see if path after expansion is a valid path ( file or directory ) .
Reset the SELinux context on a given path
Get an SELinux context from a given path
versionchanged : : Neon
Check the source list and return the source to use
Return the contents after applying the templating engine
Return the managed file data for file.managed
versionchanged : : 2016.3.5
see what changes need to be made for a file
Return a dictionary of what changes need to be made for a file
Check for the changes in the file metadata.
unified diff of two files
was retrieved with get_managed and
containing this path is available .
Taken and modified from os.makedirs to set user , group and mode for each
Get major/minor info from a device
Check if a file exists and is a character device .
Check if a file exists and is a block device .
Check if a file exists and is a FIFO .
Lists the previous versions of a directory backed up using Salt 's : ref : ` file
Grep for a string in the specified file
Return a list of all physical open files on the system .
Move a file or directory
Recursively calculate disk usage of path and return it
Set a key/value pair in sqlite3
Get a value from sqlite3
Delete a key/value pair from sqlite3
learned via a specific protocol .
versionchanged : : 2018.3.0
is absent .
including all connected containers
need to create the zmq router device
The server equivalent of ReqChannel.crypted_transfer_decode_dictentry
see if a fresh AES key is available and update the components
Authenticate the client , use the sent public key to encrypt the AES key
Decorator wrapper for salt.utils.path.which
Decorator wrapper for salt.utils.path.which_bin
Return all inline documentation for runner modules
Return all inline documentation for wheel modules
return the aggregate
Create a temporary file
Format the low data for RunnerClient()'s master_call() function
Execute a runner function asynchronously ; eauth is respected
Execute a runner function synchronously ; eauth is respected
Print out the documentation!
Execute the runner sequence
execute specific runner
Activate this register to turn on a minion status tracking register , this
Query |reclass| for the top data (states of the minions).
is idempotent .; Create a new , empty pipeline .
is idempotent .
Retrieve metadata about one or more pipelines.
Get a list of pipeline ids and names for all pipelines .
Get the pipeline id , if it exists , for the given name .
specified pipeline .
Get a boto connection to Data Pipeline .
Get a boto3 session
Create a blank virtual machine image file of the specified size in
existing disk image to another format using qemu-img
Performs a ping to a host
Return information on open ports and states
Performs a traceroute to a 3rd party host
Query DNS for information about a domain or ip address
routing information for given destination ip
Performs a DNS lookup with dig
Return a list of all the interfaces names
assigned to the host .
using a particular
Send a message to a Pushover user or group .
Send an PushOver message with the data
describing the target of an EntryPoint object .
Load execution modules
loaded raw and bypassing the __virtual__ function
Return the master services plugins
Returns the proxy module for this salt-proxy-minion
Returns the returner modules
Returns the utility modules
Returns the pillars modules
Returns the tops modules
Returns the wheels modules
Returns the outputters modules
Returns the auth modules
Returns the file server modules
Returns the roster modules
Load the thorium runtime modules
Returns the state modules
Load the beacon modules
Returns the custom logging handler modules
Returns the render modules
Returns the grain functions
Returns the grains cached in cfn , or None if the cache is too old or is
Return the functions for the dynamic grains and the values for the static
call a function inside a loader directory
Make a very small database call
Return modules for SPM's package database
Return the cloud functions
Returns the executor modules
Inject a variable into a module .; is used to inject `` globals '' like
used by the LazyLoader to inject globals into a function at
Return the error string for a missing function .
refresh the mapping of the FS on disk
Clear the dict
Strip out of the opts any logger instance
Iterate over all file_mapping files in order of closeness to mod_name
Load a single item if you have it
Load all of them
Apply the __outputter__ variable to the functions
Given a loaded module and its default name determine its virtual name
Get an individual DMI string from SMBIOS info
Return DMI records from SMBIOS
Structurize DMI records into a nice list
Parse the raw DMIdecode output of a single handle
trying to fish out at least ints & lists from strings
Build the mapping for YAML
pass them in correctly is they are declared
Convert a decimal octet into an integer .
Verify that the netmask is valid .
is allocated for private networks .
Expand a shortened IPv6 address .
is otherwise IETF reserved .
Generate Iterator over usable hosts in a network.
Convert snetinfo object to list
Validate the beacon configuration
Emit the network statistics of this host.
Return apcaccess output
Return load
Return battery charge
Ensures a host's core dump configuration.
be obtained from; Ensures the given password is set on the ESXi host .
setting NTP servers , ensuring the
Configures a host 's VMotion properties such as enabling VMotion and setting
Configures a host 's VSAN properties such as enabling or disabling VSAN , or
Manage the SSH configuration for a host including whether or not SSH is running or
Ensures the specified syslog configuration parameters .
Configures the disk groups to use for vsan .
Configures the host cache used for swapping .
is set to the specified value .
are set on the device .
are provided as an individual string or list format .; are configured .
is configured on the system .
are set to the specified values .; be ignored .
is configured on the device .; being unable to
gets called when the proxy starts up .
return the first one that
Return the targets from the flat yaml file , checks opts for location but
Get a decrypted secret from the tISMd API
Execute queries against POSTGRES, merge and return as a dict
Yield a POSTGRES cursor
normalizes the config block into a set of queries we
Returns the cmdline of a Process instance .
Returns the create_time of a Process instance .
Returns the name of a Process instance .
Returns the status of a Process instance .
Returns the username of a Process instance .
Return a list of top CPU consuming processes during the interval .
Return a dictionary of information for a process id ( PID ) .
Kill a process by PID .
processes matching a pattern .
Return the pids for processes matching a pattern .
Return the percent of time the CPU is busy .
Return the percent of time the CPU spends in each state ,
Return a list of disk partitions and their device , mount point , and
Return a list of disk partitions plus the mount point , filesystem and usage
Return the total number of bytes of physical memory .
Return the boot time in number of seconds since the epoch began .
Return network I/O statistics.
Return disk I/O statistics.
Return logged-in users.
Retrieve the lsof information of the given process name .
Retrieve the netstat information of the given process name .
Retrieve the ss information of the given process name .
corresponding to a `` ps aux '' filtered
Ensure pagerduty service exists.
does not exist .
compare salt state info with the PagerDuty API json structure ,
return a dictionary or a list
Get the current DSC Configuration
Remove the current DSC Configuration .
Reapplies the previous configuration.
Get the status of the current DSC Configuration
For detailed descriptions of the parameters see:
Gather lm-sensors data from a given chip
string into XXhYYmZZs format
given retention policy is present .
Return a dictionary of only the subset of keys/values specified in keys
Fire an event off up to the master server
be formed as a dict .
Send an event to the Salt Master
named cluster is present with the specified properties .
named cluster is absent
Send the results to Splunk .
Print the output data in JSON
Try to get all sort of parameters for Server Density server info .
is monitored with Server Density .
Return an sqlite connection
list contents of a queue
Return a list of sqlite databases in the queue_dir
List contents of a queue
are escaped properly in sqlite3 fashion .
Delete an item or items from a queue
return them .
Returns a list of the all topics..
see if an SNS topic exists .
Create an SNS topic.
Delete an SNS topic.
Get list of all subscriptions to a specific topic.
Subscribe to a Topic.
Unsubscribe a specific SubscriptionArn of a topic.
given topic name .
set the current ioloop to io_loop for the context
Get all environments from the top file
given directory in the given environment
Update the salt minion from the URL defined in opts [ 'update_url ' ]
versionadded : : 2015.5.1
versionadded : : 2015.5.8,2015.8.3
versionadded : : 0.10.0
versionadded : : 2016.3.6,2016.11.4,2017.7.0
versionadded : : 2019.2.0
versionadded : : 2015.8.11,2016.3.2
versionchanged : : 2015.8.11,2016.3.2
refresh the pillar data .
named function is running return the data associated with it/them .
removes all caches on a minion .
removes job cache folders and files on a minion .
Return the data for a specific cached job id .
Sends a signal to the named salt job 's process
Sends a termination signal ( SIGTERM 15 ) to all currently running jobs
Sends a kill signal ( SIGKILL 9 ) to all currently running jobs
Used to regenerate the minion keys .
sends a request to the master to revoke its own key .
versionchanged : : 2017.7.0
Execute a runner function .; be run on the master ,
be run against a; Execute a wheel module and function .
be used in pillars
Create a session to be used when connecting to Zenoss .
Make a request to the Zenoss API router
found , returns None .
connect to a zenoss server and add a new device entry .
set the prod_state in zenoss .
file path against unwanted directories and decides whether file is marked as deleted .
desired deleted files .
Formats the output of the restartcheck module.
based systems .
changed in an NILinuxRT system using md5sum and timestamp
Besides the normal Linux kernel driver interfaces, NILinuxRT-supported hardware features an
openeded by running processes and seeks for packages which need to be restarted .
Configures the default VSAN policy on a vCenter .
Assigns a default storage policy to a datastore
Return a cloud client
is found on a provider
Return details on an instance.
using Salt Cloud
Execute a salt cloud map file
Execute a single action on the given provider/instance
List block storage volumes
Attach volume to a server
List private networks
Create private network
List virtual interfaces on a server
Attach private interfaces to a server
Return a list of sysctl parameters for this minion
Return a single sysctl parameter for this minion
persist a simple sysctl parameter for this minion
Send a message to a PushOver channel .
True if valid , otherwise False .; Check if address is a valid IP .
Return the AAAA record for `` host `` .
Return a list of IPs of the nameservers for `` domain ``
Return the allowed IPv4 ranges in the SPF record for `` domain `` .
Return a list of lists for the MX of `` domain `` .
Return the TXT record for `` host `` .
Assign a single sysctl parameter for this minion
Recursively workout the file name from an augeas change
are fully qualified and affect only one file .
Returns an instance of the redis client
rewrite the append-only file
save the dataset to disk
redis server configuration values
Set redis server configuration values
Return the number of keys in the selected database
Deletes the keys from redis , returns number of keys deleted
Return true if the key exists in redis
Set a keys time to live in seconds
Set a keys expire at given UNIX time
Remove all keys from all databases
Remove all keys from the selected database
redis key value
Determine if a hash fields exists.
Get all fields and values from a redis hash , returns dict
given number .
Returns number of fields of a hash.
Returns the values of all the given hash fields .
multiple values .
Set the value of a hash field.
Return all the values in a hash.
associated values .
Get information and statistics about the server
glob style patterns
Get redis key type
Get the UNIX time in seconds of the last successful save to disk
Get the length of a list in Redis
Ping the server , returns False on connection errors
Set redis key value
save the dataset to disk and then shut down the server
Make the server a slave of another instance , or promote it as master
Get members in a Redis set
Return the current server UNIX time in seconds
Get the length of a sorted set in Redis
Get a range of values from a sorted set in Redis by index
Get ip for sentinel master
Get host information about slave
Create a boto3 client connection to EFS
Creates a new , empty file system .
Creates a mount target for a file system .
associated with a file system .
Deletes a file system , permanently severing access to its contents .
Deletes the specified mount target .
Deletes the specified tags from a file system .
Get all EFS properties or a specific instance property
Get all the EFS mount point properties for a specific filesystemid or
Return the tags associated with an EFS instance .
Modifies the set of security groups in effect for a mount target
is installed on the minion
is uninstalled from the minion
Send an email with the data
Get a list of IP addresses from a CIDR .
Get a list of IPv6 addresses from a CIDR .
Get the netmask address associated with a CIDR address .
Get the broadcast address associated with a CIDR address .
's running longer than
used for the MySQL connection .
Return a mysql cursor
Return data to a mysql server
Return event to mysql server
Save the load to the specified jid id
Return the load data that marks a specified jid
Return the information returned when the specified job id was executed
Return a dict of the last function called for all minions
Return a list of all job ids
Return a list of minions
Purge records from the returner tables.
rows to a set of backup tables , then purge rows .
Called in the master 's event loop every loop_interval .
Verify options and log warnings
Return data to the local syslog
error(msg : string)
Check if a pid file exists and if it is associated with
Find configuration files on the system.
Open suitable config file.
coalescing settings of network device
sure a datacenter exists .
Generate all the possible paths within an OpenConfig-like object.
Set a value under the dictionary hierarchy identified
Traverse a dict or list using a colon-delimited ( or otherwise delimited ,
Recursive version of the default dict.update
Apply the defaults to a Python dictionary having the structure as described
Render a field found under the `` field `` level of the hierarchy in the
works similarly to
install a single feature , use the `` name ``
Remove the windows feature To remove a single feature , use the `` name ``
Check if an origin match cors allowed origins
Remove all futures that were waiting for request ` request ` since it is done waiting
Get an event ( asynchronous of course ) return a future that will get it later
Timeout a specific future
Callback for events on the event sub socket
Verify that the client is in fact one we have
Initialize the handler before requests are called
used for the request
is auth 'd
get/posts etc .
Serlialize the output based on the Accept header
get the data from the urlencoded forms
Deserialize the data based on request content type headers
Format the incoming data into a lowstate object
Set default CORS headers
Return CORS headers for preflight requests
are done over post , this is a parked endpoint
Authenticate < rest_tornado-auth > ` against Salt 's eauth system
determine salt-api capabilities
lowstates to the appropriate clients
Dispatch local client commands
Return a future which will complete once jid ( passed in ) is no longer
Disbatch local client_async commands
Disbatch runner client commands
Disbatch runner client_async commands
getting lists of minions or getting minion
return the job id
getting lists of previously run jobs or getting
Fire an event in Salt with a custom event tag and data
Called several times each second
Ensure user account is absent
Manage user account
write to file
Check if proxy conf exists and update
Check if proxy for this name is running
execute proxy process
Create the salt proxy file and start the proxy process
Publishes minions as a list of dicts.
Publishes the data to the event stream .
Associate grains data with a minion and publish minion update
returned by Salt for a particular minion .
Creates a new job with properties from the event data
Tag: salt/key
Check if any minions have connected or dropped .
Process events and publish data
based on the target , function and tgt_type
Call ` func ` at regular intervals and Waits until the given function returns
Return a list of service that are enabled on boot
Return all available boot services
Enable the named service to start at boot
named service to start at boot
get connection args
Returns the API version derived from endpoint 's response .
intended to be used within Keystone-enabled modules .
Create EC2-compatible credentials for user per tenant
Delete EC2-compatible credentials
Return ec2_credentials for a user (keystone ec2-credentials-get)
Return a list of ec2_credentials for a specific user ( keystone ec2-credentials-list )
Return a specific endpoint ( keystone endpoint-get )
Return a list of available endpoints ( keystone endpoints-list )
Create an endpoint for an Openstack service
Delete endpoints of an Openstack service
Create a named role .
Delete a role (keystone role-delete)
Return a specific roles ( keystone role-get )
Return a list of available roles ( keystone role-list )
Add service to Keystone service catalog
Delete a service from Keystone service catalog
Return a specific services ( keystone service-get )
Return a list of available services ( keystone services-list )
Create a keystone tenant
Delete a tenant (keystone tenant-delete)
keystone project-delete ) .
Return a specific tenants ( keystone tenant-get )
Return a specific projects ( keystone project-get )
Return a list of available tenants ( keystone tenants-list )
Return a list of available projects ( keystone projects-list ) .
Update a tenant 's information ( keystone tenant-update )
Update a tenant 's information ( keystone project-update )
Return the configured tokens ( keystone token-get )
Return a list of available users ( keystone user-list )
Return a specific users ( keystone user-get )
Create a user ( keystone user-create )
Delete a user (keystone user-delete)
Update a user 's information ( keystone user-update )
Verify a user 's password
Update a user 's password ( keystone user-password-update )
Add role for user in tenant (keystone user-role-add)
Return a list of available user_roles ( keystone user-roles-list )
writing list functions
create roster targets .
match via glob
Return the configured ip
is safe for
Try to reactivate the expired domain name
Try to renew the specified expiring domain name for a specified number of years
Try to register the specified domain name
Checks the availability of domains
Returns information about the requested domain
Returns a list of TLDs as objects
Returns a list of domains for the particular user as a list of objects
given `` minion_id `` .
Assign status data to a dict.
Return invalid status.
Return valid status.
Run a command .
Find all buildout configs in a subdirectory.
Get the current bootstrap.py script content
Check for buildout versions.
Get the most appropriate download URL for the bootstrap script .
Upgrade current bootstrap.py with the last released one.
Run the buildout bootstrap dance ( python bootstrap.py ) .
Run a buildout in a directory .
Run buildout in a directory .
Execute queries against SQLite3, merge and return as a dict
Read pillar data from HTTP response.
Verify user password.
is preserved for backwards compatibilty .
assigned to a username .
Get username line from switch.
Get grains for minion.
commands to the NX-OS device .
Execute one or more show ( non-configuration ) commands .
run ` show version ` on the NX-OS device .
run ` show running-config ` on the NX-OS device .
Configures the Nexus switch with the specified commands .
running config .
password on switch .
Assign role to username.
Remove role from username.
send configuration commands to the device over a
send configuration commands using NX-API .
send exec and config requests over NX-API .
Returns the data compressed at gzip level compression .
reads chunk_size bytes at a time from a file/filehandle and
Relocate an absolute path to a new root directory.
Build a canonical unit name treating unit names without one
boolean telling whether or not the named service is available
modified/updated unit files , and run a daemon-reload if any are
removing masks before making changes to a
Try to figure out the default runlevel .; is kept in
get all the unit files
get all the initscripts
Returns the path to the sysv service manager ( either update-rc.d or
Return the current runlevel
running the command with -- scope from
Build a systemctl command line .
leverages __context__ to keep from running 'systemctl
is assumed disabled if the `` startup '' symlink
is not available , but a unit file exist in
versionadded : : 0.15.0
Return a list of all running services , so far as systemd is concerned
versionadded : : 2015.8.5
Return a list of all available services
versionadded : : 2015.5.0
versionchanged : : 2015.8.12,2016.3.3,2016.11.0
Return the status for a service via systemd .
named service is enabled to start on boot
Manage libvirt keys.
call the virt functions .
Stops a VM by shutting it down nicely .
Stops a VM by power off .
Starts an existing guest , or defines and starts a new VM with specified arguments .
Takes a snapshot of a particular VM or by a UNIX-style wildcard .
Reboots VMs
deprecated : : 2016.3.0
starts a new network with specified arguments .
starts a new pool with specified arguments .
Return `` True `` if service is running
Return `` True `` if directory < svc_path > is really a service :
Return `` True `` if service < name > is autostarted by sv
match `` name ``
Add a path that may contain available services .
Return a list of paths to services with `` name `` that have the specified `` status ``
have the specified service `` status ``
Returns the list of service 's name that are aliased and their alias path ( s )
Show properties of one or more units/jobs or the manager
Start service ``name`` at boot.
Do n't start service `` name `` at boot
Remove the service < name > from system .
Execute a command and read the output as YAML
download the metadata for each file in all buckets
Initialize it if it does not exist .
Return the filename of the cache for bucket contents .
Retrieve the content of all buckets and cache the metadata to the buckets
Return the contents of the buckets cache file
Looks for all the files in the S3 bucket cache metadata
's old or missing go grab the
Turn a pass argument into the correct option
Parse a logadm configuration file .
Parse a logadm options string
Show configuration
Show parsed configuration
arguments map to which flags and options .
Set up pattern for logging.
Remove log pattern from logadm
return a list of string encodings to try
Split a locale specifier .
Join a locale specifier split in the format returned by split_locale .
according to the format returned by ` locale -a ` .
given data structure
Return a mongodb connection object
Return the return information associated with a jid
Return the most recent jobs that have executed the named function
expected path of a Let 's Encrypt live cert
Return the expiry date of a cert
Date before a certificate should be renewed
Obtain/renew a certificate from an ACME CA, probably Let's Encrypt.
Return information about a certificate
Check if a certificate needs renewal
Split the field in drbd-overview
Count the number of spaces before the first character
Figure out the sections in drbdadm status
Analyse the line of local resource of ``drbdadm status``
Analyse the line of volumes of ``drbdadm status``
Analyse the line of peer nodes of ``drbdadm status``
Call action for different lines
Show status of the DRBD devices, support two nodes only.
Using drbdadm to show status of the DRBD devices ,
string the return code of the powershell command
Create a Win32_TerminalServiceSetting WMI Object as $ RDP and execute the
List information about the sessions.
Get information about a session .
Disconnect a session .
Initiate the logoff of a session.
is provided ,; Return the available fileserver environments .
Return a list of files from the salt fileserver
Return a list of symlinked files and dirs
Return a list of directories in the given environment
is provided , then the cache for; Update the fileserver cache .
Run the logic for saltkey
Return the minion keys directory paths
Generate minion RSA public keypair
Generate master public-key-signature
Check the minion cache to make sure that old minion data is cleared
is not running
match the of a key and return the key 's location
return the current state of the
Return a dict of local keys
Return a dict of managed keys and what the key status are
managed keys with local keys
Return a dict of managed keys under a named status
Return the specified public key or keys based on a glob
is passed , it is evaluated as a glob .
Accept all keys in pre
Delete all denied keys
Delete all keys
Reject all keys in pre
Return the fingerprint for a specified key
Return fingerprints for all keys
differ depending on packaging
Get the username and password from opts , grains , or pillar
returns a authentication handler.
does not seem to be a; Extract the version from the war file name .
used to issue the command to tomcat via the manager
need only a path option
list all the deployed webapps
return the status of the webapp ( stopped | running | missing )
return details about the server
Deploy a WAR file
replaces the $ CATALINA_HOME/bin/digest.sh script
Return server version from catalina.sh version
Return all server information from catalina.sh version
catalina to start , stop , securestart , forcestop .
Get the appoptics options from salt .
Return an appoptics connection object.
Parse the return data and return metrics to AppOptics .
versionadded : : Fluorine
is a file ,
Returns the AC/DC values in an dict for a guid and subguid for a the given
Sets the AC/DC values of a setting with the given power for the given scheme
given power scheme
Ensure that a pagerduty schedule exists.
set if we really have an identifier
Check if :
Takes a tab list structure and renders it to a list for applying it to
be used to build a crontab command .
Writes the contents of a file to a user 's crontab
Takes a list of lines to be committed to a user 's crontab and writes it
match their counterparts from
Return the contents of the user 's crontab
Return the contents of the specified user 's crontab
Return the specified entry from user 's crontab .
Set up a special command in the crontab.
be used in a cron entry
specified user .
Remove a special cron job for a specified user .
Remove a cron job for a specified user .
Set up an environment variable in the crontab.
Given a bucket name , check to see if the given bucket exists .
Given a valid config , create an S3 Bucket .
Given a bucket name , delete it , optionally emptying it first .
given S3 bucket .
Given a bucket name describe its properties .
owned by the authenticated sender of the request .
Given a valid config , update the ACL for a bucket .
Given a valid config , update the CORS rules for a bucket .
Given a valid config , update the Lifecycle rules for a bucket .
Given a valid config , update the logging parameters for a bucket .
Given a valid config , update the notification parameters for a bucket .
Given a valid config , update the policy for a bucket .
Given a valid config , update the replication configuration for a bucket .
Given a valid config , update the request payment configuration for a bucket .
Given a valid config , update the tags for a bucket .
Given a valid config , update the versioning configuration for a bucket .
Given a valid config , update the website configuration for a bucket .
Delete the CORS configuration for the given bucket
Delete the lifecycle configuration for the given bucket
Delete the replication config from the given bucket
Delete the tags from the given bucket
Remove the website configuration from the given bucket
Takes a username as a string and returns a boolean .
create or update as needed .
escalation_rules dict to a string for comparison
need to be changed .
need to be changed
Ensure the RabbitMQ user exists.
named user is absent
instructs a running state to pause at a given
Remove a pause from a jid , allowing it to continue
die before executing the given state id ,
Execute a single state orchestration routine
Display the state data from a specific sls , or list of sls files , after
Designed for use in states .; Send a message to an SMTP recipient .
Adds a distinguished name to a distinguished name list .
Add a list of policy distinguished names .
Adds a domain name to a domain name list .
Add a list of policy domain names .
Add an IP address to an IP address list.
Retrieves a list of all IP address lists .
Retrieves a specific policy distinguished name list .
Retrieves a specific policy domain name list .
Retrieves a specific IP address list .
is running as an admin or; Run a command as another user .
works for non-priviledged users
based on the local data store on the minion
find image with given name , returns
given image is present with properties
clustered cassandra nodes
is present with the specified options
Set the approval state of a change request/record
Delete an existing record
Run a non-structed ( not a dict ) query on a servicenow table .
Update the value of a record 's field in a servicenow table
return a tuple of package name , separator , and
Verify that the given package is installed and is at the correct version
Verify that given package is not installed .
Scan for the configured services and fire events
Returns the credentials merged with the config data ( opts + pillar ) .
Create a JIRA issue using the named settings .; Return the JIRA ticket ID .
existing user .; Return `` True `` when the issue has
Return `` True `` when it successfully; Add a comment to an existing ticket .
Check if the issue is closed .
is now returning some complex types that
Run a single module function or a range of module functions in a batch .
Calls a function from the specified module .
deprecated : : 2017.7.0
is on the list
List all InfluxDB databases
exists in Influxdb
Create a database
Remove a database
admins or database users .
database user exists .
Create a cluster admin or a database user .
Change password for a cluster admin or a database user.
Remove a cluster admin or a database user .
Get an existing retention policy .
Check if a retention policy exists .
Add a retention policy .
existing retention policy .
Querying data
provided log handlers .
handling mechanism for py3
Override the default error handling mechanism
Return status for requested information
named sysctl value is set in memory and persisted to the
Return the minion configuration settings
versionadded : : 2016.3.4
wraps the execution of freebsd-update command .
named package is not installed .
Adds a new location to install flatpak packages from .
List all nictags
vms connect to nictag
Check if nictags exists
Add a new nictag
Update a nictag
Delete nictag
Initialize the module .
Query the URI
Set state to the device by ID.
Parse device(s) ID(s) from the common params.
info about all available lamps .
lamp ON/OFF .
lamp is ON , then blink ON-OFF-ON , otherwise OFF-ON-OFF .; Blink a lamp .
Ping the lamps by issuing a short inversion blink to all available devices .
Return the status of the lamps .
Rename a device .
Lamp alert
Set a color to the lamp .
Set an effect to the lamp.
Set the mired color temperature.
Find the requested branch in the specified repo
requested bookmark in the specified repo
Find the requested tag in the specified repo
ref tuple if ref is in the repo .
Return a list of hglib objects for the various hgfs remotes
Place an update.lk
Execute an hg pull on all of the repos
Return a list of refs that can be used as environments
match the path and ref , read the file out of hg
Return a file hash , the hash type is set in the master config file
Return a dict containing the file lists for files and dirs
Get a list of all files on the file server in a specified environment
Get a list of all directories on the master
returns the cache class .
is there , check to see if it needs to
using the specified module
stored in the specified bank .
is None Then return empty dict
Ensure the autoscale group exists.
ensure that termination_policies are set
ensure that scaling_policies are set
ensure scheduled actions are setup
ensure that notification_configs are set
ensure that cloudwatch_alarms are set
named autoscale group is deleted .
kubernetes API connection singleton the old way
kubernetes API connection singleton
Checks connections with the kubernetes API server.
Return the names of the nodes composing the kubernetes cluster
Return the details of the node identified by the specified name
identified by ` label_name ` to ` label_value ` on
Return the names of the available namespaces
Return a list of kubernetes deployments defined in the namespace
Return a list of kubernetes services defined in the namespace
Return a list of kubernetes pods defined in the namespace
Return a list of kubernetes configmaps defined in the namespace
Return the kubernetes deployment defined by name and namespace
given namespace defined by the specified name
Return the kubernetes secret defined by name and namespace .
Deletes the kubernetes deployment defined by name and namespace
Deletes the kubernetes service defined by name and namespace
Deletes the kubernetes pod defined by name and namespace
Creates the kubernetes deployment as defined by the user .
Creates the kubernetes secret as defined by the user .
Creates the kubernetes configmap as defined by the user .
Creates a namespace with the specified name .
existing deployment with a new one defined by name and
existing service with a new one defined by name and namespace ,
Create a Kubernetes Object body instance .
Read a yaml file and , if needed , renders that using the specifieds
Converts a dictionary into kubernetes ObjectMetaV1 instance.
Converts a dictionary into kubernetes AppsV1beta1DeploymentSpec instance.
Converts a dictionary into kubernetes V1PodSpec instance.
Converts a dictionary into kubernetes V1ServiceSpec instance.
has string keys and values .
Accept the provided key
Prepare the master to expect a signing request
patching the `` collections.abc `` and `` inspect ``
Parses the auth/authconfig line
Parse the iscsiname line
Parse the partition line
Parse the raid line
Parse the services line
Parse the updates line
Parse the volgroup line
Convert a kickstart file to an SLS file
validates the minutes parameter .; be any number
Sets the same value; Sets the amount of idle time until the machine sleeps .
Display the amount of idle time until the computer sleeps.
Set the amount of idle time until the computer sleeps.
Display the amount of idle time until the display sleeps.
Set the amount of idle time until the display sleeps.
Display the amount of idle time until the hard disk sleeps.
Set the amount of idle time until the harddisk sleeps.
is on or off if supported
wake from sleep when modem activity is
wake from sleep when network activity
restart after a power
restarts automatically after a system freeze .
sleep computer ' is on or off if
sleep the computer .
Return a conn object for the passed VM data
Initalize Docker on Minion as a Swarm Manager
Join a Swarm Worker to the cluster
leave the swarm
Create Docker Swarm Service Create
Swarm Service Information
Remove Swarm Service
Displays Information about Swarm Nodes with passing in the server
Remove a node from a swarm and the target needs to be a swarm manager
docker swarm nodes/needs to target a manager node/minion
Ensure the IAM role exists.
do n't happen to be order-sensitive , but
is deleted .
see if the host is responding .
Get a data entry in a datastore
Helper function to the grains from the proxied devices.
get all available update packages .
is ignored the '- ' and
have been ignored .; Ignored updates are shown
Check the status of automatic update scheduling .
Enable/disable automatic update scheduling.
containing the name
Install a named update .
Return a list of all updates that have been downloaded locally .
named update so that it can be installed later with the
be installed later with the
versionadded : : 2016.3.0
be just the id and key; Return the credentials for AWS signing .
using Signature Version 2 Signing
using Signature Version 4 Signing
Get a signature key .
Perform a query against AWS services using Signature Version 2 Signing
Try to get region from instance identity document and cache it
Return the region to use , in this order :
generates a list of master ( host , port ) addresses .
return a tuple of ( host , port )
objects that match the expression in the `` regex ``
match the expression in the `` regex ``
Return a list of parent `` ciscoconfparse.IOSCfgLine `` objects , which matched
Return a list of parent `` ciscoconfparse.IOSCfgLine `` lines as text , which
Return a list of detailed matches , for the configuration blocks ( parent-child
allows you to modify system tuned parameters
Removes RPM/SLA probes from the network device.
make it if its closed )
Verify that you have the views you need .; be disabled by
Return a job id and prepare the job id directory
Return data to couchbase bucket
Save the load to the specified jid
Save/update the minion list for a given jid .
Return a properly formatted jid dict
Return the Redis server connection details from the __opts__ .
Return the Redis server instance .
Build the key opts based on the user options .
Return the Redis key for the bank given the name .
Return the Redis key given the bank name and the key name .
Return the Redis key for the SET of keys under a certain bank , given the bank name .
Build the bank hierarchy from the root of the tree .
algorithm that builds the list of banks to remove ,
Store the data in a Redis key.
Fetch data from the Redis cache.
is specified , remove; Remove the key from the cache bank with all the key content .
contains the specified key .
given name or id exists .
Returns the resource id if created , or False; Create a VPC resource .
True if successful , otherwise False .
Get a VPC resource based on resource type and name or id .
based on resource type and name , id , or tags .
Get an AWS id for a VPC resource by type and name .
Given a resource type and name , return { exists : true } if it exists ,
Given VPC properties , return the VPC id if a match is found .
Given a VPC ID , check to see if the given VPC ID exists .
Given a valid CIDR block , create a VPC .
Given a VPC ID or VPC name , delete the VPC .
Given a VPC ID describe its properties .
matching the filter criteria if provided .
Given subnet properties , find and return matching subnet ids
Given a valid VPC ID or Name and a CIDR block , create a subnet for the VPC .
Given a subnet ID or name , delete the subnet .
Check if a subnet exists .
Given a subnet ( aka : a vpc zone identifier ) or list of subnets , returns
Given a subnet id or name , describe its properties .
Given a VPC ID or subnet CIDR , returns a list of associated subnets and
attaching it to an existing VPC .
Delete an internet gateway (by name or id).
Given gateway properties , find and return matching nat gateways
Checks if a nat gateway exists.
Return a description of nat gateways matching the selection criteria
Create a NAT Gateway within an existing subnet .
Delete a nat gateway (by id).
Given a valid VPN connection type , a static IP address and a customer
Given a customer gateway ID or name , delete the customer gateway .
Given a customer gateway ID , check if the customer gateway ID exists .
Given valid DHCP options , create a DHCP options record , optionally associating it with
Return a dict with the current values of the requested DHCP options set
Delete dhcp options by id or name.
Given valid DHCP options id and a valid VPC id , associate the DHCP options record with the VPC .
Check if a dhcp option exists .
Given a vpc_id , creates a network acl .
based on the network_acl_id or network_acl_name provided .
Checks if a network acl exists.
Given a network acl and subnet ids or names , associate a network acl to a subnet .
Given a subnet ID , disassociates a network acl .
Creates a network acl entry .
Deletes a network acl entry .
Creates a route table .
Deletes a route table .
Checks if a route table exists.
Checks if a route exists.
Given a route table and subnet name or id , associates the route table with the subnet .
Dissassociates a route table .
Replaces a route table association.
Creates a route .
Deletes a route .
Replaces a route.
Given route table properties , return route table details if matching table ( s ) exist .
Given route table properties , return details of all matching route tables .
find subnet explicit route table associations
peering connection between two VPCs .
:type name: String
Returns any VPC peering connection id(s) for the given VPC
Get the ID associated with this name
Delete a VPC peering connection.
Check if a VPC peering connection is in the pending state .
Check if a VPC peering connection is in the pending state , and requested from the given VPC .
retrieve an AWS resource id keyed by name .
Return a boto connection for the service .
are `` truthy '' ( neither None ,
named module .
based on IP address or CIDR notation
in_dict contains the series of recursive keys defined in keys .
Helper function to:
Return the active number of minions to maintain
Return a list of minions to use for the batch run
Execute the batch run
Return the status for a service via dummy , returns a bool
Send a message to an XMPP user
Send a message to an list of recipients or rooms
Ensures a record is present .
Ensures a record is absent .
Generates a salted hash suitable for /etc/shadow .
Returns a random integer number between the start and end number.
Returns a random number within a range.
Grab MySQL Connection Details
using a MySQL user table
Create a check on a given URL .
Delete a check on a given URL
checked by uptime
call the vagrant functions .
look for changes from any previous init of machine.
return it in dictionary form
Add a user to the minion
Return the list of all info for all users
Change the uid for a named user
Change the default login class of the user
Return user information
Get the login class of the user
Change the username for a named user
Open the connection to the Nexsu switch over the NX-API .
Executes an RPC request over the NX-API.
Warning , this is a picklable decorator !
Manage a multiprocessing pool
be called from another process when running a map in
be called from another process when building the
dict to defaults and allow for opts to be overridden in
Pass the cloud function and low data structure to run
List all available sizes in configured cloud systems
List all available images in configured cloud systems
List all available locations in configured cloud systems
Query select instance information
create , names is a list of vm names to allocate
execute a map
Destroy the named VMs
Create the named VMs , without using a profile
Perform actions with block storage devices
Execute a single action via the cloud plugin backend
Return the configured providers
Get a dict describing the configured providers
Return a dictionary describing the configured profiles
Return a mapping of what named VMs are running on what VM providers
Return an optimized mapping of available providers
Return a mapping of all image data for available providers
Return a mapping of all configured profiles
Create/Verify the VMs in the VM data
Reboot the named VMs
Create a single VM
Create vm config.
passed on the command line and determine how to
Perform an action on a VM which may be specific to this cloud provider
Perform a function against a cloud provider
Remove any mis-configured cloud providers from the available listing
Read in the specified map and return the map structure
Create a data map of what to execute on
Execute the contents of the VM map
Wrapper for user.info Salt function
Return default GnuPG home directory path for a user
Create the GPG object
Helper function for Listing keys
searching keys from keyserver
Search keys from keyserver
List keys in GPG keychain
Create a key in the GPG keychain
Get a key from the GPG keychain
Export a key from the GPG keychain
add them to keychain
Set the trust level for a key in GPG keychain
Sign message or file
Verify a message or file
Encrypt a message or file
Decrypt a message or file
versionadded : : 2018.3.4
is older than the timeout value then apply the
Check the context for the notifier and construct it if not present
Watch the configured files
is installed , run the command; Run the actual gem command .
Installs one or several gems.
Uninstall one or several gems.
Update one or several gems .
Update rubygems.
Print out the version of gem
installed gems .
Add a gem source .
Remove a gem source .
List the configured gem sources .
Generate an icinga2 ticket on the master.
Generate an icinga2 certificate and key on the client.
Save the certificate on master icinga2 node .
Request CA certificate from master icinga2 node.
Setup the icinga2 node.
Return available Linode images.
Return available Linode datacenter locations.
Return available Linode sizes.
Boot a Linode.
Clone a Linode.
Create a single Linode VM .
Creates a Linode Configuration Profile .
given linode .
Return the size of of the data disk in MB
Returns public and private IP addresses.
named Linode .
Returns the Linode ID for a VM from the provided name .
decode a user-supplied Linode plan label
Returns the Linode Plan ID .
Return a list of the VMs that are on the provider .
Reboot a linode .
Displays details about a particular Linode VM.
is only an estimate , based on
Start a VM in Linode .
Updates a Linode 's properties .
Returns the DATA response from a Linode API query as a single pre-formatted dictionary
format and parse linode data
Make a web call to the Linode API .
Wait for a Job to return.
Wait for a certain status from Linode.
Return linode status by ID
provided name fits Linode 's labeling parameters .
Given the following NAPALM grains , we can determine the Capirca platform name :
load the configuration of a policy term .
load the configuration of a policy filter .
load the configuration of the whole policy .
be used inside a state SLS ,
Get Pure Storage FlasBlade configuration
Private function to
Return name of Snapshot
Private function to check
is started via the given parameters .
Prepare the arguments
Execute a single command via the salt-ssh subsystem and return a
Execute a single command via the salt-ssh subsystem and return all
Execute a salt-ssh call synchronously .
Execute a command on a random subset of the targeted systems
Verify that the given module is set to the given target
Filters a list of dictionary by a set of key-value pair.
Filters a dictionary of dictionaries by a key-value pair.
close the config session with the network device ,
Builds the config logic for `load_config` and `load_template` functions.
passed as arguments .
Calls the method traceroute from the NAPALM driver object and returns a dictionary with the result of the traceroute
Executes a ping on the network device and returns a dictionary as a result .
returns a list of dictionaries with details of the ARP entries .
Returns a detailed view of the LLDP neighbors.
Returns the MAC Address Table on the device .
be loaded from a file or from inline string .
loads the result on the device .
prompt if the configuration has been changed .
check if the configuration was changed .
Set parameters for iDRAC in a blade.
Manage a Dell Chassis .
Manage switches in a Dell Chassis.
Update firmware for a single host
update the firmware on host
are set for the socket .
try to renegotiate authentication
Send the minion id to the master so that the master may better
Register an on_recv callback
forking we need to create all of the local sockets to listen to the
incoming messages from underylying tcp streams
incoming streams and add messages to the incoming queue
Shutdown the whole server
Override _create_stream() in TCPClient.
Try to connect for the rest of time !
received messages ( that we did n't initiate )
Send given message , and return a future
specified in the configuration file
is on the master side this will; Do anything necessary pre-fork .
Publish "load" to minions
match the path and ref , read the file out of git
given option and
raise an error with a logical formatted message .
returns the contents of a text file
return current configuration
validate an integer
validate a float
dotted quad or integer CIDR netmask
validate an IPv6 integer netmask
is in `` within `` and optionally a `` dtype ``
contains one or more space-delimited values
lookup the validation function for a [addrfam][attr] and
return current configured interfaces
given options and outputs valid settings for ETHTOOLS_OPTS
given options and outputs valid settings for ETHTOOLS_PPPOE_OPTS
given options and outputs valid settings for requested
given options and outputs valid settings for bond2 .
given options and outputs valid settings for BRIDGING_OPTS
given options and outputs valid settings for a
given options and outputs valid settings for
Writes a file to disk
Return what would be written to disk
Return what would be written to disk for interfaces
Create a bond script in /etc/modprobe.d with the passed settings
Build an interface script for a network interface.
using up commands .
Return the content of a bond script
Return the contents of an interface script
Return the contents of the global network script .
Return the routes for the interface
Apply global network configuration.
Build the global network script .
Set a single salt process environment variable .
Set multiple salt process environment variables from a dict.
Get a single salt process environment variable .
Determine whether the key exists in the current salt process
Get one or more salt process environment variables .
Extract key and value from key=val string.
Returns the directory of the pillars ( repo cache + branch + root )
Accept the key for the VM on the hyper, if authorized.
Set a Physical Device to be used as an LVM Physical Volume
is not being used by lvm
Create an LVM Volume Group
Create a new Logical Volume
Remove a given existing Logical Volume from a named existing volume group
Return a client object for accessing consul
Called once for each message produced to indicate delivery result .
Return information to a Kafka server
Verify that the correct versions of composer dependencies are present .
update the directory to ensure we have the latest versions
see if a cache cluster exists .
see if a replication group exists .
Create replication group.
Get replication group information.
Get the configuration for a cache cluster .
Get hostname from cache node
Get hostname from replication cache group
Return a list of all cache subnet groups with details
Return a list of all cache subnet group names
see if an ElastiCache subnet group exists .
Create an ElastiCache subnet group
Get information about a cache subnet group .
Delete an ElastiCache subnet group.
Create a cache cluster .
Delete a cache cluster.
Create a cache security group .
Delete a cache security group.
Authorize network ingress from an ec2 security group to a cache security
get the string to print out , try the configured outputter , then
Get the progress bar from the given outputter
Update the progress iterator for the given outputter
Print the passed data using the desired output
Return a printer function
Return the formatted outputter string for the passed data
Return the formatted outputter string , removing the ANSI escape sequences .
Return the formatted string as HTML .
prevent unsafe strings
Split out a domain in its parents
schema = OrderedDict({
Use dig to lookup addresses
Use drill to lookup addresses
Use Python's socket interface to lookup addresses
Use host to lookup addresses
Use dnspython to lookup addresses
Use nslookup to lookup addresses
return their data
Query DNS for information.
Return a list of addresses for name
Validate and parse DNS record data for a CAA record
Return PTR name of given IP
Validate and parse DNS record data for SOA record(s)
Validate and parse DNS record data for SPF record(s)
Generate SRV record data
Generate SRV record name
Validate and parse DNS record data for SRV record(s)
Generate an SSHFP record
Validate and parse DNS record data for TLSA record(s)
Generate a TLSA rec
Find an SRV service in a domain or it's parents
Parse through system-known services
Parse a resolver configuration file ( traditionally /etc/resolv.conf )
Get the returner options from salt .
Fetch a given option value from the config .
generating all duples `` ` option name - > value `` `
profile specific options if applicable
comparing SHA1s .
build list of opts for git.branch , for the purposes of
Return the local revision for before/after comparisons
exists in the given directory
versionadded : : 2018.3.3,2019.2.0
Return a result dict if :; Execute the onlyif and unless logic .
Pushing a text note .
Verify the webhook signature from travisci
named interface is configured properly .
Manage network interface static routes.
are configured properly .
Creates new mediatype .
does not exist , eventually deletes the mediatype .
Sanitize host string.
Return status of a port
given hostname or None if not found .
Get list of possible host names and convention names.
Return only first element of the hostname from all possible list.
Return a socket object for the addr
Returns the fully qualified hostname
Returns the hostname of a given IP
telling if the entity name is a reachable host ( IPv4/IPv6/FQDN/etc ) .
telling if the passed IP is a valid IPv4 or IPv6 address .
telling if the value passed to it was a valid IPv4 address .
telling if the value passed to it was a valid IPv6 address .
Filters and returns only valid IP objects.
Returns the interfaces IP address , e.g .
Return the list of hosts within a network .
Get the size of a network .
Returns the `` natural '' mask of an IPv4 address
Returns an IPv4 netmask
Returns the number of bits that are set in a 32bit int
ifconfig to return a dictionary of interfaces with various information
Obtain interface information for *NIX/BSD variants
= 8 where the ifconfig
Returns a dictionary of interfaces with various information about each
Obtain interface information for Windows systems
Return a dictionary of information about all the interfaces on the minion
Return the address of the network
Turns an IPv4 netmask into it 's corresponding prefix length
supported ) and optionally netmask
dotted quad and returns a string representing its binary
is available , return interface info and no error , otherwise
Return the hardware address ( a.k.a .; given interface on AIX
Return the hardware address ( a.k.a .
Return the details of ` iface ` or an error if it does not exist
Returns a list of subnets to which the host belongs
is within specified subnet , otherwise False
Return the full list of IP adresses matching the criteria
Convert a hex string to an ip , if a failure occurs the original hex is
Convert a MAC address to a EUI64 identifier
Return a dict describing all active tcp connections as quickly as possible
Return a set of ip addrs active tcp connections
Parse a single line from the contents of /proc/net/tcp or /proc/net/tcp6
Returns set of ipv4 host addresses of remote established connections
OpenBSD specific helper function.
Linux specific helper function.
Generates a MAC address with the defined OUI prefix .
Convert a MAC address string into bytes .
Provides a convenient alias for the dns_check filter.
Return the ip resolved by dns , but do not exit on failure , only raise an
Takes a string argument specifying host or host : port .
Verify if hostname conforms to be a FQDN .
List active mounts on Linux systems
List active mounts on AIX systems
List active mounts on FreeBSD systems
List active mounts on OpenBSD systems
List active mounts on Mac OS systems
Resolve user and group names in related opts
List the active mounts .
versionchanged : : 2016.3.2
Verify that this mount is represented in the fstab , change the mount
Remove the mount point from the auto_master
Verify that this mount is represented in the auto_salt , change the mount
List the contents of the auto master
Mount a device
remount a device , if the device is not already mounted , mount
unmount a device by specifying the directory it is mounted on
passed is a fuse mountable application .
Return a dict containing information on active swap
Activate a swap disk
Deactivate a named swap mount
Return the contents of the filesystems in an OrderedDict
versionadded : : 2018.3.3
Returns an instance with just those keys
Compare potentially partial criteria against line
built filesystems entry dictionary
Returns the __virtual__ .
Calls arbitrary methods from the network driver instance.
Returns the options of the napalm device .
Initialise the connection with the network device through NAPALM .
is used to make the execution module functions
Return the final state output .
listen for webhooks to send to the reactor .
be downloaded if; are installed .
are uninstalled .
Updates that match the passed criteria are installed .
is essentially the raw record , minimally munged to provide
are formatted in logstash 's expected format .
connected minions on a salt-master
Return the first configured instance .
Return a dict of all available VM locations on the cloud provider with
Return a list of the images that are on the provider
Return a list of the image sizes that are on the provider
Return a list of the VMs that are on the provider
Return the image object to use
Used by create_node ( ) .; Return the VM 's size .
Return the VM 's location
Create a single VM from a data dict
Make a web call to DigitalOcean
Show the details from DigitalOcean concerning a droplet
Show the details of an SSH keypair
cloud provider .
Upload a public key
Return the ID of the keyname
check termination protection and warn if enabled .; Destroy a node .
Creates a DNS record for the given name if the domain is managed with DO .
given hostname if the domain is managed with DO .
Show the details of a floating IP
Create a new floating IP
Delete a floating IP
Unassign a floating IP
format and parse node data .
Reboot a droplet in DigitalOcean .
loop through all node information .
based on a
Adds a cluster to the Postgres server .
Return a list of cluster of Postgres server ( tuples of version and name ) .
does n't try; Remove a cluster on a Postgres server .
parse the output of pg_lscluster
Get the grains from the proxied device .
Check etcd for all data
is not present .
are open for a protocol , in a direction .
csf.nics_skipped < salt.states.csf.nics_skipped > `
name
Ensure the state of a particular option/setting in csf.
Allow top level cfg to be YAML
Remove the cached pip version
Locate the pip binary , either from ` bin_env ` as a virtualenv , as the
Get the location of a cached requirements file ; caching if necessary .
Return the path to the activate binary
be used to complete
Process the requirements argument
Install packages with pip
Uninstall packages individually or from a pip requirements file
Return a list of installed packages either globally or in the specified
check to see if
Check whether or not an upgrade is available for all packages
versionadded : : 2017.7.3
checking for the specified SSL certificates
Start the server loop
Get the os architecture using rpm -- eval
passed arch are 32-bit
return a pkginfo namedtuple
Resolve the package name and arch into a unique name referred to by salt .
parse an rpm/repoquery command 's output .
Given a list of comments , strings , a single comment or a single string ,
string into epoch , version and release .
exists with the specified rules .
given a group name or a group name and vpc id ( or vpc name ) :
Split rules with lists into individual rules.
see if two rules are the same .; Needed to compare rules fetched
given a list of desired rules ( rules ) and existing rules ( _rules ) return
given a group name or group name and vpc_id ( or vpc name ) :
specified name does not exist .
validate tags are correct
ordered dictionary to a dictionary
Quorum state
Check consul for all data
Query consul for all keys/values within base path
remove any keys which
formatting to be used as pillar data and
is a string - return it as is .
Gather group members
Check time range
Read the last btmp file and return information on the failed logins
Sprinkle with grains of salt, that is
Prepare the payload for Slack
Generate a report of the Salt function
Send a message to a Slack room through a webhook
Send a slack message with the data through a webhook
Checkout git repos containing :ref:`Windows Software Package Definitions
Return the actual lxc client version
set in __context__
are potentially available as LXC bridges
Interface between salt.cloud.lxc driver and lxc.init
Return a random subset of cpus for the cpuset config
Network configuration defaults
Return a list of dicts from the salt level configurations
Parse the nic setup inside lxc conf tuples back to a dictionary indexed by
does not exist , then create it , if it does exist
Initialize a new container .
lxc.init to be used from the saltcloud lxc driver
Create a new container .
Create a new container as a clone of another container
Return a list of the containers available on the minion
classified by state
Raise an exception if the container does not exist
is not currently running , start it .
Start the named container
Stop the named container
Freeze the named container
Unfreeze the named container.
Destroy the named container.
named container exists .
Returns the state of a container.
Returns the value of a cgroup parameter for a container
Set the value of a cgroup parameter for a container.
Returns information about a container
versionchanged : : 2015.5.0
Edit LXC configuration options
Determine if systemD is running
Get the operational state of a systemd based container
Check that the system has fully inited
named container can be attached to via the lxc-attach
Common logic for lxc.run functions
Get the MD5 checksum of a file from a container
versionchanged : : 2015.8.0
Read in an LXC configuration file .
Write out an LXC configuration file
is already present inside the
Reboot a container .
Reconfigure a container.
Returns a container pid.
Add a veth to a container .
match the pattern from the config data
return known serializer based on
generate an event
Return Bluez version from bluetoothd -v
Get the many addresses of the Bluetooth adapter
Power a bluetooth device on or off
Enable this bluetooth device to be discoverable .
Scan for bluetooth devices in the area
Block a specific bluetooth device by BD Address
Pair the bluetooth adapter with a device
Unpair the bluetooth adapter from a device
Send a message to an MS Teams channel .
Scan for processes and fire events
List the nodes , ask all 'vagrant ' minions , return dict of grains ( enhanced ) .
List the nodes , ask all 'vagrant ' minions , return dict of grains .
Provision a single machine
Destroy a node .
Reboot a vagrant minion .
Remove the rtag file
Write the rtag file
Check whether or not a refresh is necessary
Returns the first version of the list of available versions which matches
Set up openstack credentials
delete an object from a container .
List the contents of a container , or return an object from a container .
Create a new container , or upload an object to a container .
Get the entire accountPolicy and return it as a dictionary .
Set a value in the user accountPolicy .
Return the value for a key in the accountPolicy section of the user 's plist
Converts a unix timestamp to a human readable date/time
Return information for the specified user
Get the date/time the account was created
Get the the number of failed login attempts
Set the maximum age of the password in days
Get the maximum age of the password
be required to; Sets the date on which the password expires .
be able to; Sets the date on which the account expires .
Deletes the account password
named user ( insecure , the password will be in the
Ensure the RabbitMQ policy exists.
named policy is absent
Return the preferred Internet protocol .
Return a list of VMs
Return a list of VMs with minimal information
Return a list of VMs with all the information about them
Return a list of VMs with the fields from ` query.selection `
Get VM on this OpenStack account
List available images for OpenStack
List available sizes for OpenStack
List networks for OpenStack
List subnets in a virtual network
be sent to create_server
Request an instance to be built
Delete a single VM
Call function from shade .
Merge a data structure into another by choosing a merge strategy
Parse a string or file through Salt 's renderer system
Serialize a Python object using a : py : mod : ` serializer module
Deserialize a Python object using a : py : mod : ` serializer module
Create a standardized comment block to include in a templated file .
Execute the salt-cloud command line
sure the user is inside the specified htpasswd file
sure the user is not in the specified htpasswd file
see celery documentation .
Execute the salt command given by cmd dict .
returns dict of function signature ( s ) specified by cmd .
does and also a client type string .
Create token with creds.
token is valid Then returns user name associated with token
Get a single salt event .
fires event with data and tag
Get current timezone (i.e.
Get current numeric timezone offset from UTC (i.e.
Sets the timezone using the tzutil .
Compares the given timezone with the machine timezone .
activate the given product key
Set the quota for the system
Report on quotas for a specific volume
-u -g are passed in; Parse the output from repquota .
Calls out to setquota , for a specific user or group
Runs the quotastats command , and returns the parsed output
Turns on the quota system
Turns off the quota system
is on or off
Ensure pagerduty user exists.
Generate rsync options
versionchanged : : 2016.3.0
Ensure an role exists
Install a bundle ID or command as being allowed to use
existing assistive access application .
Remove a bundle ID or command as being allowed to use assistive access .
Get a list of all of the assistive access applications installed ,
Deserialize any string of stream like object into a Python data structure .
Serialize Python data to YAML.
Get the serial number of a certificate file
Get all of the certificate serials in the specified store
given cert into the given Certificate Store
named DNS record is present with the given ttl .
named DNS record is absent .
runs it through the JSON
run at ( 1 ) or return None .
queued and running jobs or only those with
Remove jobs from the queue.
Add a job to the queue .
Print the at ( 1 ) script that will run for the passed job
Return match jobs list
Render profile as Jinja2.
Get profile.
Get available profiles.
Get the current user
given , the current euid will; Get the uid for a given user name .
Using the win32 api , determine if the user with token 'user_token ' has
Get a user name for publishing .; find the user is `` root '' attempt to be
Change the current process to belong to the specified user ( and the groups
initialise uid/gid and umask
does n't exist , a; Returns the specified user 's default group .
Returns a list of all of the system group names of which the user
Returns a dict of all of the system groups as keys, and group ids
Returns a list of all of the system group IDs of which the user
given , the current egid; Get the gid for a given group name .
serve HTML for all URLs
is supplied , put it in the cookie dict
is a list of restricted IPs , verify current
Handle both simple and complex CORS requests
Determine the best output format based on the Accept header , execute the
Determine the best handler for the requested content type
skip a processor function if process_request_body is False
run through CherryPy 's formatter )
Unserialize raw POST data in JSON format to a Python data structure.
Unserialize raw POST data in YAML format to a Python data structure.
Unserialize POST/PUT data of a specified Content-Type.
incoming unserialized request data
Returns a WSGI app and a configuration dictionary
Pull a Low State data structure from request and execute the low-data
Send one or more Salt commands in the request body
Show the list of minion keys or detail on a specific key
Authenticate < rest_cherrypy-auth > ` against Salt 's eauth system
Check if this is a valid salt-api token or valid Salt token
Return a websocket connection of Salt 's event stream
Serve a single static file ignoring the remaining path
Set an attribute on the local instance for each key/val in url_map
Assemble any dynamic or configurable URLs
Combine the CherryPy configuration with the rest_cherrypy config values
Return a set of services that are installed but disabled
is available , otherwise returns
Return the status for a service .
named service is enabled , false otherwise
Create a virtualenv and optionally manage it with pip
given options and outputs valid settings for bond1 .
given options and outputs valid settings for a vlan
returns the contents of a file
Build a route script for a network interface .
Return the contents of the interface routes script .
Set the current idle delay setting in seconds
Set the clock format, either 12h or 24h format.
is visible in the clock
Get whether the idle activation is enabled
Get key in a particular GNOME schema
Set key in a particular GNOME schema
get the value for user in gsettings
set the value for user in gsettings
List the packages currently installed in a dict : :
returns the results in a dict
specifying any packages; List the modified files that belong to a package .
specifying any packages will; List the files that belong to a package .
List the files that belong to a package , sorted by group .
Return the name of the package that owns the file .
Return a formatted diff between current file and original in a package .
Return a detailed package ( s ) summary information .
versionadded : : 2015.8.9
Get connection to haproxy socket .
List servers in haproxy backend.
Enable Server in haproxy
Get server weight
be useful to
Show HaProxy frontends
Show HaProxy Backends
is configured with the included beacon data .
Initialise netscaler connection
Return a service group ressource or None
Returns a list of members of a servicegroup or None
Returns a member of a service group or None
Checks if a service group exists
Add a new service group
Delete a new service group
Check if a server : port combination is a member of a servicegroup
Check if a server : port combination is in state UP in a servicegroup
Enable a server : port member of a servicegroup
Add a server : port member to a servicegroup
Returns a service ressource or None
Checks if a service is UP
Enable a service
Disable a service
Checks if a server exists
Add a server
Delete a server
Update a server 's attributes
Check if a server is enabled globally
Enables a server globally
Checks if a vserver exists
Add a new lb vserver
Delete a lb vserver
Bind a servicegroup to a vserver
Binds a SSL certificate to a vserver
List zones for the given profile
given zone_id on the given profile
Create a new zone .
existing zone .
Create a new record .
Delete a zone.
Delete a record .
Export Zone to the BIND compatible format.
Call an extended method on the driver
Return a string representation of a DNS record type to a
Make a web call to IFTTT .
Trigger a configured event in IFTTT.
Opens the connection with the network device .
Return the connection status with the remote device .
Closes connection with the device.
Calls a specific method from the network driver instance .
Return information about the license, if the license is not
corresponding swaggerfile is deployed to the
retrieve stage variables from pillars/options , if the
associated with the given api_name deployed by boto_apigateway 's
generate a md5 hash of the swagger definition file
generate pretty printed json output
see if given name has any of the patterns in given matches
reduce the amount of information that will be kept in the change log
logging create/update/delete operations to AWS ApiGateway
helper function to update errors in the return structure
Ensure the spcifieda usage plan with the corresponding metrics is deployed
identified by name is no longer present
identified by name is added to provided api_stages
help validate the convention established in the swagger file on how
contains only known elements
High level check/validation of the input swagger file based on
return the tuple of model and its schema to create on aws .
specified in the swagger file
returns the deployment label dictionary ( mainly used by
find whether there are other stages still associated with a deployment
find whether there are deployments left with stages associated
find the deployment id that the stage name is currently assocaited with .
find the deployment label that the stage_name is currently associated with .
return the deployment id matching the desired deployment label for
given stage_name 's stage variables with the given stage_variables
associate the stage_name to the given deploymentId and make this current
matches the given api_name and the hardcoded _Swagger.AWS_API_DESCRIPTION
delete the given stage_name .; tied to the given
helps determine if the given stage_name is already on a deployment
tie the given stage_name to a deployment matching the given swagger_file
clean up resources and models if we detected a change in the swagger file
create the top level rest api in AWS apigateway
delete a Rest Api named defined in the swagger file 's Info Object 's title value .
reference models created on aws apigw
map model schema to aws notation
build the list of models the given object schema is referencing .
build a map of model to their list of model reference dependencies
find the next model that should be created
deploy swagger file 's definition objects and associated schema to AWS Apigateway as Models
construct lambda name based on the rule specified in doc string of
construct the lambda uri for use in method integration
construct the method request params , models , request_templates and
specified in a response schema
construct the method response params , models , and integration_params
create a method for the given resource path , along with its associated
deploy resources defined in the swagger file .
Create an event on the PagerDuty service
Execute an arbitrary query on the specified database
requires that a; Perform a query against an S3-like API .
writes updates into a temporary
provides the list of nodegroups of which the minion is a member .
be found at :
Make a web call to QingCloud IaaS API .
Return a dict of all available locations on the provider with
Return a list of the images that are on the provider .
Show the details from QingCloud concerning an image .
Return a list of the instance sizes that are on the provider .
Used by list_nodes ( ) -related; Normalize the QingCloud instance data .
Return a list of the instances that are on the provider .
Show the details from QingCloud concerning an instance .
Create a single instance from a data dict .
Start an instance.
Stop an instance.
Destroy an instance.
is present .
Find changes in ansible return data
Clears community details.
Re-adds the value of a specific key in the dict, only in case of valid str value.
desired config and clears interesting details .
build diff parts ( added , removed , updated ) .
Builds the diff dictionary.
Computes the differences between the existing and the expected SNMP config .
Calls the configuration template to apply the configuration changes on the device .
Configures the SNMP on the device as specified in the SLS file .
Check version number against version specification info and return a
Takes a package name and version specification ( if any ) and checks it using
Takes a package name and version specification ( if any ) and checks it is
Compares two version strings using pkg_resources.parse_version .
sure the package is installed
looks up all packages in the available
using salt-cloud .
specified names exist .
Create a single instance on a cloud provider , using a salt-cloud profile .
Check that a block volume exists .
Check if a block volume is attached .
is present in the schedule
is disabled in the schedule
Prepare the connection with the remote network device , and clean up the key
Return the connection object to the pyeapi Node .
Invoke an arbitrary pyeapi method.
Sends the commands over the transport to the device .
Configures the node with the specified commands .
Retrieves the config from the device .
Run ``oscap xccdf`` commands on minions.
List the profiles available
Return current active profile
Activate specified profile
Send a message to a Slack room .
Send an slack message with the data
wish to handle the decoded data
convert string to date time object
character encoding to unicode .
Reboot a running system .
Shutdown a running system.
Abort a shutdown.
Set the Windows computer name
has been changed , and the; Get a pending computer name .
Set the Windows computer description
Get system information.
requires a restart before this will
Requires a reboot .; Join a computer to an Active Directory domain .
join the domain .
Requires a restart .; Unjoin a computer from an Active Directory Domain .
Get the domain or workgroup the computer belongs to .
workgroup the computer belongs to .
attempts to parse the input time_str as a date .
Get the system time .
is an element of the date , but
< mm-dd-yy > format for the date .
are pending Component Based Servicing tasks that
is a pending domain join action that requires a
are pending file rename operations that require a
are pending Server Manager tasks that require a
are pending updates that require a reboot .
is a reboot pending .
named extension is present .
named extension is absent .
check configuration variables for auth backends
authenticate plaintext and digest methods
Provide authentication via Apache-style htpasswd files
Provide authentication via Apache-style htdigest files
Gate function for _htpasswd and _htdigest authentication backends
File based authentication
named bridge exists , eventually creates it .
named bridge does not exist , eventually deletes it .
Execute a command and read the output as YAMLEX
named deployment is absent from the given namespace .
named deployment is present inside of the specified
named service is present inside of the specified namespace
named service is absent from the given namespace .
named namespace is absent .
named namespace is present .
named secret is absent from the given namespace .
named secret is present inside of the specified namespace
named configmap is absent from the given namespace .
named configmap is present inside of the specified namespace
named pod is absent from the given namespace .
named pod is present inside of the specified
named label is absent from the node .
Ensures the label folder does n't exist on the specified node .
named label is set on the named node
Device-Mapper Multipath flush
Set debconf selections from a file or a template
Set debconf selections
Scan for Docker events and fire events
Return configuration
is a helper call to update_object .
is a low level api call .
Create a cname record .
Get CNAME information.
is a helper call to delete_object .
Get host information
Get all host information
Get host domain name
Get hostname
Get mac address from host record.
Get ipv4 address from host record.
Get host ipv4addr information
Get host ipv6addr information
is helpful when looking up subnets to use
Delete host
Get ip range
Delete ip range.
Get A record
Delete A record
infoblox A record .
infoblox A record is removed .
Get the kapacitor URL .
Get a dict of data on a task .
Run a Kapacitor task and return a dictionary of info .
Define a task .
'ip a '
Parse the contents of `` /proc/net/route ``
Convert hex fields from /proc/net/route to octects
Returns the state of connman
Returns the technologies of connman
Returns a list with all connman services
Verify if a connman service is connected
validate ipv4 values
coresponding service to given interface if exists , otherwise return None
given connman service
return dns list
Get values for some options and a given section from a config file .
return requestmode for given interface
given interface using a blacklist .
Return information about an interface from config file.
return base details about given interface
given ethercat interface
given tcpip interface
return details about given interface
converts a dictionary object into a list of strings
Get details about all the interfaces on the minion
Enable or disable an interface on a legacy distro
Enable or disable an interface
persist a configuration in the ini file
need reboot if it does n't; specified adapter to use EtherCAT adapter mode .
specified adapter to use DHCP with linklocal fallback
specified adapter to use DHCP only
is not detected as a service by Connman ( i.e .; is down )
specified adapter to use ipv4 manual settings
given interface .
Return an event object suitable for the named transport
containing the arguments passed to an orchestration job
build a namespaced event tag string
Calculate the master stats and return the updated stat info
Return the string URI for the location of the pull and pub sockets to
matching the passed tag .
Establish the publish connection
Close the publish connection (if established)
Establish a connection with the event pull socket
Check the pending_events list for events that match the tag
Check if the event_tag matches the search check .
Get a single publication .
Get the raw event without blocking or any other niceties
Creates a generator that continuously listens for events
Send a single event into the publisher with payload dict `` data '' and
Send a single event to the master , with the payload `` data '' and the
Helper function for fire_ret_load
based on information in the return load
Invoke the event_handler callback each time an event arrives.
Bind the pub and pull sockets for events
Get something from epull , publish it out epub , and return the package ( or None )
Spin up the multiprocess event returner
Take an event and run it through configured filters .
Fire an event off on the master server
running '' dict , this is the return dict from a state
Load data by keys.
Export to the Kiwi config.xml as text.
Get package manager.
Set preferences.
Get user groups.
existing local users .
Create repositories.
Set packages and collections.
Create a system description .
Create document.
Set a key/value pair in the vault service
given period of time , then fire a result of True , requiring
Returns a tuple of (user, host, port) with config, pillar, or default
interprets a string as JSON for usage with mongo
List all MongoDB databases
exists in MongoDB
Remove a MongoDB database
Get MongoDB instance version
List users of a MongoDB database
Create a MongoDB user
Remove a MongoDB user
has specified roles
Grant one or many roles to a MongoDB user
Insert an object or list of objects into a collection
Update an object into a collection
Find an object or list of objects in a collection
ensure that a host with the provided name exists .
Perform any needed setup .
Is the marathon api responding ?
Set up glance credentials, returns
Add image to given dictionary
glance image-create )
glance image-delete )
glance image-show )
Return a list of available images ( glance image-list )
given image .
Known valid names of schemas are :
is met .
Retrieve CSRF and API tickets for the Proxmox API
Execute the HTTP request to the API
based op id 's rather than names as identifiers this
Retrieve a VM based on the ID .
start when the IP is already being used .
requesting a task that runs for a longer period of time a UPID is given .
Retrieve the ( latest ) logs and retrieve the status for a UPID .
Retrieve all hypervisors (nodes) available on this environment
Retrieve all VMs available on this environment
Return the script deployment object
Return a list of the hypervisors ( nodes ) which this Proxmox PVE machine manages
Return a list of the VMs that are managed by the provider
Convert a stringlist ( comma separated settings ) to a dictionary
Convert a dictionary to a stringlist ( comma separated settings )
Download https://<url>/pve-docs/api-viewer/apidoc.js
Return the parameter list from api for defined path and HTTP method
submit the requestdata to create a new node
Get VM configuration
has been reached on a node
has been finished successfully
setting VM status
Get the status for a VM , either via the ID or the hostname
Start a node .
Stop a node ( `` pulling the plug '' ) .
Get summary from the rsync successful output.
Get changes from the rsync successful output.
is always copied to the target .
Get the configuration of the device tree at the given path
Set a data entry in a datastore
Initialize the PostgreSQL data directory
List plugins in an installed wordpress path
Show a plugin in a wordpress install and check if it is installed
Activate a wordpress plugin
Check if wordpress is installed and setup
Run the initial setup functions for a wordpress install
based on regex
Use a custom dumper to ensure that defaultdict and OrderedDict are
Return available OpenNebula images.
Return available OpenNebula locations.
Returns a list of clusters in OpenNebula.
Returns a list of data stores on OpenNebula.
Lists all security groups available to the user and the user's groups.
Lists all templates available to the user and the user's groups.
Lists all virtual networks available to the user and the user's groups.
Reboot a VM .
Start a VM .
Stop a VM .
Returns the OpenNebula version .
given cluster name .
given data store name .
given host name .
given image name .
given security group name .
given template name .
given virtual machine 's name .
given virtual network 's name .
Returns the template format to create a disk in open nebula
Allocates a new image in OpenNebula .
existing image .
Sets the Image as persistent or not persistent .
Deletes a snapshot from the image .
Replaces the image template contents.
Deletes the given security group from OpenNebula .
given security group .
Replaces the security group template contents.
Allocates a new template in OpenNebula .
existing virtual machine template .
Deletes the given template from OpenNebula .
Instantiates a new virtual machine from a template .
Replaces the template contents.
be performed on a given virtual machine .
Allocates a new virtual machine in OpenNebula .
given virtual machine .
Initiates the instance of the given VM on the target host .
Detaches a disk from a virtual machine .
Sets the disk to be saved in the given image .
Takes a new snapshot of the disk image .
Deletes a disk snapshot based on the given VM and the disk_id .
be supplied .; given virtual machine .
Returns the monitoring records for a given virtual machine .
Creates a new virtual machine snapshot from the provided VM .
Deletes a virtual machine snapshot from the provided VM .
Replaces the user template contents.
Allocates a new virtual network in OpenNebula .
Deletes the given virtual network from OpenNebula .
reserved address range from a virtual network .
Retrieves information for the virtual network.
Releases a virtual network lease that was previously on hold .
coming from opennebula and raise if it 's not XML .
Uses the OpenNebula cloud provider configurations to connect to the
Helper function for the list_* query functions - Constructs the
covert xml into a data dictionary .
receive the value that will be assigned
versionadded : 0.14.0
be preceded by the module name
return its location
formatted string for XML operation .
Sends an edit request to the device .
Moves an xpath to the after of its section.
Moves an xpath to the bottom of its section.
Sends a set request to the device .
Used to verify success of commands .; Validates a response from a Palo Alto device .
changing configuration until the lock is released .
does not exist or is not configured with the; exists in the configured state .
Commits the candidate configuration to the running configuration .
Deletes a Palo Alto XPATH to a specific value .
is downloaded .
overwrite the existing value , even if it is not
Moves a XPATH value to a new location.
Release config lock previously held.
exists on the device .; ensure that all configurations are set appropriately .
does not exist or is not configured with; object exists in the configured state .
is None or is not callable , return a lambda that returns
based on remote execution and __opts__ [ 'cmd_safe ' ]
is a valid template engine , process the cmd and cwd through
Retrieve the level code for use in logging.Logger.log().
gather the pillar data fresh
see if the given command can be run
Do the DRY thing and only call subprocess.Popen ( ) once
running commands quietly for minion startup
Check if we are running in the globalzone
List all zones
activate ) the specified zone .
specified zone .
Prepares a zone for running applications .
make sure the configuration of the specified
Move zone to new zonepath.
specified zone from the system .
Install a zone by copying an existing installed zone .
Search the environment for the relative path
have an environment setting , or defaults
Return a chunk from a file based on the data received
Update caches of the storage containers.
Return a file hash based on the hash type set in the master config
Return a list of all files in a specified environment
Return a list of all directories in a specified environment
Get the cache path for the container in question
Get the azure block blob service for the container in question
does n't validate
Return useful information from a SmartOS compute node
Return useful information from a SmartOS zone
SmartOS zone pkgsrc information
Provide grains for SmartOS
salt events and forward them to Logentries
Check if node is peered .
Ensure that the volume exists
Check if volume has been started
Add brick(s) to an existing volume
Get the requests options from salt .
return value from XML
Post data to Nagios NRDP
Send a message to Nagios with the data
Return an influxdb client object
Return data to a influxdb data store
Return the running jobs on the master
Return a dict of JID metadata , or None
is a health check that will confirm the PID is running
Check a whitelist and/or blacklist to see if the value matches it .
Get pillar data for the targeted minions, either by fetching the
Get grains data for the targeted minions, either by fetching the
cached mine data for the targeted minions .
Clear the cached data/files for the targeted minions.
fires the event every second
connected minions and update the cache
remove sockets on shutdown
secure the sockets for root-only access
shutdown cache process
starts updates in intervals and
Return a response in an SMS message
was n't found , otherwise a group object
Return a dict of the files located with the given path and environment
found in an environment
Return all of the files names in all available environments
Read the contents of a text file , if the file is binary then
named file , by default the first file found is written , but the
Send an xmpp message with the data
Combine the host header , IP address , and TCP port into bindingInformation format .
has been deployed .
Delete a website from IIS.
Create an IIS binding.
Remove an IIS binding .
Assign a certificate to an IIS binding .
Remove a certificate from an IIS binding .
Create an IIS application pool.
Remove an IIS application pool .
Set the value of the setting for an IIS container.
Create an IIS application.
Remove an IIS application .
Create an IIS virtual directory.
Remove an IIS virtual directory .
guess the url of zabbix frontend .
JSON request to Zabbix API.
generate the authentication token .
Extends the params dictionary by values from keyword arguments .
versionadded : : 2017.7
Retrieve the version of the Zabbix API .
Delete zabbix users.
given alias exists .
according to the given parameters .
multiple users .
Delete media by id.
matches the given filter criteria exists
enabled user groups .
Delete hosts.
matches the given filter criteria exists .
Update host inventory items
Delete the host group.
Delete host interface
Create new host usermacro.
Delete host usermacros.
existing host usermacro .
existing global usermacro .
mediatypes according to the given parameters .
Create new mediatype
Delete mediatype
Update existing mediatype
templates according to the given parameters .
Send Zabbix API call
Create the named container if it does not exist
is not present , destroying it if present
is running and restart it if restart is True
is stopped , kill it if kill is true else stop it
is migrated to another host
Return the bcache UUID of a block device .
Attach a backing devices to a cache set
Detach a backing device(s) from a cache set
Stop a bcache device
Create a backing device for attachment to a set .
Create BCache cache on a block device.
Show or update config of a bcache device.
Show the full status of the BCache system and optionally all it 's involved devices
Check the state of a single bcache device
Read out BCache SuperBlock
Basename of just about any dev
Full SysFS path of a device
Resolve a bcacheX or cache to a real dev
interface with bcache SysFS
interface with backing devs SysFS
logging around sysfs.attr
parsing BCache 's SysFS interface
Map Bcache's size strings to real bytes
sizing info about a blockdev
REALLY DESTRUCTIVE STUFF RIGHT AHEAD
Wait for lfunc to be True
Simple wrapper around cmd.run_all
Returns the release number of a given release code name in a
compare release codename versions to the minion 's current
Install rbenv systemwide
Updates the current versions of rbenv and ruby-build
Install a ruby implementation .
Uninstall a ruby implementation .
List the installed versions of ruby
sets the currently defined default ruby
List the installable versions of ruby
Execute a ruby command with rbenv 's shims from the user or the system
Execute a ruby command with rbenv 's shims using a specific ruby version
have the given bundle ID or path to command
Enable the service
Disable the service
Check if the service is available
Ensure that the service is running
named service is dead by stopping the service if it is running
is enabled on boot , only use this state if you
is disabled on boot , only use this state if you
called to invoke the watch command .
Used with salt-cp , pass the files dict , and the destination .
receives files copied to the minion using `` salt-cp `` and is
Create a file client and add it to the context .
Render a file as a template before setting it down .
Used to recursively copy a directory from the salt master
Download a file from a URL to the Minion cache directory and return the
Used to cache a single file on the Minion
Download and cache everything under a directory from the master
Cache a local file on the minion in the localfiles cache
Return the hash of a file , to get the hash of a file on the
Return the permissions of a file , to get the permissions of a file on the
pushed to the master will have global read permissions ..
Push a directory from the minion up to the master , the files will be saved
Ensure image exists and is up-to-date
is a mixture of the CLI options -- log-driver and -- log-opt
Additional container-specific post-translation processing
are passed as multiple instances of a given CLI option .
is a list of PATH : WEIGHT pairs , but the API expects a list of
be specified multiple times ,
is in the format NAME [ : RETRY_COUNT ] but the API expects { 'Name ' :
be either a string or a numeric uid
be a list of absolute paths
be an absolute path
Try to guees the kubemaster url from environ ,
based on URL
put any object in kubernetes based on URL
Get name or names out of json result from API server
Check that name is DNS subdomain : One or more lowercase rfc1035/rfc1123
Check that name is IANA service : An alphanumeric ( a-z , and 0-9 ) string ,
Check that name is DNS label : An alphanumeric ( a-z , and 0-9 ) string ,
Get all labels from a kube node .
Replace labels dict by a new one
is namespace is defined otherwise return all namespaces
create namespace on the defined k8s cluster
Replace secrets data by a new one
Get k8s namespaces
Return the correct pillar driver based on the file_client option
Returns the extra data from the minion 's opts dict ( the config file ) .
Return a future which will contain the pillar data from the master
Return the pillar data from the master
need to incur the overhead of caching
set it to the cache , if not found .
see if the on demand external pillar is allowed
Gather the lists of available sls data from the master
need to be altered to conform to the file client
Pull the file server environments out of the master options
Gather the top files
merge the top files
Returns the sorted high data from the merged top files
derived from the top file
return the states
Collect a single pillar sls file and render it
Extract the sls pillar files from the matches and render them into the
updates the `` pillar `` variable
Render the external pillar data
Render the pillar data and return
configured to do so
is loaded , and convert to and extract data from
Get the certificate location contexts and their corresponding stores .
Get the available certificates in the given store .
Get the details of the certificate file .
given certificate store .
Remove the certificate from the given certificate store .
named schema is present in the database .
Return a memcache server object
Return data to a memcache data store
Ensure the ELB exists.
Removing an instance from
ensure that ELB policies are set
validate tags on elb
Return the object that is referenced by the specified proxy .
be used the current thread of execution .
return the object referenced by a proxy .
Return a list of the new modules , pass an lsmod dict before running
make it persistent .
is true only comment line
Return a list of all available kernel modules
Return a list of the loaded module names
Load the specified kernel module
Required.
Ping the device on the other end of the connection
Run command through switch's cli
Get grains for proxy minion
Get username line from switch
Get roles that the username is assigned from switch
Remove user from switch
Assign role to username
run ` show run ` on switch
Add one or more config lines to the switch running config
Delete one or more config lines to the switch running config
is in the running command
Replace string or full line matches in switch's running config
named group is present with the specified privileges
named group is absent
Given a port number and protocol , returns the port definition expected by
Given a port number or range , return a start and end to that range .
come in as a list of VAL1 : VAL2 pairs , but map to a list
be a single string , or a list of strings .
be expressed as an integer number of bytes , or a string
is a list of PATH : RATE pairs , but the API expects a list of
is a list of key/val pairs , but the API expects a dictionary in
be a list of label names , or a list of name=value pairs .
is an open connection from the minion to the defined
Return a mode value that is predictable
is running in , can be set to enforcing ,
Set up an SELinux boolean
force a specific version for an SELinux module
Installs custom SELinux module from given file
Removes SELinux module
is set in an OpenStack configuration file .
Return Fibre Channel port WWNs from a Linux host.
Return Fibre Channel port WWNs from a Windows host.
Return list of fiber channel HBA WWNs
is supported on the vCenter :
Returns a vim.cluster.VsanVcClusterConfigSystem object
Returns a host's vsan system
Creates a disk group
Removes capacity disk(s) from a disk group.
Removes a disk group .
Returns the extended cluster vsan configuration object
Reconfigures the VSAN system of a cluster .
created via the VSAN API
Pass in either a raw pem string, or the path on disk to the location of a
Return the argname value looking up on all possible attributes
Return a serializable form of the config instance
Adds any missed schema attributes to the _attributes list
Returns the definition of the complex item
Returns a dictionary of the complex attributes
see if an RDS exists .
see if an RDS option group exists .
see if an RDS parameter group exists .
see if an RDS subnet group exists .
Create an RDS Instance
Create an RDS read replica
Create an RDS option group
Create an RDS parameter group
Create an RDS subnet group
Update an RDS parameter group.
Return RDS instance details.
Return a detailed listing of some , or all , DB Instances visible in the
Return a detailed listing of some , or all , DB Subnet Groups visible in the
Return the endpoint of an RDS instance .
Delete an RDS instance.
Delete an RDS option group.
Delete an RDS parameter group.
Delete an RDS subnet group.
Returns a list of `DBParameterGroup` descriptions.
Returns a list of `DBParameterGroup` parameters.
Modify settings for a DB instance.
using the SenseHat sensors .
be private .
Makes flavor to be absent
is a compatibility matcher and is NOT called when using
Ensure the existence (or not) of LDAP entries and their attributes
process entries and return before/after views
using the provided directives
convert various things to a set
Set a parameter in /etc/shadow
named user .; be a properly defined
Sets the value for the date the password was last changed to days since the
versionchanged : : 2014.7.0
use only /etc/shadow
be matched by name
detailed info about one , or all , zones in the bound account .
hosted zones in the bound account .
hosted zone .
Create a Route53 hosted zone .
Create a Route53 healthcheck
Get a record from a zone .
Modify a record in a zone .
Create a new Route53 Hosted Zone .
based on pass_path .
try to find a pass path ( string ) that can be handed off to pass
based on pass_path
using a specified format style
Return the firewall-cmd location
Perform zone management
added for or enabled in all zones
Add a new zone
Delete an existing zone
Add a new service
Delete an existing service
added for or enabled in a zone
added for zone as a space separated list .
is omitted , default zone will be used .
be specified multiple times .; Remove a service from zone .
Add a new port to the specified service .
is enabled on a zone .
Enable masquerade on a zone.
Remove masquerade on a zone.
Allow specific ports in a zone.
Remove a specific port from a zone .
List all ports in a zone.
Add port forwarding.
List port forwarding
Block a specific ICMP type on a zone
blocks on a zone
bound to a zone
Bind an interface to a zone
Bind a source to a zone
Add a rich rule to a zone
Return list of mdadm devices
Returns the connection details of the following proxies : esxi
specify which proxy types are supported by a function
connects to a target system ( vCenter or ESXi host ) using the
Run an ESXCLI command directly on the host or list of hosts.
Retrieve information on ESXi or vCenter network dump collection and
Enable or disable ESXi core dump collection.
Enable or disable an ESXi firewall rule set.
Set the specified syslog configuration parameter.
Retrieve the syslog configuration .
Reset the syslog service to its default settings .
Upload an ssh key for root to an ESXi host via http PUT.
Retrieve the authorized_keys entry for root .
Get the date/time information for a given host or list of host_names .
Get the NTP configuration information for a given host or list of host_names .
Get the service name 's policy for a given host or list of hosts .
Get the VMotion enabled status for a given host or a list of host_names .
Get the VSAN enabled status for a given host or a list of host_names .
given host or list of host_names .
Return system information about a VMware environment.
Returns a list of datacenters for the the specified host.
given host of list of host_names .
Start the named service for the given host or list of hosts .
given host or list of hosts .
Update the date/time on the given host or list of host_names .
Update the password for a given host .
Returns the dict representation of the DVS config
Returns the dict representation of the DVS link discovery protocol
Returns the dict representation of the DVS product_info
Returns a list of dict representations of the DVS infrastructure traffic
Returns a list of distributed virtual switches (DVSs).
Applies the values of the config dict dictionary to a config spec
Applies the values of the product_info_dict dictionary to a product info
Applies the values of the capability_dict dictionary to a DVS capability
infra traffic resources ,
Applies the values of the resource dictionaries to network resource pools,
Creates a distributed virtual switch ( DVS ) .
Updates a distributed virtual switch ( DVS ) .
Returns the out shaping policy of a distributed virtual portgroup
Returns the security policy of a distributed virtual portgroup
Returns the teaming of a distributed virtual portgroup
Returns a dictionary with a distributed virutal portgroup data
Returns a list of distributed virtual switch portgroups.
Returns the uplink portgroup of a distributed virtual switch .
Applies the values in out_shaping_conf to an out_shaping object
Applies the values in sec_policy_conf to a security policy object
Applies the values in teaming_conf to a teaming policy object
distributed portgroup spec
Creates a distributed virtual portgroup .
Updates a distributed virtual portgroup .
Removes a distributed virtual portgroup .
Returns a dictionary representation of a policy
Returns a list of storage policies.
Returns the default vsan storage policy .
Returns a list of the metadata of all capabilities in the vCenter.
Applies a policy dictionary to a policy spec
Creates a storage policy .
Updates a storage policy .
assign the the storage policies .
Assigns a storage policy as the default policy to a datastore.
Returns a list of dict representations of VMware datacenters.
Creates a datacenter .
Returns a cluster dict representation from
Returns a dict representation of an ESX cluster.
Applies the values of cluster_dict dictionary to a cluster spec
Creates a cluster .
Updates a cluster .
Returns a list of dict representations of the datastores visible to the
Creates a ESXi host disk group with the specified cache and capacity disks .
needs to be visible to the proxy .
Removes a datastore .; is raised .
Lists all licenses on a vCenter.
Adds a license to the vCenter or ESXi host
Returns the entity associated with the entity dict representation
Validates the entity dict representation
Lists the licenses assigned to an entity
Assigns a license to an entity
specified VMware environment .
Returns a list of dict representations of the disks in an ESXi host.
Erases the partitions on a disk .
Lists the partitions on a disk .
dict representation on an ESXi host .
disk group on an ESXi host with the specified cache and
Remove the diskgroup with the specified cache disk .
Returns the host cache configuration on the proxy host.
Configures the host cache on the selected host .
checks to see if the host provided is a vCenter Server or
format the stdout from the get_coredump_network_config function .
format the stdout from the get_firewall_status function .
format the stdout from the get_syslog_config function .
returns a host object either from the host location or the host_name .
returns a dictionary containing a list of SSD and Non-SSD disks .
returns a dictionary of host_name keys with either a list of eligible
resets the config and populates the return dictionary .
sets the config and populates the return dictionary .
Adds an ESXi host to a vSphere Distributed Virtual Switch and migrates
Returns the target object of a proxy .
Returns the running esxdatacenter 's proxy details
Returns the running esxcluster 's proxy details
Returns the running esxi 's proxy details
Returns vm object properties.
Queries the virtual machine config file and returns
vm container version or schedules upgrade ,
Sets CPU core count to the given value
Sets memory size to the given value
advanced config list
Sets configuration parameters for the vm
Removes configuration parameters for the vm
Removes extra config parameters from a virtual machine
Returns key number of the SCSI controller keys
specifying to add/edit
specifying a virtual
Returns a vim.vm.device.VirtualDevice.BackingInfo object specifying a
specifying to add/edit a
Returns a vim.vm.device.VirtualDeviceSpec object specifying to
Returns a list of vim.vm.device.VirtualDeviceSpec objects representing
specifying to add/edit an
vim.vm.customization.AdapterMapping object containing the IP
representing a serial port
representing the disks to be created for a
Returns a list of vim.vm.device.VirtualDeviceSpec objects representing the
Returns the device with the given key , raises error if the device is
Returns the device with the given label , raises error if the device is
Updates the size and unit dictionary values with the new unit values
Compares virtual machine current and new configuration, the current is the
converts the virtual machine properties to the available format
returns the config spec objects in a list .
specifying the scsi
Returns a list of vim.vm.device.VirtualDeviceSpec specifying
specifying to edit a
specifying to remove a virtual
Creates a virtual machine container .
Updates the configuration of the virtual machine if the config differs
given vmx file .
specified by it 's name .
remove a virtual machine
Deletes a virtual machine defined by name and placement
Acquire Azure ARM Credentials
load the selected client and return a management client object
Log an azurearm cloud error exception
Extract all pages within a paged object as a list of dictionaries
incoming parameters .
found in the `` new '' dictionaries are compared to; representing Azure objects .
are n't met .
Convert the libcloud Node object into something more serializable.
object into something more serializable .
Convert the libcloud GCEAddress object into something more serializable.
Convert the libcloud load-balancer object into something more serializable.
Show the details of the existing instance .
Return a dict of available instances sizes ( a.k.a machine types ) and
Return a dict of all available VM images on the cloud provider with
allows partial name matching and returns a
override libcloud to find the zone .
override libcloud to find the machine type in the proper zone .
configured tags .
configured metadata and add 'salt-cloud-profile ' .
Return public IP, private IP, or hostname for the libcloud 'node' object
Return a GCE libcloud network object with matching name
configured subnetwork .
Return a GCE libcloud region object with matching name .
create a static IP address .
allowed user-string to specified REST API format .
configured SSH credentials .
... versionchanged:: 2017.7.0
delete a network .
Show the details of an existing network .
is used if not specified .; Create a GCE firewall rule .
delete a firewall rule .
Show the details of an existing firewall rule .
Create an HTTP health check configuration.
delete a health check .
Show the details of an existing health check .
Create a static address in a region .
delete a static address .
Show the details of an existing static address .
Create a load-balancer configuration .
delete a load-balancer .
Show the details of an existing load-balancer .
existing node/member to an existing load-balancer configuration .
Remove an existing node/member from an existing load-balancer configuration .
delete a disk snapshot .
delete a persistent disk .
Create a new persistent disk .; specify ` disk_name ` and ` location ` ,
specify ` name ` and ` disk_name ` .; Create a new disk snapshot .
Show the details of an existing disk .
Show the details of an existing snapshot .
Detach a disk from an instance.
existing disk to an existing instance .
Call GCE 'reset ' on the instance .
Call GCE 'start on the instance .
Call GCE 'stop ' on the instance .
be called with `` -a destroy '' or -d; Call 'destroy ' on the instance .
Request a single GCE instance from a data dict.
Create a single GCE instance from a data dict .
Download most recent pricing information from GCE and save locally
Issues HTTP queries to the logstash server.
salt events and forward them to logstash .
is present with the specified properties
Ensure that the grant is absent
Internal cassandra nodetool wrapper.
Return a pycassa system manager connection object
existing column families for all keyspaces
Return a dictionary of column family definitions for the given
Update a tenant 's quota
specified tenant 's quota value
Creates a new port
Updates a port
Deletes the specified port
Creates a new network
Updates a network
Deletes the specified network
Creates a new subnet
Updates a subnet
Deletes the specified subnet
Creates a new router
Updates a router
Delete the specified router
Adds an internal network interface to the specified router
Adds an external network gateway to the specified router
Removes an external network gateway from the specified router
Creates a new floatingip
Updates a floatingip , disassociates the floating ip if
Deletes the specified floatingip
Creates a new security group
Updates a security group
Deletes the specified security group
Creates a new security group rule
Deletes the specified security group rule
configured VPN services for a tenant
Fetches information of a specific VPN service
Creates a new VPN service
Updates a VPN service
Deletes the specified VPN service
Creates a new IPsecSiteConnection
Deletes the specified IPsecSiteConnection
Creates a new IKEPolicy
Deletes the specified IKEPolicy
Creates a new IPsecPolicy
Deletes the specified IPsecPolicy
Create a new firlwall rule
Deletes the specified firewall rule
Update a firewall rule
Create a basic chroot environment .
Executes a Salt function inside a chroot environment .
Save the register to < salt cachedir > /thorium/saves/ < name > , or to an
Search the environment for the relative path.
are asked to update ( regular interval ) lets reap the cache
Return a dict containing the file lists for files , dirs , emtydirs and symlinks
Return a dict of all symlinks based on a given path on the Master
Emit the load averages of this host.
Calls lxd init --auto -- opts
Set an LXD daemon config option
Get an LXD daemon config option
Get an pyxld client , this is not ment to be runned over the CLI .
Saves an object (profile/image/container) and
Authenticate with a remote LXDaemon.
Lists containers
Create a container
Gets a container from the LXD
Delete a container
Rename a container
Get container state
Start a container
Stop a container
Restart a container
Freeze a container
Unfreeze a container
Migrate a container.
Get a container config value
Set a container config value
Delete a container config value
Get a container device
Add a container device
Delete a container device
Put a file into a container
Get a file from a container
Execute a command list on a container .
Lists all profiles from the LXD.
Creates a profile .
Gets a profile from the LXD
Deletes a profile .
Get a profile config item .
Set a profile config item .
Delete a profile config item.
Get a profile device .
Set a profile device .
Delete a profile device.
Lists all images from the LXD.
Get an image by its fingerprint
Get an image by an alias
Delete an image by an alias or fingerprint
Create an image from simplestreams
Create an image from a file
Copy an image from another LXD instance
Create an alias on the given image
is currently not restricted to the image )
Get all snapshots for a container
Create a snapshot for a container
Delete a snapshot for a container
Get information about snapshot for a container
be put into mongodb , which does n't like ` . `
given config and devices with the object
Sets the dict item key of the attr from obj .
Translates a plyxd model object to a dict
Sets the ftp proxy settings
Returns the current domains that can bypass the proxy
Sets the domains that can bypass the proxy
Sets the http proxy settings , only works with Windows .
Get an SMB connection
create a directory structure on an SMB share
allows a string to be
allows a file to be
Looks like a file handle
Get the contents of a template file and provide it as a module type
Fetch all of the templates in the src directory
shutils.copytree but over existing directories , does a recursive merge copy .
apply Jinja2 templating to both
Prompt the user to choose between a list of options , index each one by adding an enumerator
extending the salt ecosystem
Manage the SNMP sysContact , sysLocation , and sysServices settings .
Manage the sending of authentication traps .
Manage the SNMP accepted community names and their permissions .
Check the version of npm to ensure this module will work .
Install an NPM package.
Uninstall an NPM package.
installed NPM packages .
cached NPM packages .
cached packages .
List path of the NPM cache directory.
Send raw ipmi command
Set channel access
get non-volatile Channel Access
Set user access
Get user access
Set user name
Get user name
Set user password and (modes)
Get sensor readings
Request power state change
use on next reboot
Request identify light
Get max users in channel
Get user from uid and access on channel
is created with provided settings .
Delete user (helper)
Verify that a TLS certificate is valid now and ( optionally ) will be valid
Compares two objects and return a boolean value
Validate the beacon configuration .
Watch napalm function and fire events.
Execute ilom commands
Configure the port HTTP should listen on
Configure the port HTTPS should listen on
Enable SSH on a user defined port
Create user
Configure Network Interface
Configure SNMP
Enable the RDP service and make sure access to the RDP
Disable the RDP service
is present and up-to-date in Kapacitor .
is absent from Kapacitor .
use a list of NTP servers
Get list of configured NTP servers
dictionary from the device .
Displays the facts gathered during the connection .
executes the RPC provided as arguments on the junos device .
Set the device's hostname
commit the changes loaded in the candidate configuration .
Roll back the last committed configuration changes and commit
Returns the difference between the candidate and the current configuration
Send a ping RPC to a device
Executes the CLI commands and returns the output in specified format .
reboot a device running Junos OS .
Installs the given configuration file into the candidate configuration .
Resets the device to default factory settings
Installs the given image on the device .
Copies the file from the local device to the junos device
Attempts an exclusive lock on the candidate configuration.
Unlocks the candidate configuration.
Loads the configuration from the file provided onto the device .
Perform a commit check on the configuration
Retrieve data from a Junos device using Tables/Views
Mount an image
exists on the Dell DRAC
Ensure the DRAC network settings are consistent
have additional IPC transports other than UxD and TCP , add them here
have additional IPC transports other than UXD and TCP , add them here
List currently configured reactors
Add a new reactor
Delete a reactor
Convert IP address representation to ZMQ (URL) format.
Display output for the salt-run virt.query function
Return all interface configs
is enabled , otherwise `` False ``
Enable an interface
Disable an interface
convert the netmask to the CIDR subnet length
Set static IP configuration on a Windows NIC
Set static DNS configuration on a Windows NIC
Set DNS source to DHCP on Windows
Add the specified group
Return a list of gids in use
Remove the named group
Replaces members of the group with a provided list.
Return information about a group
formatted information in a pretty way .
Return info on all groups
Change the gid for a named group
Read pillar data from Cobbler via its API.
Verify that ruby is installed , install if unavailable
Check that ruby is installed
Verify that the specified ruby is installed with RVM .
Verify that the gemset is present .
named tablespace is present with the specified properties .
Execute queries against SQLCipher, merge and return as a dict
Yield a SQLCipher cursor
bind to an LDAP server .
Search an LDAP database.
Add an entry to an LDAP database.
Delete an entry from an LDAP database.
Modify an entry in an LDAP database.
return index and host keys .
Check if grafana dashboard row and _row differ
is managed .
named grafana dashboard is deleted .
sequenced data .
Returns the logical value .
Implementation of tojson filter (only present in Jinja 2.9 and later).
Searches for a pattern in the text.
passed as argument .
Removes duplicates from a list.
Returns the average value of a list .
Returns the union of two lists .
Returns the intersection of two lists .
Return a file client .
Cache a file from the salt master
Cache a file only once
find imported jinja files .
printed mappings are YAML friendly .
Render a formatted multi-line XML string from a complex Python
named index alias is present .
named index template is absent .
named index template is present .
named pipeline is absent
named pipeline is present .
Ensure that the search template is absent
named search template is present .
Returns the prefix for a pkg command , using -j if a jail is specified , or
return the version of 'pkg '
pkg search `` will return all packages for which the pattern is a match .
is designed to manipulate packages in jails and chroots , use
Return dict of uncommented global variables.
representing the package version or an empty string if not
Refresh PACKAGESITE contents
List the packages currently installed as a dict : :
Return pkgng stats.
installed packages into yaml+mtree file
created by pkg backup -d and recreates the database .
installed packages against known vulnerabilities
Install package(s) from a repository
Remove a package from the database and system
named or all packages ( run a `` pkg upgrade `` ) .
Cleans the local cache of fetched remote packages
Sanity checks installed packages
package installed a specific file
Searches in remote package repositories
Fetches remote packages
Displays UPDATING entries of software packages
Version-lock packages
Query the package database those packages which are
is identical .
List those packages for which an upgrade is available
Parse the output from the `` pkg upgrade -- dry-run `` command
Do a cmp-style comparison on two packages .
specified to 'install '
List the files that belong to a package , grouped by package .
Get package build time, if possible.
Try to get a license from the package .
based on the /var/lib/dpkg/info/ < package > .list
Get the package information of the available packages , maintained by dselect .
Returns a detailed summary of package information for provided package names.
Override specific variables within a function's global context.
return the ChildContextDict
Get a list of all specified files
Take a path and return the contents of the file as a string
Parse the files indicated in opts [ 'src ' ] and load them into a python
Make the salt client call
Make the salt client call in old-style all-in-one call method
Make the salt client call in the new fasion chunked multi-call way
List the RAID devices .
specified RAID device
Destroy a RAID device .
Create a RAID device .
config file .
Assemble a RAID device .
specified RAID component device
Add new device to RAID array.
Given a domain name , check to see if the given domain exists .
Given a domain name describe its status .
Given a domain name describe its properties .
Given a valid config , create a domain .
Given a domain name , delete it .
Add tags to a domain
Remove tags from a trail
List tags of a trail
Ensure a security group exists.
Parse a logrotate configuration file .
Get the value for a specific configuration line .
Set a new value for a specific configuration line .
Convert a dict to a multi-line stanza
are able to get the lock ( or hit the timeout )
Remove lease from semaphore .
are ` min_nodes ` in the party at ` name ` , optionally blocking if not available .
Execute a list of CLI commands .
Execute an arbitrary RPC request via the Nexus API.
are allowed to do deep merging .
Convert obj into an Aggregate instance
Merge obj_b into obj_a.
Migrate old minion and master pki file paths to new ones.
Simple internal wrapper for cmdmod.retcode
Simple internal wrapper for cmdmod.run
Merge values all values after X into the last value
Detect the datatype of a property
Create a property dict
Parse output of zpool/zfs get command
Internal magic for from_auto and to_auto
escape a zfs command
Check the system for ZFS support
Check if zpool-features is available
Return a dict of zpool properties
Convert zfs bool to python bool
zfs on/off bool
zfs yes/no value
python int ( bytes )
Convert python int (bytes) to zfs size
used for name , path , ... )
Convert python value to zfs value
Pass an entire dictionary to from_auto
Pass an entire dictionary to to_auto
escape a zpool command
Parse the result of a zpool/zfs command
Read the log file and return match whole string
Convert key/value to tree
Evaluate Pepa templates
Validate Pepa templates
Return list of disk devices
are SSD or HDD .
are installed .; Uses the CD ,
are removed .; Uses the CD ,
Grant access to auto-mounted shared folders to the users.
Perform a lightweight check to see if the master daemon is running
Execute the specified function in the specified client by passing the
< all-salt.modules > ` asynchronously
< all-salt.modules > ` synchronously
< all-salt.modules > ` against subsets of minions
< all-salt.modules > ` against batches of minions
Run salt-ssh commands synchronously
< all-salt.runners > ` synchronously
< all-salt.runners > ` asynchronously
< all-salt.wheel > ` synchronously
< all-salt.wheel > ` asynchronously
allows for a; load the json when it starts .
starting with the quick ones and going down the list )
specified by there specs
Grains for backwards compatibility
Provide grains from the SmartOS metadata
Check xdg locations for config files
Ensure the SNS topic exists.
named sns topic is deleted .
Start the saltnado !
named user and group are present on the minion
Performs basic sanity checks on a relative path.
received via salt master to the OS 's native
Generate the list of files managed by a recurse state
Generate the list of files that need to be kept when a dir based function
Compile a list of valid keep files (and directories).
Clean out all of the files and directories in a directory ( root ) while
Check what changes need to be made on a directory
Check the changes in directory metadata
see if a file needs to be updated or created
Check if the symlink ownership matches the specified user and group
return a boolean indicating
Check the symlink function
give us a standard tuple list for sources and
Iterate a list of sources and process them as templates .
ensure `` arg `` is a list of strings
Check the shortcut function
creating directories when the `` makedirs `` option is set
Create a symbolic link ( symlink , soft link )
exists , it will; named file or directory is absent .
unwanted files based on specific criteria .
Verify that the named file or directory is present or exists .
Converse *recurse* definition to a set of strings.
reaching max_depth .
Recurse through a subdirectory on the master and copy said subdirectory
scheduling to backup storage directory .
Line-based editing of a file.
based editing of a file .
delimited by two line markers
specified lines in a file .
specified commented lines in a file
appears at the end of a file .
has been applied to the specified file or directory
Replicate the 'nix `` touch '' command to create a new empty
defined by the `` source `` option exists on the minion , copy it
exists on the system , rename it to the named file .
be used in template in file.managed state .
managed file .
Create a special file similar to the 'nix mknod command .
Execute the check_cmd logic .
encoded file and write it to disk
Create a Windows shortcut
Return the rendered script
Return a dictionary with gateway options .
string for ssh/scp command .
Return the script as a string for the specific os
return them as PEM file strings
was available then we will have a pki_dir key in
removes a specified key from the accepted keys dir
Rename a key , when an instance has also been renamed
Return a minion 's configuration for the provided options and VM
Return a master 's configuration for the provided options and VM
Return a salt configuration dictionary , master or minion , as a yaml dump
is the primary entry point for logging into any system ( POSIX or
Return the ssh_usernames .
Wait until a function finishes, or times out
specified port can be made on a specified
Run a command remotly via the winexe executable
Run a command remotly using the psexec protocol
be established .
Check if the windows credentials are valid
be accessed via password or ssh key
Copy the install files to a remote Windows box , and execute them
Copy a deploy script to a remote server , execute it , and remove it
Run the inline script commands , one by one
return from the dict , and
Fire deploy action
copy a file to a server
using either sftp or scp , as
be run against Windows boxes
be run against Windows boxes using WinRM .
be run as root
is called from a multiprocess instance , to wait for a minion
Converts an IP address to an integer
falls within one of the private IP ranges
Check whether the specified name contains invalid characters
Remove a host from the known_hosts file
Return a list of the VMs that are on the provider , with select fields
Lock a file ; if it is already locked , then wait for it to become available
Unlock a locked file
needs to happen when
needs to happen
Initialize the cachedir needed for Salt Cloud to keep track of minions
Creates an entry in the requested/ cachedir .; means that Salt Cloud has
Changes the info inside a minion's cachedir entry.
Moves a minion from the requested/ cachedir into the active/ cachedir.
Deletes a minion 's entry from the cloud cachedir .
Return a list of minion data from the cloud cache , rather from the cloud
Update the salt-bootstrap script
configured to do so , update the cloud cachedir with the current list of
Cache node individually
see if any nodes which were previously known about
differ , fire an event
Strip out user-configured sensitive event data.
try its best to convert any Unicode text into ASCII
Retrieve particular user's password for a specified credential set from system keyring.
provider password in system keyring
user for a password and stores it in system keyring
Accepts index in form of a string
retrieves some required argument .
Return the salt_interface type to connect to .
is either 0400 or 0600 .
Use the configured templating engine to template the userdata file
Removes the PAExec service and executable that was created as part of
request the master update the publish session key
Generate a RSA public keypair for use with salt
is intended to be the; Load a private key from disk .
Read a private key off the disk .
Read a public key off the disk .
Returns the signature .; sign a message .
verify the signature on a message .
creates a signature for the given public-key with
Generate an M2Crypto-compatible signature
Verify an M2Crypto-compatible signature
Returns a key object for a key in the pki-dir
Return the string representation of a public key
reconnect to the origin
breaks the functional
Send a sign in request to the master , sets the key information and
Return keypair object for the minion.
Generates the payload used to authenticate with the master
is used to decrypt the AES seed phrase returned from
Wraps the verify_signature method so we have
Checks if both master and minion either sign (master) and
Return the AES key received from the master after the minion has been
Verify that the master is the same one that was previously accepted .
sign it with HMAC-SHA256
verify HMAC-SHA256 signature and decrypt data with AES-CBC
encrypt a python object
un-serialize a python object
return the data
have a bug that requires tricking
Perform a gluster -- xml command and log result .
Checks for python2.6 or python2.7
Return peer status information
Add another node into the peer list.
Create a glusterfs volume
List configured volumes
Check the status of a gluster volume .
versionadded : : 2015.8.4
Start a gluster volume
Stop a gluster volume
Deletes a gluster volume
glusterfs volume .
Unset quota on glusterfs volume
List quotas of glusterfs volume
Check the version of Bower to ensure this module will work .
Create bower command line string
Install a Bower package .
using systemd 207 or later ignore `` /etc/sysctl.conf `` and only
persist a simple sysctl parameter for this minion .
read per-node pillar information .
has an xmlns attribute , then etree will add it
contains items .
including attributes .
support state config via the stateconf renderer .
deriving the current state of the container from the inspect
run a function that requires the use of a docker.Client ( )
trigger a refresh of salt mine data .
Change the state of a container
Get the method to be used in shell commands
are not parents ) .
Remove container name from HostConfig:Links values to enable comparing
bytes as human-readable file sizes
running low-level API calls
updating the data structure
updating the data structure .
Take input kwargs and return a kwargs dict to pass to docker-py 's
versionadded : : 2016.3.7,2016.11.4,2017.7.0
Returns the containers and images , if any , which depend on the given image
made to container 's filesystem since it was
Check if a given container exists
running the `` docker; Return the history for an image .
Returns information about the Docker images on the Minion.
running the `` docker inspect ``
is different from
Returns a list of tagged images
versionadded : : 2017.7.2
mapping information for a given container .
Returns information about the Docker containers on the Minion.
Returns the state of the container
Searches the registry for an image
Runs the ` docker top ` command on a specific container
Returns a dictionary of Docker version information.
Create a new container
Copy a file from inside a container to the Minion
Copy a file from the host into a container
Exports a container to a tar archive.
Removes a container
depend ) which do
belonging to the specified image , with
versionchanged : : 2015.8.4
Removes an image
versionadded : : 2015.8.3
Create a new volume
Pauses a container
Restarts a container
Stops a running container
Unpauses a container
exit gracefully , and return its exit code
Common logic for docker.run functions
run a script on a container
Run :py:func:`cmd.script <salt.modules.cmdmod.script>` within a container
Prepares a self contained tarball that has the state
Generates the chunks of lowdata from the list of modules
Executes a Salt function inside a running container
Apply the states defined by the specified SLS modules to the running
is a valid list of peers and transforms domain names into IP Addresses
Manages the configuration of NTP peers and servers on the device , as specified in the state SLS file .
Add a virtual service .
is properly configured and scaled .
Sets the time at which the password expires ( in seconds since the UNIX
Sets the time at which the account expires ( in seconds since the UNIX
Check the token , returns a 401 if the token is invalid .
Listens for a "websocket client ready" message.
is enabled , check that the origin is allowed
call , either from a virtualenv , an argument
Return the command list to use
Start the named service .
Reload the daemon's configuration files
Reload config and add/remove/update as necessary
List programs and its state
Display the raw output of status
Run any custom supervisord command
Reads the config file using configparser
Add a log to the logadm configuration
Remove a log from the logadm configuration
Add a new inbound or outbound rule to the firewall policy
existing firewall rule identified by name and optionally by ports ,
Enable all the firewall profiles (Windows only)
Send a message to a Mattermost channel .
Send an event to a Mattermost channel .
Return the value of key at path in vault , or entire secret
used must allow this .
Verify that the raid is present
runs it through the HJSON
Ensure function exists.
Ensure alias exists.
Ensure event source mapping exists.
Return a list of load balancers .
Return a list of supported protocols .
Create a new load balancer instance
Destroy a load balancer
Get the details for a load balancer by name
Add a new member to the load balancer
List the members of a load balancer
Install new alternative for defined <name>
installed alternative for defined < name > and < path >
Return the primary name associate with the load , if an empty string
Return the token and set the cache data for use
happen in the same amount of time
Returns ACL for a specific user.
modify the access list right before it 'll be applied to the request .
Read in a load and return the groups a user is a member of
requesting user is allowed to set custom expire
create a token .
Return the name associated with the token , or False if the token is
Remove the given token from token storage .
Authenticate a user by the token specified in load .
Authenticate a user by the external auth module specified in load .
Authenticate a user by the key passed in load .
specified in load .
create the authorization data sets
Determine if token auth is valid and yield the adata
given eauth is valid and yield the adata
Read in the access system to determine if the validated user has
Determine what type of authentication is being requested and pass
Execute the CLI options to fill in the extra data needed for the
Create the token from the CLI and request the correct data to
Request a token from the master
Returns a list of the requester's topics
specified by name or ARN .
returned might differ based on the
Set an attribute of a topic to a new value.
Returns a list of the subscriptions to a specific topic
Returns all of the properties of a subscription.
needs to be rebooted .
Create a dictionary with the details for the updates in the collection .
Create a dictionary with a summary of the updates in the collection .
Get the contents of `` _updates `` ( all updates ) and puts them in an
Refresh the contents of the `` _updates `` collection .
Gets a list of all updates available on the system that match the passed
Search for either a single update or a specific list of updates.
passed in the updates collection .
Used by the uninstall function .; running commands .
runs it through the YAML
Write a default to the system
Delete a default from the system
Manage the computer's description field
Manage the computer's name
is not in the; is joined to the Domain .
Reboot the computer
Shutdown the computer
Send a message to a Slack channel .
Parse BTRFS device info data.
Get BTRFS filesystem information.
known BTRFS formatted devices on the system .
Defragment only one BTRFS mountpoint.
mounted BTRFS filesystem .
List currently available BTRFS features.
disk the chunks are allocated .
Create a file system on the specified device .
Resize filesystem.
be mounted .
remove devices from the particular mounted filesystem .
Parse properties list.
be path of BTRFS device ,; given btrfs object .
Create subvolume `name` in `dest`.
Delete the subvolume(s) from the filesystem
List the recently modified files in a subvolume
Get the default subvolume of the filesystem path
Helper for the line parser.
List the subvolumes present in the filesystem .
Set the subvolume as default
Show information of a given subvolume
Create a snapshot of a source subvolume
given subvolume are completely removed from the
Add a user to the minion .
is the only required parameter .
Remove a user from the minion
Get the Security ID for the user
Add user to a group
Change the home directory of the user , pass True for persist to move files
Change the groups this user belongs to , add append=False to make the user a
does n't return the userprofile we can get it from the
Return a list of groups the named user belongs to
Return a list of all users on Windows
Get the username that salt-minion is running under .
Validates a mac address
True if the IP address ( and optional subnet ) are valid , otherwise
True if the value passed is a valid netmask , otherwise return False
Get memcached status
Retrieve value for a key
Set a key on the memcached server , overwriting the value if it exists .
Delete a key from memcache server
succeeds if the key; Replace a key on the memcached server .
Increment the value of a key
Publish a command `` from the minion out to other minions '' .
Execute a runner on the master and return the data from the runnr function
Create RackSpace Queue.
Delete an existings RackSpace Queue.
see if a Queue exists .
Show information about Queue
Return a list of startup dirs
Get a list of automatically running programs
Get pillar data from Vault for the configuration ``conf``.
Export a file or directory from an SVN repository
working directory has been changed .
Compile pillar data
is presnt .
is downloaded locally .
config commands to the NX-OS device .
commands over SSH or NX-API
Open a connection to the NX-OS switch over SSH .
parse command output for error information
Open a connection to the NX-OS switch over NX-API .
Executes an nxapi_request request over NX-API.
using the block mode
Convert an AutoYAST file to an SLS file
Return the color theme to use
Return the colors as an easy to use dict .
Creates new user group .
does not exist , eventually delete user group .
output the contents of a file:
default Markdown )
proccessed lowstate data that was not blacklisted
return a data dict in yaml string format .
format requisite as a link users can click
returns a dict of proccessed data
return the results of the GetVersionExW Windows API call .
tell you if the
Run a dscl -create command
Change the default group of the user
Change the default shell of the user
Change the home directory of the user
Change the user 's Full Name
Change the groups to which the user belongs .
Return user information in a pretty way
Return a list of groups the named user belongs to .
obfuscating the password used for AutoLogin
Return the apf location
is running otherwise return False
see if the vendor directory exists in this directory
Run PHP's composer with a specific action.
Install composer dependencies for a directory.
Update composer itself.
Sync all custom types
Sync execution modules from ``salt://_auth`` to the master
Return available Packet os images.
Return available Packet datacenter locations.
Return available Packet sizes.
Return available Packet projects.
Wait for a certain status from Packet.
Create a single Packet VM .
List devices, with all available information.
keeping only a brief listing .
Destroys a Packet device by name .
Poll imgadm and compare available images
mock a decorator that takes parameters
named command is not running .
Query NetBox API for minion data
Run the command configured
Check the current value with the passed value
worker function for the others to use
wm_preferences: sets values in the org.gnome.desktop.wm.preferences schema
desktop_lockdown: sets values in the org.gnome.desktop.lockdown schema
desktop_interface: sets values in the org.gnome.desktop.interface schema
Deserialize from TOML into Python data structure.
Serialize Python data to TOML.
Render the template_file , passing the functions and grains into the
Get the grains from the proxied device
is called by the : mod : ` salt.modules.chassis.cmd < salt.modules.chassis.cmd > `
Is the chassis responding ?
Store a key value.
Fetch a key value .
Remove the key from the cache bank with all the key content .
containing all entries stored in the specified bank .
specified color .
Displays a message on the LED matrix.
Displays a single letter on the LED matrix .
POST a payload
do a GET request to Netbox .
Get a list of items from NetBox .
Get a single item from NetBox .
Return the requested value from the aws_kms key in salt configuration .
Return the boto3 session to use for the KMS client .
Return the response dictionary from the KMS decrypt API call .
Return the configured KMS data key decrypted and encoded in urlsafe base64 .
Given a blob of ciphertext as a bytestring , try to decrypt
try to decrypt any object .
be rendered that was encrypted using AWS KMS envelope encryption .
Replaces the last 1/4 of a string with X 's
retrieve the named value from grains , if the named value is not
Determine whether a key exists in the grains dictionary.
Return all of the minion's grains
Return one or more grains
Set new grains values in the grains config file
Given a lookup string in the form of 'foo : bar : baz '' return a nested
Perform a one-time generation of a hash and write it to the local grains .
is used like setval but works; Set a key to an arbitrary value .
Used to make sure the minion 's grain key/value matches .
use the local UNIX socket .; Get a session to XenAPI .
returns xl or xm command line path
returns label 's uuid
returns a full record for uuid
returns metrics record for a rectype
get value from record
Return a list of virtual machine names on the minion
detailed information about the vms .
Return list of all the vms and their state.
Return a dict with information about this node
Return info about the network interfaces of a named vm
Return a list off MAC addresses from the named vm
Return the disks of a named vm
allocated to VM .
Set which CPUs a VCPU can use.
Send a soft shutdown signal to the named vm
Pause the named vm
Resume the named vm
Reboot a domain via ACPI request
Reset a VM by emulating the reset button on a physical machine
Migrates the virtual machine to another hypervisor
is equivalent to pulling the
is a hypervisor of any kind
used by the vms on this hyper in a
combined network counters used by the vms on this hyper in a
sanitize message output
parsing configuration values into python values
converting pythonic values to configuration file values
debugging cfg files
Create an in-memory configuration for the specified zone.
Create an in-memory configuration from a template for the specified zone.
specified configuration from memory and stable storage .
Export the configuration from memory to stable storage.
Import the configuration to memory from stable storage.
internal handler for set and clear_property
internal resource hanlder
Add a resource
Remove a resource
Display the configuration from memory
display a nag-messsage to the log if vulnerable hash-type is used .
Log environment failure for the daemon and exit with the error code.
Run the preparation sequence required to start a salt master server .
Start the actual master .
run any shutdown operations on this method .
Run the preparation sequence required to start a salt minion .
Start the actual minion .
Start the actual minion as a caller minion .
Run the preparation sequence required to start a salt proxy minion .
Start the actual proxy minion .
Run the preparation sequence required to start a salt syndic minion .
Start the actual syndic .
Execute rac commands
Configure the nameservers on the DRAC
remote logging , by default syslog will automatically be
List all DRAC users
Delete a user
Change users password
Create user accounts
Configure users permissions
perform a one off PXE boot
Get the modified time of the RPM Database .
Get the checksum of the RPM Database .
Hook after the package installation transaction.
is essentially a whitespace-delimited; Parse the master.cf file .
Set a single config value in the master.cf file .
given values into the style of line normally used in the
is not just a `` name = value '' file ;
does not already; Set a single config value in the main.cf file .
Write out configuration file .
Show contents of the mail queue
Delete message(s) from the mail queue
Get collection options
Change collection options
managed keystore .
existing keystore or creates a new one if necesssary .
Removes a certificate from an existing keystore .
Store information in a file.
Fetch information from a file.
Return the epoch of the mtime for this cache file
Check that the key is found in the registry .; refers to keys and not
Check that the value/data pair is found in the registry .
Refresh the windows environment .
Enumerates the subkeys in a registry key or hive .
Enumerates the values in a registry key or hive .
is passed , it will be the value
Cast the `` vdata ` value to the appropriate data type for the registry type
versionadded : : 2015.5.4
Delete a registry value entry or the default value for a key.
Return the properly formatted ssh value for the authorized encryption key
format user input .
Defined in man sshd_config ( 5 ); Expand the AuthorizedKeysFile expression .
Get absolute path to a user's ssh_config.
Replace an existing key
Return a dict containing validated keys in the passed file
Return a public key fingerprint based on its base64-encoded representation
Return the minion 's host keys
Return the authorized keys for users
Check a keyfile from a source destination against the local keys and
see if a key needs updating , returns `` update '' , `` add '' or `` exists ''
Remove an authorized key from the specified user 's authorized key file ,
Remove an authorized key from the specified user 's authorized key file
Add a key to the authorized_keys file , using a file as the source .
Add a key to the authorized_keys file .
parses ssh-keygen -F function output and yield line
parses ssh-keygen -F and ssh-keyscan function output
Check the record in known_hosts file , either by its value or by fingerprint
Remove all keys belonging to hostname from a known_hosts file .
Download SSH public key from remote host "hostname", optionally validate
Look up the value for an option .
Retrieve the region for a particular AWS service based on configured region and/or profile .
Retrieve full set of values from a boto3 API call that may truncate
Normalize the directory to make comparison possible
Returns a list of items in the SYSTEM path
Check if the directory is configured in the SYSTEM path
Add the directory to the SYSTEM path in the index location.
Display the output as table .
Build the unicode string to be displayed .
is longer then the width , will split by space and continue on the next line .
rows content to be displayed .
row content and displays .
shared function for * _absent
ensure filesystem is absent on the system
ensure snapshot is absent on the system
ensure hold is absent on the system
internal handler for filesystem_present/volume_present
ensure filesystem exists and has properties set
ensure volume exists and has properties set
ensure bookmark exists
ensure snapshot exists and has properties set
ensure a dataset is not a clone
dict with current snapshots
dict with info for a new snapshot
maintain a set of snapshots based on a schedule
return a state error dictionary , with 'sid ' as a field if it could be returned
Gets the DACL of a path
filter by user if one is provided .; Get the ACL of an object .
convert an ace to a textual representation
set the inheritance
enable/disable inheritance on an object
Disable inheritance on an object
Check a specified path to verify if inheritance is enabled
verify the ACE ( access control entry ) specified exists
returns the bit value of the string object type
returns the necessary string value for an HKEY for the win32security module
returns a permission bit of the string permission value for the specified object type
returns the permission textual representation of a specified permission bit/object type
returns the acetype bit of a text value
returns the textual representation of a acetype bit
returns the propagation bit of a text value
processes a path/object type combo and returns :
Ensure a subnet exists and is up-to-date
does not exists
Determine what the most resource free host is based on the given data
Scan the query data for the named VM
called without options all hosts
List the virtual machines on each host , this is a simplified query ,
connected to this master
is used to create a new virtual machine .
Return the information on the named VM
restart an existing VM
Destroy the named VM
Migrate a VM from one host to another.
Install a certificate
Uninstall a certificate from a keychain
List all of the installed certificates
Get the friendly name of the given certificate
Get the default keychain
Set the default keychain
Returns the hash of a certificate in the keychain .
Ensure the SQS queue exists.
Remove the named SQS queue if it exists .
>>> _parse_op('>')
stopped building wheels for python3.4 '' )
>>> _check_ver('2.7.15', 'gt', '2.7')
port uninstall `` .
passed package ( s ) with `` port install ``
Update ports with ``port selfupdate``
Set a value in a db , using a uri in the form of `` sdb : // < profile > / < key > `` .
Perform a one-time generation of a hash and write it to sdb .
exists as a temporary rule based
Returns true a rule for the ip already exists
Execute csf command
Returns the cmd args for csf basic allow/deny commands .
Handles the cmd execution for allow and deny commands .
Extract comma-separated values from a csf.conf
Handles the cmd execution for tempdeny and tempallow commands .
Builds the cmd args for temporary access/deny opts.
Add an rule to the temporary ip allow list.
Add a rule to the temporary ip deny list .
csf allowed hosts
csf denied hosts
replace the incoming or outgoing ports
based on direction and protocol .
append to the
run a state with the given chunk via salt-ssh
Set the return code based on the data back from the state system
Check the pillar for errors , refuse to run the state if there are errors
started state jobs to finish running
Takes a list of filerefs and returns a merged list
Set "slsmod" keys to None to make
Create the seed file for a state.sls run
Return a list of strings that contain state return data if a state function
Return a list of dicts of prior calls to state functions .
queue the state run if requested
Execute a single low data call
Retrieve the highstate data from the salt master and display it
be applied to this minion
Call a single ID from the named module ( s ) and handle all requisites
Display the state data from a specific sls or list of sls files on the
Return the top data that the minion will use for a highstate
Initialize Datadog connection
Schedule downtime for a scope of monitors.
Cancel a downtime by id or by scope .
Post an event to the Datadog stream.
Get the xbps version
Update list of available packages from installed repos
Install the passed package
repos known by XBPS
Find what file a repo is called in .
Add an XBPS repository to the system.
Remove an XBPS repository from the system .
configured on the network device .
Returns a list of the configured NTP servers on the device.
Returns a dictionary containing synchronization details of the NTP peers.
Configures a list of NTP peers on the device .
configured on the device .
Process the return from Salt
Return events to Elasticsearch
is used to regenerate all keys in an environment .
versionchanged : : 2019.2.0
Check the version of active minions
Bootstrap minions with salt-bootstrap
Bootstrap Windows minions via PsExec.
Return a dictionary of all available services on the system
Return the service info for a service by label, filename or path
Return all installed services
Start the specified service
Restart the named service
Retrieve the API params from the config file .
Build the API URL .
Make the HTTP request and return the body as python object .
Send out the email using the details from the `` message `` argument .
Send a command to the modjk loadbalancer
Check if the worker is in ` activation ` state in the targeted load balancers
Wrapper function for the stop/disable/activate functions
look up all grains in the metadata server
named value exists in the grains dictionary .
Ensure VPC exists.
given settings exist .
Ensure a subnet exists.
verify a subnet 's route table association
Ensure an internet gateway exists.
named internet gateway is absent .
is associated to a VPC .
named route table is absent .
Ensure a nat gateway exists within the specified subnet
pending requested peering connection between two VPCs .
given disk .
Create a virtual server
Create a pool on the F5 load balancer
Add a node to a pool
see if a pool exists
Check a pool member exists in a specific pool
Connect to F5
see if a virtual server exists
List all the load balancer methods
List all available locations
Return a dict of all available VM sizes on the cloud provider with
Return a dict of all available VM images on the cloud provider .
Return a dict of all custom VM images on the cloud provider .
Check that given ipset set exists .
specified set .
Check that an entry exists in the specified set .
Flush entries in the specified set,
Return list of members for a set
Return information about the set
decode the return data
Update the local CA bundle file from a URL
Create the named vm
is present with the specified properties .
List all available profiles
List profiles for user
Add profile to user
List all available roles
List roles for user
Add role to user
List all available authorization
List authorization for user
Add authorization to user
Manage the configuration of a specific policy term .
Get current system keyboard setting
Set current system keyboard setting
Get current X keyboard setting
Generate a secure password .
Generate /etc/shadow hash
Jenkins One-At-A-Time Hash Function
Splay a salt function call execution time across minions over
named index is absent .
Return full path to service command
Return full path to service rc script
Return rcvar
are set to run on boot
are available but not enabled to start at boot
Switch on/off service start at boot.
see if given python is installed .
Verify that python is installed , install if unavailable
Verify that the specified python is installed with pyenv .
Verify that python is uninstalled
Verify that the specified python is not installed with pyenv .
require pyenv be installed
Send a message via SMTP
debconf questions for all packages in the following format : :
debconf questions for a package .
debconf questions from a template .
debconf questions from a file .
Call both with prep_jid on all returners in multi_returner
Write return to all returners in multi_returner
Write load to all returners in multi_returner
Merge the load data from all returners
Return all job data from all returners
Clean out the old jobs from all returners ( if you have it )
Ensure domain exists and is up-to-date
Runs the compound target check
given id is present and is configured
given id is present and restart if set .
given continuous query is present .
Set the timezone for the system.
Set up nova credentials
Boot (create) a new instance
List storage volumes
Create a block storage volume
Destroy the volume
Attach a block storage volume
Suspend an instance
Resume an instance
Lock an instance
Delete an instance
Return a list of available flavors ( nova flavor-list )
Add a flavor to nova ( nova flavor-create ) .
nova by id ( nova flavor-delete )
Return a list of project IDs assigned to flavor ID
Add a project to the flavor access list
Remove a project from the flavor access list
Add a keypair to nova ( nova keypair-add )
Add a keypair to nova ( nova keypair-delete )
Return a list of available images ( nova images-list + nova image-show )
Sets a key=value pair in the metadata for an image (nova image-meta set)
Delete a key=value pair from the metadata for an image
Return detailed information for an active server
Add a secgroup to nova ( nova secgroup-create )
nova ( nova secgroup-delete )
Return information about a server
returns a Root object with the proper models added .
Returns the difference between two configuration entities structured
Parse configuration from the device.
Return the native config .
load the config on the device using the OpenConfig or IETF
Return the compliance report using YANG objects .
exists at a given path
Manage RBAC properties for user
matches the given pillar target .
matches the given data target
matches the given pcre target
Return the first match in a dictionary of target patterns
Search a dictionary of target strings for matching targets
Execute varnishadm command
Return server version from varnishd -V
Show params of varnish cache
make the candidate configuration with; dirtyID tags from the candidate config result .
captures the query string and sends it to the Palo Alto device .
Because different versions of Palo Alto support different command sets, this function
append `` comment `` to `` ret [ 'comment ' ] ``
Ensure that the specified kernel module is loaded
Verify that the named kernel module is not loaded
Invoke a function in the lxc module with no args
Returns the host for a container.
Return a dict of hosts and named guests
Initialize a new container
using lxc.init in saltcloud compatibility mode
Return a generator iterating over hosts
defined containers ( running , stopped , and frozen ) for the named
Purge the named container and delete its minion key if present .
Start the named container .
Return the location of the SELinux VFS directory
Return the mode selinux is running in
Return the selinux mode from the config file
Set the SELinux enforcing mode
Set the value for a boolean
Set the value of multiple booleans
Return a structure listing all of the selinux booleans on the system and
Enable or disable an SELinux module.
Install custom SELinux module from file
Return a structure listing all of the selinux modules on the system and
Establish a connection to etcd
Return data to an etcd server or cluster
Ensure a Linux ACL is present
Ensure a Linux ACL list is present
retrieve objtype from pillars if objname
set the Roles to the identity pool
Ensure Cognito Identity Pool exists.
passed pcre regex matches
Return the running jobs on this minion
Write job information to cache
are an insane number of processes being created
Return a boolean stating whether or not a file 's trailing newline should be
Construct pillar from file tree.
given `` root_dir `` specific to Nodegroup names
Compile pillar data for a single root_dir for the specified minion ID
args as a list of strings
Return the set of GUIDs found in guid_string
Execute a prlsrvctl command
List information about the VMs
Clone a VM
Delete a VM
Query whether a VM exists
Start a VM
Stop a VM
Restart a VM by gracefully shutting it down and then restarting
Reset a VM by performing a hard shutdown and then a restart
Status of a VM
Run a command on a VM
convert a snapshot ID to a snapshot name .
convert a snapshot name to a snapshot ID .
Validate snapshot name and convert to snapshot ID
List the snapshots
Create a snapshot
Delete a snapshot
Revert a VM to a snapshot
Executes the given rpc .; returned data can be stored in a file
Changes the hostname of the device.
Commits the changes loaded into the candidate configuration .
Rollbacks the committed changes .
Gets the difference between the candidate and the current configuration .
Executes the CLI commands and reuturns the text output .
Shuts down the device.
commits the configuration provided .
Copies the file from the local device to the junos device.
Loads the configuration provided onto the junos device .
Create a file system on the specified device
attributes for the specified device ( using tune2fs )
Return all contents of dumpe2fs for a specified device
> = 2.1.9 are required
Lookup a hostname and determine its address family .
bind to the sockets to verify that they are available
Verify that the named files exist and are owned by the named user
Verify that the named directories are in place and that the environment
Check user and assign process uid/gid.
leading up to , and including , a path .
verify that the current
Check the number of max allowed open files and adjust if needed
needs to be under and verifies that the path is
passed id is valid
Check a string to see if it has any potentially unsafe routines which
logging configuration is found , show a warning
List all available package upgrades on this system
run a `` pacman -Sy `` , return a dict : :
do identical things but with different pacman commands ,
name globs )
Run the api
Set self.focus for kwarg queries
is to store the sql field list
takes a list of database results and iterates over ,
Execute queries, merge and return as a dict.
enslaved interfaces ( GNU/Linux - brctl )
creates the bridge
deletes the bridge
removes an interface from a bridge
Internal, sets STP state
Internal, returns bridges and member interfaces (BSD-like: ifconfig)
enslaved interfaces ( NetBSD - brconfig )
adds an interface to a bridge
is required to specify the
Internal, dispatches functions by operating system
Returns the machine 's bridges list
Returns the bridge to which the interfaces are bond to
Spanning Tree Protocol state for a bridge
Checks for specific types in the state output.
are allowed ,
List all of the extended attributes on the given file/directory
Read the given attributes on the given file/directory
Removes the given attribute from the file
Causes the all attributes on the file/directory to be removed
Get an auth token
Make a web call to RallyDev .
Requires a valid query string .; Query a type of record for one or more items .
Show an item
Update an item.
is an instance of `` binary_type `` , return
is an instance of `` text_type `` , return
is an instance of `` text_type `` , return `` s `` , otherwise
Check if data is hexadecimal packed
Combine the host header , IP address , and TCP port into bindingInformation
List details of available certificates in the LocalMachine certificate
Execute a powershell command from the WebAdministration PS module .
matching the match dictionary .
set functions .
deployed websites .
Create a basic website in IIS .
Modify a basic website in IIS .
Stop a Web Site in IIS .
Start a Web Site in IIS .
Get all configured IIS bindings for the specified site .
Create an IIS Web Binding.
binding `` to target the
List certificate bindings for an IIS site.
Assign a certificate to an IIS Web Binding .
Remove a certificate from an IIS Web Binding .
configured IIS application pools .
Stop an IIS application pool.
Start an IIS application pool.
Restart an IIS application pool.
Get the value of the setting for the IIS container .
Get all configured IIS applications for the specified site .
Get all configured IIS virtual directories for the specified site , or for
Remove an IIS Configuration backup from the System .
processes that correspond to the passed
be an IP address and the master running; Create a DNS record .
Create both A and PTR (reverse) records for a host.
Delete both forward (A) and reverse (PTR) records for a host only if the
Given a list of comments , or a comment submitted as a string , return a
Remove the trailing slash from the URI in a repo definition
Authenticate with vCenter server and return service instance object.
adding new IDE controllers
Check if the IP address is valid and routable
Check if the IPv6 address is valid and routable
Check if the salt master has a valid and
Convert a string representation of a HostHostBusAdapter into an
Show the vCenter Server version with build number .
List all the data centers for this VMware environment
distributed virtual portgroups for this VMware environment
List all the clusters for this VMware environment
List all the datastore clusters for this VMware environment
List all the datastores for this VMware environment
List all the datastores for this VMware environment, with extra information
Returns a dictionary with basic information for the given datastore
List all the hosts for this VMware environment
pools for this VMware environment
List all the standard networks for this VMware environment
Return a list of all VMs and templates that are on the specified provider , with no details
Return a list of all VMs and templates that are on the specified provider , with basic fields
Return a list of all VMs and templates that are on the specified provider , with fields
List all available details of the specified VM
Return a list of all the templates present in this VMware environment with basic
List all the folders for this VMware environment
List snapshots either for all VMs and templates or for a specific VM/template
suspend a VM using its name
reset a VM using its name
do an immediate power off of a VM using its name .
destroy a VM from the VMware environment
create a single VM in the VMware environment .
cloning from shapshots
clonespec only if values are valid
Returns the clone spec
Create a new data center in this VMware environment
Create a new cluster under the specified datacenter in this VMware environment
rescan a specified HBA or all the HBAs on the Host System
upgrade VMware Tools on all virtual machines present in
upgrade VMware Tools on a specified virtual machine .
List hosts for each cluster; or hosts for a specified cluster in
List clusters for each datacenter; or clusters for a specified datacenter in
List all HBAs for each host system; or all HBAs for a specified host
distributed virtual switches for this VMware environment
List all the vApps for this VMware environment
put the specified host system in maintenance mode in this VMware environment
Create the specified folder path in this VMware environment
Create a snapshot of the specified virtual machine in this VMware
's current snapshot .
Remove a snapshot of the specified virtual machine in this VMware environment
Remove all the snapshots present for the specified virtual machine .
Convert the specified virtual machine to template.
Add a host system to the specified cluster or datacenter in this VMware environment
Remove the specified host system from this VMware environment
Connect the specified host system in this VMware environment
Create a new datastore cluster for the specified datacenter in this VMware environment
specified host system in this VMware environment
Creates a new usermacro .
Return the DIFFERENCE of the result sets returned by each matching minion
returns a dictionary of minion pools along with
does n't exist , then; given location .
Performs the same task as saltutil.sync_all module
object method function to construct and execute on the API URL .
Show current usages statistics
Generate random integers
return its index .
Get the index of a given entry , raising an IndexError if it 's not
Remove an element .; Do not raise an exception if absent .
meaning it will not be upgraded .
is the daemon 's name and
named user is present with the specified properties .
replacing one or more sections .
Get value of a key from a section in an ini file.
Returns the value of; Remove a key/value pair from a section in an ini file .
Retrieve a section from an ini file .; Returns the section as dictionary .
Remove a section in an ini file .; Returns the removed section as dictionary ,
return it as dictionary .
Render the python module 's components
Ensure the RabbitMQ VHost exists.
= { }
Execute svn
Display the Subversion information from the checkout .
working copy of the remote Subversion repository
Update the current directory , files , or directories from
Create an unversioned copy of a tree.
Given bytes , bytearray , str , or unicode ( python 2 ) , return bytes ( str for
Given str , bytes , bytearray , or unicode ( py2 ) , return str
Convert the string name of a boolean to that boolean value.
Return a single or double quote , if a string is wrapped in extra quotes .
is binary or text
Given a human-readable byte string ( e.g .
Create a regular expression at runtime which should match ignoring the
passed value matches the specified expression .
Check for glob or regexp patterns for include_pat and exclude_pat in the
suppresses tracebacks on broken pipes ( i.e .
debugging context around a line in a given string
containing lines from two files , and return
Converts camelCase (or CamelCase) to snake_case.
camelCase ( or CamelCase if uppercamel is `` True `` ) .
Queries the specified network device for rx/tx ring parameter information
Queries the specified network device for coalescing information
Queries the specified network device for associated driver information
ring parameters of the specified network device
specified network device
Queries the specified network device for the state of protocol offload and other features
Return a Unix timestamp as a string of digits
Return the base path for certs from CLI or from options
Return a serial number in hex using os.urandom ( ) and a Unix timestamp
write out to the index.txt
write out the index.txt database file in the appropriate directory to
Check that the X509 version is correct
Verify whether a Certificate Authority (CA) already exists
Get the certificate path or content
Returns a datetime.datetime object
Create a Certificate Authority ( CA )
Fetch X509 and CSR extension definitions from tls:extensions:
Create a Certificate Signing Request ( CSR ) for a
Create a Self-Signed Certificate ( CERT )
Create a Certificate ( CERT ) signed by a named Certificate Authority ( CA )
Create a PKCS # 12 browser certificate for a particular Certificate ( CN )
Return information for a particular certificate
Create an empty Certificate Revocation List.
Revoke a certificate .
convert a seco.range range into a list target
Filter minions by a generic filter.
match via pcre
match via list
match the special list-only groups defined by
are returned by a range query
submit the XML to create a node
Make a web call to a Parallels provider
Show the details from Parallels concerning an image
Show the details from Parallels concerning an instance
Send a Telegram message with the data .
Walk though the jid dir and look for jobs
Return a job id and prepare the job id directory .
Return data to the local job cache
Save/update the serialized list of minions for a given job
Return a dict mapping all job ids to job information
Return a list of all jobs information filtered by the given criteria .
Clean out the old jobs from the job cache
Update (or store) the end time for a given job
Retrieve the stored endtime for a given job
Save the register to msgpack files
Load the register from msgpack files
Build the SLSMap
Build the SLSString .
be updated after
Handles global secondary index for the table present state.
be created , and to be deleted .
ret iff there was a failure or in test mode .
is throttling us , as all the pipelines are started at the same time .
based on sls files in the winrepo_dir
containing Windows Software Package Definitions
Sync custom modules into the extension_modules directory
returning the resulting list .
Invoke a state run on a given target
Execute a single module function on a remote minion via salt or salt-ssh
Watch Salt's event bus and block until a condition is met
Execute a runner module on the master
Executes multiple runner modules on the master in parallel.
Execute a wheel module on the master
List all Slack rooms.
incoming webhook .
Change an attribute for a named user
Add a user in the group .
Remove a user from the group .
use only /etc/group
plugin if it 's in the directory with salt command specified in run_type
get the result of run_type
Run one nagios plugin and return retcode of the execution
get the result of cmd.retcode
List all the nagios plugins
are up to date .
return the result
Query the PagerDuty API
belonging to an API call .; Used for list_services ( ) and
Install pyenv systemwide
Updates the current versions of pyenv and python-Build
Install a python implementation .
Uninstall a python implementation .
List the installed versions of python .
sets the currently defined default python .
Execute a python command with pyenv 's shims from the user or the system .
Execute a python command with pyenv 's shims using a specific python version .
Ensure RDS instance exists.
Ensure RDS replica exists.
Ensure DB subnet group exists.
exists and update parameters .
Convert an interval string like 1w3d6h into the number of seconds, time
satisfy criteria specified in
belonging to this account
Designed for use in states .
Store a certificate to the given store
Execute a function through the master network interface .
Execute a function
allows us to turn off storing jobs for different classes
Execute a function from low data
Return a dictionary of functions and the inline documentation for each
Execute the function in a multiprocess and return the event tag to use
Print all of the events with the prefix 'tag'
Generate a JSON file to serve as an index for short-URL lookups
True if successful; Executes the passed command .
Executes the passed command .
validate the enabled parameter .
wait `` seconds for a system parameter to be changed before
Run a launchctl command and raise an error if it fails
is a helper function for getting the available macOS services .
Gets the UID or Username of the current console user .
manages software package repositories .
deletes the specified repo on the system , if it exists .
Read pillar data from Foreman via its API.
Creates a DynamoDB table .
see if a table exists .
Delete a DynamoDB table.
Update a DynamoDB table .
Creates a single global secondary index on a DynamoDB table .
Updates the throughput of the given global secondary indexes .
Describe a DynamoDB table .
given a valid index
Install a Perl module from CPAN
remove a Perl module that was installed from CPAN .
installed Perl modules , and the version installed
Show information about a specific Perl module
Return a dict of CPAN configuration values
Get a cursor and run a query .; Reconnect up to ` retries ` times if
Create table if needed
Initialize connection and create table if needed
containing all entries stored in the specified
Create a new database and opens it .
Purge the database .
Flush table.
existing tables and their descriptions .
Open database from the path with the name or latest.
Create a table from the object .
Store an object in the table.
Update object(s) in the database.
Delete object from the database.
True if object is aligned to the criteria .
Get objects from the table.
Send an event with the given tag and data .
versionadded : : 2015.8.2
cloud cache data for target .
Fetch data from a salt.cache bank.
Return an postgres cursor
Add an item or items to a queue
Daemonize a process
Daemonize a module function process if multiprocessing is True and the
Notify systemd that this process has started
Save the pidfile
Return the pid from a pidfile as an integer
cleaning up multiprocessing procs
determine if a process is running
Create a processes and args + kwargs
assuming this one is dead ) , then remove the old one
start all available api modules
Check the children once
Kill all of the children
Make a web call to VictorOps
Ensure the telemetry alert exists.
Ensure the telemetry alert config is deleted
List the installed packages .
install a package from the given path
Install a .pkg from an URI or an absolute path .
Return a list of the currently installed app ids .
Update the specified app with the given configuration .
Remove the specified app from the server .
Return configuration and status information about the marathon instance.
Restart the current server configuration for the specified app .
Return a list of the configured DNS servers of the specified interface
Remove the DNS server from the network interface
Add the DNS server to the network interface
Get the type of DNS configuration ( dhcp / static )
Bootstraps a frontend distribution.
named export is present with the given options
named path is not exported
Check if installed packages are the latest versions
Unlock a FS file/dir based lock
is there , check to see if the file is actually being
see if there is a new enough file list cache , and
cached env names , if present .; returns None .
Generate a dict of filename - > mtime
return a boolean; Is there a change to the mtime map ?
unused cache items assuming the cache directory follows a directory
file_ignore_regex or file_ignore_glob were given in config ,
allow non-fileserver functions to clear update locks
Return the backend list
backends that support the
remote `` can either be a dictionary containing repo configuration
Clear the update lock for the enabled fileserver backends
support the update
Return the update intervals for all of the enabled fileserver backends
Return the environments for the named backend or all backends
Return environments for all backends for requests from fileclient
Initialize the backend , only do so if the fs supports an init function
made using the RemoteClient
made using the LocalClient
return the fnd structure , this structure is passed
Serve up a chunk of a file
hashing and stating files
Deletes the file_lists cache files
Return a list of files from the dominant environment
Emulate the channel send method , the tries and timeout are not used
runs it through the
based on netmask , sitting in the `` glob '' spot because
Evaluate a jsonnet input string .
Parses fmdump output
fmdump verbose output
Parsbb fmdump/fmadm output
fmadm.repqired , fmadm.replaced , fmadm.flush
Parse fmadm faulty output
Display fault management logs
Display log details
Display fault manager configuration
specified fault manager module
Reset module or sub-component
Display list of faulty resources
supplied is valid in a manner similar to the
are supplied .
Ask the kernel to update its local partition data .
given < device >
Check if partition satisfies the alignment constraint of part_type .
Copies the file system on the partition <from-minor> to partition
are : :; Sets the system ID for the partition .
List the system types that are supported by the installed version of sfdisk
destroying all data
Create a new disklabel ( partition table ) of label_type .
Make a part_type partition for filesystem fs_type , beginning at start and
Make a < part_type > partition with a new filesystem of < fs_type > , beginning
works only on Mac , PC98 , and
Rescue a lost partition that was located somewhere between start and end .
Resizes the partition with number < minor > .
Removes the partition with number < minor > .
Changes a flag on the partition with number <minor>.
are the same as
selected device .
see if the partition exists
Return the full path for the packages and repository freezer
is already a frozen state .
Return the list of frozen states .
Save the list of package and repos in a freeze file .
contains the packages and repos from a
Returns the rabbitmq-plugin command path if we 're running an OS that
Looks for rabbitmqctl warning, or general formatting, strings that aren't
Convert rabbitmqctl output to a dict of data
assuming whitespace-delimited output ) .
Return a list of users based off of rabbitmqctl user_list .
Return a list of vhost based on rabbitmqctl list_vhosts .
exists based on rabbitmqctl list_users .
based on rabbitmqctl list_vhosts .
Add a rabbitMQ user via rabbitmqctl user_add < user > < password >
Deletes a user via rabbitmqctl delete_user .
Changes a user's password.
Adds a vhost via rabbitmqctl add_vhost .
Sets permissions for vhost via rabbitmqctl set_permissions
Lists permissions for vhost via rabbitmqctl list_permissions
List permissions for a user via rabbitmqctl list_user_permissions
Add user tags via rabbitmqctl set_user_tags
Join a rabbit cluster
queue details of the / virtual host
Return a dictionary of policies nested by vhost and name
Set a policy based on rabbitmqctl set_policy .
based on rabbitmqctl clear_policy .
exists based on rabbitmqctl list_policies .
enabled and disabled ) .
Enable a RabbitMQ plugin via the rabbitmq-plugins command .
Return the list of packages based on the mirror provided .
Check if the package is valid on the given mirrors .
Retrieve the correct setup.exe .
Run the cygcheck executable .
Install one or several packages .
Uninstall one or several packages.
Update all packages.
Extract the preferred IP address from the ipv4 grain
Execute the render system against a single reaction file and return
Take in the tag from an event and return a list of the reactors to
Return a list of the reactors
Add a reactor
rewriting the 'state ' key in the low
Render a list of reactor files and returns a reaction struct
Enter into the server loop
Populate the client cache with an instance of the specified type
Execute a reaction by invoking the proper wrapper func
executing : ref : ` runner modules < all-salt.runners > `
running : ref : ` execution modules < all-salt.modules > `
execute remote exec functions locally on the Minion
token by name
Execute a runner asynchronous :
Deserialize any string or stream like object into a Python data structure .
formatted string or file .
Cribbed from python3 's ConfigParser.read_dict function .
exists in S3 .
be deleted , so this function
Find respective roster file.
Return a list of loaded roster backends
Return a dict of { 'id ' : { 'ipv4 ' : < ipaddr > } } data sets to be used as
Return a dict without any of the __pub * keys ( or any other keys starting
Raise a SaltInvocationError if invalid_kwargs is non-empty
Return a single arg structure for the publisher to safely use
Parse out the args and kwargs from a list of input values .
yaml.safe_load the arg
supports callable classes
is a string
Return a dict containing the arguments and default arguments to the
is returned from the loader and return the
Take an input value and split it into a list , returning the resulting list
Build the required arguments and keyword arguments required for the passed
Parse a python-like function call syntax .
used for the init of the class and the kwargs used to
Return name, version and if rpm package for specified target
List the filesets/rpm packages currently installed as a dict :
named fileset ( s ) /rpm package ( s ) .
specified fileset ( s ) /rpm package ( s ) .
Clear out all of the data in the minion datastore, this function is
Return all of the data in the minion datastore
passed data structure
Update a key with a value in the minion datastore
set a value in the minion datastore
Pop (return & delete) a value from the minion datastore
Get a ( list of ) value ( s ) from the minion datastore
returning only keys that exist in the first dict and are
query Keystone for more information about an entity
Sanatize the the arguments for use with shade
Return an operator_cloud
Return an openstack_cloud
Create a group
Delete a group
Update a group
List groups
Search for groups
Get a single group
Create a project
Delete a project
Update a project
List projects
Search projects
Get a single project
Create a domain
Delete a domain
Update a domain
List domains
Search domains
Get a single domain
Create a role
Delete a role
Update a role
List roles
Search roles
Get a single role
Create a user
Update a user
List users
Get a single user
Create an endpoint
Delete an endpoint
Update an endpoint
List endpoints
Search endpoints
Get a single endpoint
Create a service
Delete a service
Update a service
List services
Search services
Get a single service
List role assignments
Grant a role in a project/domain to a user/group
create host file if it
Return the hosts found in the hosts file in as an OrderedDict
Return the ip associated with the named host
Return true if the alias is set
file for the given ip , this will overwrite
Remove a host entry from the hosts file
Add a host to an existing entry , if the entry is not in place then create
Open the connection to the Arista switch over the eAPI .
Calls an arbitrary pyeapi method.
Return the valid shells on this system
convert all login shells
Fetch network selfLink from network name.
Get instance details
Create a route to send traffic destined to the Internet through your
Takes a dictionary , max_val_size and replace_with
Takes a dictionary and iterates over its keys , looking for
return an LDAP connection object
Given a specific platform ( under the Capirca conventions ) ,
Build a map of services based on the IANA assignment list :
return the port value using the
Return the object list .
Cleanup the term opts:
given its main key .
Retrieve the pillar data from the right environment .
Return a list of non-empty dictionaries .
Merge lists of dictionaries.
given the term options .
given the filters config .
Refactor revision tag comments.
Return the configuration of a single policy term .
Return the configuration of a policy filter .
Return the configuration of the whole policy .
Execute the salt call !
generated using ` highstate_doc.render ` .
are passed , the current
Get current Windows Update settings.
see if Microsoft Update is Enabled
Get metadata about an S3 object.
Upload a local file as an S3 object.
adding profiling to a nested function in Salt
Return the django admin
Run arbitrary django management command
Run syncdb
Run migrate
Create a super user for the database .
Load fixture data
Collect static files from each of your applications into a single location
Grant the requested privilege ( s ) on the specified object to a role
List configured exports
Remove an export
Add an export
Write an exports file to disk
Trigger a reload of the exports file to apply changes
is absent from the Minion .
be passed in as part of the; Render a Genshi template .
Display ret data
iterate down through data structures to determine output
Write the mount cache file.
Returns the diff of two text blobs .
Return the config as text from a config tree .
structured Python dictionary .
Return a clean version of the config , without any special signs ( such as
Return the merge tree of the `` initial_config `` with the `` merge_config `` ,
Return the merge result of the `` initial_config `` with the `` merge_config `` ,
Return the merge diff , as text , after merging the merge config into the
Return the diff , as Python dictionary , between the candidate and the running
Return the diff , as text , between the candidate and the running config .
Show an item via pcs command
Create an item via pcs command
Authorize nodes to the cluster
Check if nodes are already authorized
Setup pacemaker cluster via pcs command
Add a node to the pacemaker cluster via pcs command
Create a CIB-file from the current CIB of the cluster
Show the value of a cluster property
Set the value of a cluster property
Show the value of a cluster stonith
Create a stonith resource via pcs command
Show a resource via pcs command
Create a resource via pcs command
clean it up , prepare for display .
Print message with an indent.
Hint message.
works the way that message parameter is a template ,
ensure that the named container exists .
Starting with systemd 219 , new; Return the container root directory .
Make the container root directory
Bootstrap an Arch Linux container
Bootstrap a Debian Linux container
Bootstrap a Fedora container
Raises an exception if the systemd version is not greater than the
Helper function to run machinectl
Common logic for nspawn.run functions
Returns the PID of a container
is None the os the
Lists all nspawn containers
running nspawn containers
named container exists
running or stopped )
info about a container
be launched at boot
is a compatibility function which provides the logic for
sending a SIGINT to its init process .
Remove the named container
Common logic for machinectl pull-* commands
Execute a `` machinectl pull-raw `` to download a .qcow2 or raw disk image ,
Execute a `` machinectl pull-raw `` to download a .tar container image ,
Prepares the ACL returned from the AWS API for comparison with a given one .
be specified using macro-style names that get expanded to
accepts a non-ARN role name , but always returns an ARN
Ensure bucket exists.
Looks to see if pkgin is present on the system , return full path
Get the pkgin version
using pkgin ^package $
get latest pkg_summary
versionchanged : 2016.3.0
List all available package upgrades.
Run pkg upgrade, if pkgin used.
List the files that belong to a package .
Verify that a device is mounted
Activates a swap device
Convert a device name , UUID or LABEL to a device name , UUID or
sure that a fstab mount point is pressent .
sure that a fstab mount point is absent .
Determine the value of the SNMP sysContact , sysLocation , and sysServices
Get the current accepted SNMP community names and their permissions .
giving precedence to last in .
setting values that has been improperly converted to a dict back to a string .
is set for the specified setting .
Manage the active log format for the SMTP server .
Manage IP list for SMTP connections.
Manage IP list for SMTP relay connections.
run commands from __proxy__
assist with paginated responses from Digicert 's REST API .
knows about .
made to CertCentral .
Retrieve a certificate by order_id or certificate_id and write it to stdout or a filename .
has been created inside Digicert 's CertCentral .; Order a certificate .
is passed in , the; return a private_key .
Return the details for an organization
Maintain a relationship between a minion and a dns name
Show certificate requests for this API key
Show a private RSA key
Return a list of accepted , denied , unaccepted and rejected keys .
Return the minion keys directory paths .
Kill the salt minion.
restart the salt minion .
state.apply < salt.modules.state.apply > ` with given options to set up test data .
generate test results output list
is valid for a module
Equivalent to a salt cli: salt web state.show_top
Generic call of salt Caller command
Determine the type of variable returned
Test if two objects are equal
Test if an boolean is True
Test if an boolean is False
is less than the returned value
Test if a returned value is empty
is not empty
return a list of paths to search for states
Mount VirtualBox Guest Additions CD to the temp directory.
Unmount VirtualBox Guest Additions CD from the temp directory.
Uses the CD , connected by VirtualBox .
Remove VirtualBox Guest Additions.
Check VirtualBox Guest Additions version.
Get a size of a disk .
Get available file systems and their types.
Get available CPU information.
Get network configuration.
operating system summary
Return configuration files.
known local accounts to the system .
known local groups to the system .
excluding local accounts .
Local users and groups.
calls grains items and picks out only
installed software .
Resolve local users and groups.
Find all unmanaged files.
Return all the summary of the particular system.
Create vm from file
Create vm from configuration
Start a vm
Stop a vm
Return a list of VMs using lookup
Send non-maskable interrupt to vm or capture a screenshot
Delete a vm
describing a VM
Lookup info on running kvm
Create snapshot of a vm
Reprovision a vm
Create a new vm
Update a new vm
Send a vm to a directory
Receive a vm from a directory
Install a capability
Uninstall a capability
List all capabilities on the system
List the capabilities installed on the system
Install a feature using DISM
Disables the feature .
List features on the system or in a package
Install a package using DISM
Uninstall a package
Display information about a package
calling of execution modules via sudo .
Using clean passes -C to the hg up; Update the repo to a given revision .
Check if a ZFS filesystem or volume or snapshot exists .
Create a ZFS File System .
Destroy a ZFS File System .
Relocate a ZFS File System .
Return a list of all datasets or a specified dataset on the system and the
mounted zfs filesystems
Mounts ZFS file systems
Unmounts ZFS file systems
Clears the specified property
Display the difference between a snapshot of a given filesystem and
given dataset to a previous snapshot .
Creates a clone of the given snapshot .
Promotes a clone file system to no longer be dependent on its `` origin ''
Creates a bookmark of the given snapshot
existing user references for the given snapshot or snapshots .
Adds a single reference , named with the tag argument , to the specified
Sets the property or list of properties to the given value ( s ) for each dataset .
raise a warning , by default , a `` DeprecationWarning `` ,
raise a warning ( by default , a `` DeprecationWarning `` )
Compares two version strings using salt.utils.versions.LooseVersion .
Compares two version numbers .
required boto libs in one central location .
versionadded : : 2017.11.0
Delete target group.
Show all employees for this company .
Specifying an empty value will; Update one or more items for this employee .
Show all users for this company.
Show all meta data fields for this company.
Make a web call to BambooHR
Get an sqlite3 connection , and initialize the package database if necessary
List info for a package
List files for an installed package
Register a package in the package database
Register a file in the package database
Unregister a file from the package database
Sends a post command to the device and returns the decoded data .
returns the session cookies .
Closes the session with the device .
is reachable , else false .
using the config option hash_type and store tdata with 'token ' attribute set
Fetch the token data from the store .
Remove token from the store.
List all tokens in the store.
os.path.islink ( )
os.readlink ( )
True if path is a reparse point ; False otherwise .
Retrieves the reparse point data structure for the given path .
Python clone of /usr/bin/which
return the first one that is found
tries to solve some issues when joining multiple absolute
Remove illegal path characters for windows
is a helper than ensures that all paths returned from os.walk are
Used to parse emerge output to provide meaningful output when emerge fails
get upgradable packages
containing the matches for a given package name from the
Update the portage tree using the first available method from the following
has a function to remove unused dependencies .
Check if the installed package already has the given requirements .
Install a p12 certificate file into the macOS keychain
Uninstall a p12 certificate file from the macOS keychain
Set the default keychain to use
Grab the opts dict of the master config by trying to import Salt
supply the WSGI app and config
Return a sqlite3 database connection
Insert minion return data into the sqlite3 database
Return the load from a specified jid
Return the information returned from a specified jid
do n't try to operate on worktrees in git < 2.5.0 .
runs the git CLI command
Expand home directory
Check every part of path for executable permission
inspect opts and split them if necessary
Do a version check and make sure that the installed version of git can
Windows only : search for Git 's bundled ssh.exe in known locations
throw an exception with the error message on an error return code .
git rev-parse to return the top level of a repo
retrieve git config options
Based on whether global or local config is desired , return a list of CLI
Interface to `git-branch(1)`_
Interface to `git-checkout(1)`_
Interface to `git-clone(1)`_
Interface to `git-commit(1)`_
Get the value of a key in the git configuration file
Returns the current branch name of a local checkout .; is detached ,
Returns the ` git-describe ( 1 ) ` _ string ( or the SHA1 hash if there are no
versionadded : : 2015.8.12,2016.3.3,2016.11.0
versionchanged : : 2015.8.2
Interface to `git-init(1)`_
Returns the upstream hash for a remote; git-ls-remote ( 1 ) ` _ .
Interface to `git-merge(1)`_
Interface to `git-push(1)`_
Interface to `git-rebase(1)`_
Get the fetch and push URL for a specific remote
cwd
Get fetch and push URLs for each remote in a git checkout
git-reset ( 1 ) ` _ , returns the stdout from the git command
Returns the SHA1 hash of a given identifier ( hash , branch , tag , HEAD , etc . )
Interface to `git-rm(1)`_
returns the stdout from the git command
Expands a user-provided specification of source files into a list of paths .
.. note::
Uses the gzip command to create gzip files
Uses the gunzip command to unpack gzip files
Uses the `` zipfile `` Python module to create zip files
Uses the `` zipfile `` Python module to unpack zip files
create rar files
unpack rar files
Process markup in the
Trim the file list for output .
retrieve the auth_token from nsot
check whether or not this minion should have this external pillar returned
is given , query nsot for that specific device , otherwise return
retrieve a dict of a device that exists in nsot
retrieve a list of all devices that exist in nsot
Query NSoT API for network devices
Return the changes
specified incron job is present for the specified user .
Return a list of Salt Queues on the backend
Provide the number of items in a queue
Pop one or more or all items from a queue
create an event on the Salt event bus to be
Get consistent opts for the queued runners
be run later .
Process queued runners
yamlify ` arg ` and ensure it 's outermost datatype is a list
Publish a command from the minion out to other minions , publications need
Publish a command from the minion out to other minions .
Return the full data about the publication , this is invoked in the same
Execute a runner on the master and return the data from the runner
Execute kadmin commands
Get all principals
Get princial details
List policies
Current privileges
Create Principal
Delete Principal
Create keytab
named host is present with the given ip
named host is absent
given hostnames are associated with the
Returns revision ID of repo
Return the list of svn remotes and their configuration information
remotes no longer configured
Completely clear svnfs cache
Clear update.lk
Execute an svn update on all of the repos
Check if an environment is exposed by comparing it against a whitelist and
Return the root of the directory corresponding to the desired environment ,
operates similarly to; match the path and ref .
Return a dict containing the file lists for files , dirs , emptydirs and symlinks
True if the given username and password authenticate for the
Retrieves the grains from the network device if not cached already .
Loads the network device details if not cached already .
Retrieves the grain value from the cached dictionary .
Retrieves device-specific grains.
Return the username .
is set by the NAPALM grain module
Return the DNS information of the host .
Return the connection optional args .
Return a list of the filenames specified in the `` results `` argument , which
named package is built and exists in the named directory
Make a package repository and optionally sign it and packages present
Take the path to a template and return the high data structure
Take template as a string and return the high data structure
Check the template shebang line and return the list of renderers specified
Check that all renderers specified in the pipe string are available .
Change the system runlevel on sysV compatible systems
Reboot the system
Return the twilio connection
Send an sms
Create a list of file ref objects to reconcile
references out of the states
Generate the execution package from the saltenv file refs and a low state
Load up the modules for remote compilation via ssh
Evaluate master_tops locally
Get a resource type api versions
Get an AzureARM resource by id
Return the first configured provider instance .
Return a connection object for a client type .
Return the location that is configured for this provider
Return a dict of all available regions .
Return a dict of all available images on the provider
Return a list of sizes available from the provider
List VMs on this Azure account
List all VMs on the subscription with full information
associated with the subscription
Show the details from AzureARM concerning an instance
Delete a network interface.
Get the public ip address details by name .
Get a network interface .
Create a network interface .
Request a VM from Azure.
Create a single VM from a data dict .
Destroy a VM .
List storage accounts within the subscription.
Get the cloud environment object .
Get the block blob storage service .
Delete a blob from a container.
Delete a managed disk from a resource group.
List virtual networks.
List subnets in a virtual network.
Ensures that the specified PowerPath license key is present
Create the .rpmmacros file in user 's home directory
Create the rpm build tree
Get the spec file and place it in the SPECS dir
Get the distribution string for use with rpmbuild and mock
include string for list of dependent rpms to build package
Create a source rpm from the given spec file and sources
Given the package destination directory , the spec file source and package
Make a package repository and optionally sign packages present
Execute the Thorium runtime
Open the connection to the Junos device , login , and bind to the
return the connection status with the remote device .
grabbing a URL .
Stolen completely from boto.providers
print it out .
Print out the grains
Execute the salt call logic
Call the module
Return the data up to the master
Execute a remote execution command
Sets global variables _OS_IDENTITY_API_VERSION and _TENANT_ID
Ensures that the keystone tenant exists
Ensures that the keystone project exists
Ensures that the keystone role exists
Ensure service present in Keystone catalog
exists for service
does n't exist in Keystone catalog
add ' for a service whose script is installed in
is a System V service ( includes those managed by
is managed by chkconfig .
is enabled for the specified
Return `` True `` if the service is enabled according to chkconfig ; otherwise
Enable the named sysv service to start at boot .
named sysv service from the system .
rename the .conf file
Return list of sysv services.
Return the enabled services .; Use the `` limit `` param to restrict results
installed services .; Use the `` limit `` param to restrict results
named service is available .; Use the `` limit `` param to
The inverse of service.available.
Stop the specified service
Reload the named service
specified command name in search path
dictionary to use in build process
dictionary to use in repo options process
dictionary to use in repo distributions process
Create the .pbuilder family of files in user 's home directory
Get the named sources and place them into the tree_base
Create a platform specific source package from the given platform spec/control file and sources
Given the package destination directory , the tarball containing debian files ( e.g .
Generate a Vault token for minion minion_id
Unseal Vault server
Validate that either minion with id minion_id, or the master, signed the
Get the policies that should be applied to a token for minion_id
Expands the pattern for any list-valued mappings , such that for any list of
Validate the current token exists and is still valid
Create Vault url for token creation
Given function name , find and return matching Lambda information .
Given a function name , check to see if the given function name exists .
Given a function name and optional version qualifier , delete it .
Given a function name describe its properties .
given code to the named lambda function .
Add a permission to a lambda function .
Remove a permission from a lambda function .
given lambda function
List all Lambda functions visible in the current scope.
List the versions available for the given function .
Given a valid config , create an alias to a function .
Given function name and alias name , find and return matching alias information .
Given a function name and alias name , check to see if the given alias exists .
Given a function name and alias name describe the properties of the alias .
Update the named alias to the configuration .
Identifies a stream as an event source for a Lambda function.
Given an event source and function name , return a list of mapping IDs
Given an event source mapping ID or an event source ARN and FunctionName ,
Update the event source mapping identified by the UUID .
Update the cache file for the bucket .
cache file for a match .
Return an MD5 file hash
Return a list of all files on the file server in a specified environment
Return a list of all directories on the master
Get AWS keys from pillar or config
Return the cached file name for a bucket path file
Looks for all the directories in the S3 bucket cache metadata.
Looks for a file's metadata in the S3 bucket cache file
Return a list of file paths with the saltenv directory removed
Return the configuration mode , either buckets per environment or a list of
given characteristics .
detailed info about the given zone .
given domain name and return detailed info about them .
detailed info about all zones in the bound account .
Update the comment on an existing Route 53 hosted zone .
hosted zone by domain name , and PrivateZone status if provided .
required to suport AWS 's domain name
process a change batch & encode the bits which need encoding .
Get all resource records from a given zone matching the provided StartRecordName ( if given ) or all
See the ` AWS Route53 API docs ` __ as well as the ` Boto3 documentation ` __ for all the details ...
matches `` mask '' .
Raise an exception if value is empty .
Remove everything that would affect paths in the filename
Get all EIP 's associated with the current credentials .
associated with the current account .
Return the first unassociated EIP
'interesting ' info about some , or all EIPs associated with the current account .
Allocate a new Elastic IP address and associate it with your account .
Free an Elastic IP address.
running instance or a network interface .
running instance .
addresses to a network interface .
Get a list of AZs for the configured region .
Given instance properties , find and return matching instance ids
Given instance properties that define exactly one instance , create AMI and return AMI-id .
Given image properties , find and return matching AMI ids
Terminate the instance described by instance_id or name .
Given instance properties , return the instance id if it exists .
Given an instance_id , return a list of tags associated with that instance .
Given an instance id , check to see if the given instance id exists .
Convert a string , or a json payload , or a dict in the right
start an EC2 instance .
see if a key exists .
Creates a key and saves it to a given path .
Imports the public key from an RSA key pair that you created with a third-party tool .
Deletes a key .
returns a list .
Get an EC2 instance attribute .
Set an EC2 instance attribute.
Get an Elastic Network Interface id from its name tag .
Get an Elastic Network Interface .
Create an Elastic Network Interface.
Attach an Elastic Network Interface.
Modify an attribute of an Elastic Network Interface.
Get a list of all EBS volumes , optionally filtered by provided 'filters ' param
matching the filter criteria , or all tags in the account otherwise .
specified resource ids .
Detach an EBS volume from an EC2 instance.
Attach an EBS volume to an EC2 instance.
Create an EBS volume to an availability zone.
given identifier ( hash , branch , tag , HEAD , etc )
return an identifier for the given revision
Export a tarball from the repository
Perform a pull on the given repository
Update to a given revision
changed files of the given repository
Return a dictionary with a copy of each interface attributes in ATTRS
Watch for changes on network settings
Set a key/value pair in memcached
Get a value from memcached
proxy settings for this mininon
Create default archive name.
existing archives .
Get the last available archive
Delete archives
Format stats of the sync output.
Run Salt Support on the minion.
Return the next iteration by popping ` chunk_size ` from the left and
Check vmware/vcenter for all data
recurse through a vim object and attempt to return all child objects
crawl an attribute specified for retrieval
serialize some objects for prettier return
sqlite3 ( with no return data ) , usually used
returns all rows , be careful ! )
Show all tables in the database
Show all indices in the database
Return the targets from the Salt Masters ' minion cache .
return the location of the GPG key directory
Given a block of ciphertext as a string , and a gpg object , try to decrypt
try to decrypt any object .; is a six.string_types
Create a gpg object given a gpg_keydir , and then use it to try to decrypt
Emit a dict with a key `` msgs '' whose value is a list of messages
Look up top data in Cobbler for a minion .
Returns statistics about the locate database
are : :
Initalizes the LXD Daemon , as LXD does n't tell if its initialized
Manage a LXD Server config setting .
Authenticate with a remote peer.
Ensure IPMI user and user privileges.
Remove user
search for minions in master caches
Ensure the launch configuration exists.
Return the configuration read from the master configuration
Return the client object and session key for the client
Get session and key
Call the Spacewalk xmlrpc api .
Add server groups to a activation key
Delete all server groups from Spacewalk
Delete all systems from Spacewalk
Delete all activation keys from Spacewalk
specified server from Spacewalk
need to enhance what modify_cache_cluster ( ) considers when deciding what is to be
given cache cluster exists .
need to enhance what modify_replication_group ( ) considers when deciding what is to be
need to enhance what modify_cache_subnet_group ( ) considers when deciding what is to be
Ensure cache subnet group exists.
given cache subnet group is deleted .
Ensure cache parameter group exists.
Given identity pool name ( or optionally a pool_id and name will be ignored ) ,
Given an identity pool name , ( optionally if an identity pool id is given ,
Creates a new identity pool .; is optional .
turn a name into an arn string ,
Given an identity pool id , set the given AuthenticatedRole and UnauthenticatedRole ( the Role
Updates the given IdentityPoolId 's properties .
+ no changes data
Create a snapper pre snapshot
Create the post states snapshot
Return the pause information for a given jid
Get a report on all of the currently paused state runs and pause
Remove a pause from a jid , allowing it to continue .
Execute the compound calls stored in a single set of high data
Execute the information stored in a template file on the minion .
Execute the information stored in a string from an sls template
Retrieve the state data from the salt master for this minion and execute it
Execute the states in one or more SLS files
is useful to apply; Execute a specific top file instead of the default .
Retrieve the highstate data from the salt master to analyse used and unused states
Returns the list of states that will be applied on highstate .
Display the low data from a specific sls .
Tests for the existance the of a specific SLS or list of SLS files on the
Tests for the existence of a specific ID or list of IDs within the
Execute a single state function with the named kwargs , returns False if
cached state files , forcing even cache runs to refresh the cache
Execute a packaged state run , the packaged state run will exist in a
Disable state runs.
Enable state function or sls run
Return messages for disabled states
Ensure that a grain is set
formed as a list .
Run this function when a new version; stored in __context__ .
returns an empty list
Returns ['--no-progress'] if on v0.10.4 or later, otherwise returns an
chocolatey.bat on the host .
Returns the version of Chocolatey installed on the minion .
install the latest version of the Chocolatey package manager
pull a vague package list from the repository .
pull a full package list from the Windows Features
install a package via Cygwin .
install a package via Ruby 's Gems .
install a package if it does n't already exist .
install a package via Python 's easy_install .
install a package via the Microsoft Web PI service .
uninstall a package .
update packages on the system .
check an installed package version , and optionally
add a source .
change the state of a source .
given overlay from the cached remote list to your locally
Remove the given overlay from the your locally installed overlays .
List the locally installed overlays .
Prepare the connection to the Django authentication framework
Simple Django auth
passed to a top file environment and determines if the
installed features .; Supported on Windows Server 2008 and Windows 8 and
List networks on a wireless interface
List all of the wireless interfaces
Poll vmadm for changes
Run a SQL query and return query result as list of tuples , or a list of dictionaries if as_dict was passed , or an empty list if no data is available .
Creates a new database .
Drops a specific database from the MS SQL server .
Checks if a role exists.
Creates a new database role .
Remove a database role .
exists in the MS SQL server .
Creates a new login .; Does not update password of existing logins .
Removes an login.
exists in a specific database on the MS SQL server .
Creates a new user .; login is not specified , the user will be created
Removes an user.
Ensure the cloudwatch alarm exists.
joining comments together and conditionally adding a period at
Check the diff for signs of incorrect argument handling in previous
parsing the networks
Resolve the image ID and pull the image if necessary
is present and
Ensure that a container (or containers) is stopped
Ensure that a container is absent
Execute the onlyif/unless/creates logic .
Provide a place to hang onto results of -- list- [ locations|sizes|images ]
Return basic data on nodes
Return all data on nodes
Remove a node from Vultr
Show the details from the provider concerning an instance
retrieve a Vultr ID
Perform a query directly against the Vultr REST API
based on configured function
returns a structure based on the
Pulls a string from redis and deserializes it from json .
update the eix database
given properties exist on the jboss instance .
given JNDI binding are present on the server .
given application is deployed on server .
Reloads configuration of jboss server.
Validates a username based on the guidelines in ` useradd ( 8 ) `
loads our states into the salt __context__
Send a message to the google chat room specified in the webhook url .
Raise an exception with __name__ from name , args from args
Fire raw exception across the event bus
said data back to
used when the master starts
Clean out the old fileserver backends
expired tokens from the master
running on the master
be placed in the filesystem with permissions 0400 so
Update the fileserver backends , requires that a salt.fileserver.Fileserver
Check if the specified filename has correct permissions
Check a keyid for membership in a signing file
Check a keyid for membership in a autosign directory .
matching grains in the autosign_grains_dir .
be signed .
Set the local file objects from the file server interface
Verify that the passed information authorized a minion to execute
Return the master options to the minion
Return the results from master_tops if configured
Gathers the data from the specified minions' mine
Return the mine data
delete a specific function from its own mine
delete all of its own mine contents
send files to the master , files are sent to the
Return the pillar data for the minion
Receive an event from the minion and fire it on the master event
Handle the return data sent from the minions
Receive a syndic minion return and format it to look like returns from
Execute a runner from a minion , return the runner 's function data
Request the return data from a specific jid, only allowed
Publish a command initiated from a minion , this method executes minion
Allow a minion to request revocation of its own key
Send a master control function back to the runner system
Send a master control function back to the wheel system
return an authentication token , the clear load needs to
sends out publications to the minions , it can only be used
slack events and forward them to salt , new version
Get all users from Slack
Get all channel names from Slack
get info from groups in config , and from the named pillar
is the pillar.get syntax for the pillar to be queried .
replaces a function in main called 'fire '
Break out the permissions into the following :
is the string of the command line
Returns a tuple of (target, cmdline,) for the response
does n't work out , and TypeError if
slack_token = string
are permitted to run a command on a target , look to see
cmdline: list
Given a list of job_ids , return a dictionary of those job_ids that have
pending messages from the message_generator , sending each
:type message_generator: generator of dict
Return a Pg cursor
Return data to a postgres server
exists , eventually creates new host group .
does not exist , eventually delete host group .
match that of the API
Compare the API results to the current statefile data
create or update an element
delete an element
Ensures that the Connection Factory is present
Ensures the transaction factory is absent .
Ensures that the JMS Destination Resource (queue or topic) is present
does n't exists
Ensures that the JDBC Datasource exists
Ensures the JDBC Datasource does n't exists
Ensures that the system properties are present
target_expressing ` splitting it into ` engine ` , ` delimiter ` ,
is None , it; Get the grains/pillar for a specific minion .
expand `` nodegroup `` from `` nodegroups `` ; ignore nodegroups in `` skip ``
Gathers the data from the specified minions' mine, pass in the target,
found by looking at nodegroups
Return the minions found by looking via a list
Return the minions found by looking via regular expressions
Retreive complete minion list from PKI dir.
search for minions in master caches If 'greedy ' ,
Return the minions found by looking via grains
Return the minions found by looking via grains with PCRE
Return the minions found by looking via pillar
Return the minions found by looking via pillar with PCRE
Return the minions found by looking via ipcidr
Return the minions found by looking via range expression
Return the minions found by looking via compound matcher
Return a set of all connected minion ids , optionally within a subset
Return a list of all minions that have auth 'd
Check the passed regex against the available minions ' public keys
returns if the expression sent in is; Return a Bool .
Validate a single regex to function comparison , the function argument
Read in the form and determine which auth check routine to execute
defines if the requested function is authorized .
is eligible for .
Check special API permissions
Check the given function name ( fun ) and its arguments ( args ) against the list of conditions .
is a dicts : { 'args ' : [ ... ] , 'kwargs ' : { ... } } or a list of such dicts .
Return a list of alerts from CLC as reported by their infra
returns a list of images available to you
returns a list of locations available to you
get the build status from CLC to make sure we dont return to early
get the system build going
named service is present .
passed glob matches the id
remove specified ASAM platform from the Novell Fan-Out Driver
list all ASAM platforms present on the Novell Fan-Out Driver
list all ASAM platform sets present on the Novell Fan-Out Driver
add an ASAM platform using the specified ASAM platform set on the Novell
Create a copy of __salt__ dictionary with module.function and module [ function ]
Initialize the directories for the files
Check the filesystem for existing files
Install a single file to the file system
Remove a single file from the file system
Get the hexdigest hash value of a file
Ensure the specified policy is set
XOR definition for multiple variables
mounted on this minion
modify any key-value pair where value is a datetime object to a string .
Retrieve full list of values for the contentkey from a boto3 ApiGateway
matching rest api information by the given name and desc .
is included ,
see if the given Rest API Name and optionally description exists .
Create a new REST API Service with the given name
given name and an optional API description
Given rest api id , return all resources for this api .
Given rest api id , and an absolute resource path , returns the resource id for
Given rest api id , and an absolute resource path , create all the resources and
Given restApiId and an absolute resource path , delete the resources starting
info about the given api key
Gets information about the defined API Keys.
given name and description .
Deletes a given apiKey
replace patch operation on an ApiKey resource
tuples on an ApiKey resource list path
update the given apiKey with the given description .
enable the given apiKey .
associate the given stagekeyslist to the given apiKey .
disassociate the given stagekeyslist to the given apiKey .
Gets information about the defined API Deployments.
given restApiId and deploymentId .
deployed deployment for a given stage
Creates a new API deployment .
given restApiId and deploymentID
given restApiId and stage name with the given variables ,
given apiID and stage name
Get all API stages for a given apiID and deploymentID
Creates a new API stage for a given restApiId and deploymentId .
stage identified by stageName from API identified by restApiId
identified by stageName from API identified by restApiId
Creates API method for a resource in the given API
Get API method for a resource in the given API
Delete API method for a resource in the given API
given resource in the given API
Delete API method response for a resource in the given API
Get API method response for a resource in the given API
Get all models for a given API
Get a model by name for a given API
see if the given modelName exists in the given restApiId
replace patch operation on a Model resource
update the schema ( in python dictionary format ) for the given model in the given restApiId
identified by name in a given API
Create a new model in a given API with a given schema , currently only contentType supported is
Get an integration for a given method in a given API
Get an integration response for a given method in a given API
Deletes an integration response for a given method in a given API
Creates an integration for a given method in a given API .
Creates an integration response for a given method in a given API
return list of usage plan items matching the given attribute value .
existing usage plans , optionally filtered to match a given plan name
verify that quota parameters are valid
Creates a new usage plan with throttling and quotas optionally applied
existing usage plan with throttling and quotas
identified by plan_id
updates the usage plan identified by plan_id by adding or removing it to each of the stages , specified by apis parameter .
given usage plan to each of the apis provided in a list of apiId and stage values
given usage plan from each of the apis provided in a list of apiId and stage value
Return a simple vdev tree from zpool.status ' config section
Return the status of the named zpool
Display I/O statistics for the given pools
Sets the given property on the specified pool
Check if a ZFS storage pool is active
Destroys a storage pool
Scrub a storage pool
given storage pool
specified device to zpool
Replaces ``old_device`` with ``new_device``
file based virtual devices for a zpool
Clears device errors in a pool.
Generates a new unique identifier for the pool
see if an autoscale group exists .
Get the configuration for an autoscale group .
Create an autoscale group.
Update an autoscale group.
create scaling policies
create scheduled actions
Get a mime multipart encoded string from a cloud-init dict .
Check for a launch configuration's existence.
return all Launch Configuration with details .
List all Launch Configurations.
Create a launch configuration .
Delete a launch configuration .
Return the arn for a scaling policy in a specific autoscale group or None
Return all AutoScale Groups visible in the account
return attribute of all instances in the named autoscale group.
desired instances from StandBy mode
List SCSI devices, with details
List scsi devices
Retrieves the vsan_datastore
Creates a new cluster , if it does n't exist on the; Configures a cluster .
Configures the cluster 's VSAN datastore
licenses on the cluster entity
is a part of salt/__init__.py
be deployed - prep the target directory and emit the
Generate a hash digest string for a file .
Unpack the Salt thin archive.
need to be deployed .
Unpack the external modules.
prevent systemd swipes only part of the files in the /tmp .
supported python version in the thin
Main program body
Mount the device in a temporary place.
Check if the subvolume is the current default .
Set the subvolume as the current default.
Check if the subvolume is copy on write
makes sure that the mount and umount happends in
sure that a btrfs subvolume is present .
sure that a btrfs subvolume is removed .
Parse the given atom , allowing access to its parts
add version to category/package
Merge /etc/portage/package.keywords and
Convert a config file to a config directory .
Move entries in the correct file.
check compatibility of accept_keywords
removing duplicates and resolving conflicts
Append a string or a list of flags for a given package or DEPEND atom to a
Append a list of use flags for a given package or DEPEND atom
given package or DEPEND atom .
Verify if the given package or DEPEND atom has the given flag .
given flags are currently not set .
Tell if a given package or DEPEND atom is present in the configuration
existing device-mapper device details .
List the contents of the crypttab
Remove the named mapping from the crypttab .
Verify that this device is represented in the crypttab , change the device to
Open a crypt device using `` cryptsetup `` .
Remove a single package with pkg_delete
Remove a package and extra configuration files .
Run a full package upgrade ( `` pkg_add -u `` ) , or upgrade a specific package
exists with matching definition .
exists and the definition matches .
standardized pipeline objects to be used for comparing
Return v1 == v2.
Return the value at key 'id ' or 'key ' .
string diff of pipeline definitions .
standardized format for lists/dictionaries .
Return a list of pipeline objects that compose the pipeline
Return a list of parameter objects that configure the pipeline
Return a dictionary of parameter values that configure the pipeline
Convert a dictionary to a list of dictionaries , where each element has
Transforms dictionary into pipeline object properties.
Serialize Python data to JSON.
Ensure that the named database is absent
Return the net.find runner options .
Return the mine function from all the targeted minions .
return the rows .
search the interfaces IPs using the MAC address .
get the interfaces hardware address using the IP Address .
Search for interfaces details in the following mine functions:
using the following mine functions :
Search in all possible entities (Interfaces, MAC tables, ARP tables, LLDP neighbors),
Execute multiple search tasks.
providing the infrastructure for
Return a copy of the string after the specified prefix was removed
returns a sanitized version
Execute Augeas commands
Get a value for a specific augeas path
Set a value for a specific augeas path
Get matches for path expression
List the direct children of a node
Returns recursively the complete tree of a node
Return a list of all installed kernels .
reboot the system .
Remove a specific version of the kernel .
Remove all unused kernel packages from the system .
Compare function for package version sorting
Ensure that the key exists in redis with the value specified
Ensure key absent from redis
Set this redis instance as a slave.
be run directly for testing
Squished GUID ( SQUID ) to GUID .
return `` True `` if it is .
Calls RegQueryValueEx
Return the install time , or provide an estimate of install time .
return the name value .
follow the Microsoft Installer standard , returns
is stored in binary format .
Returns information on a package.
Provided with a valid Windows Security Identifier ( SID ) and returns a Username
Determine the Package ID of a software/component using the
returns the version and where the version string came from , based on instructions
Update data with the next software found
searches the uninstall keys in the registry to find
be strings unless they should remain a different type
Return the names of remote refs ( stripped of the remote name ) and tags
based on the desired saltenv
Check if the relative root path exists in the checked-out copy of the
are no longer seen as fileserver envs
need to be maintained in the git config ,
was updated , return True .; Fetch the repo .
does not already exist .
Place an lock file and report on the success/failure.
Set and automatically clear a lock
Check if an environment is exposed by comparing it against a whitelist
Resolve dynamically-set branch
Return a tree object for the specified environment
Examine self.id and assign self.url (and self.branch, for git_pillar)
Return the expected result of an os.walk on the linkdir , based on the
catch an `` Exception '' class here
Return a boolean; using GitPython .
Get list of directories for the target environment using GitPython
Check the refs and return a list of the ones which can be used as salt
Get file list for the target environment using GitPython
specified file in the specified environment
Return a git.Tree object matching a head ref fetched into
Return a git.Tree object matching a tag ref fetched into refs/tags/
Return a git.Tree object matching a SHA
Using the blob object , write the file to the destination path
Checkout the configured branch/tag
do n't appear as fileserver environments
Return a boolean which; using pygit2 .
Get a list of directories for the target environment using pygit2
Get file list for the target environment using pygit2
Return a pygit2.Tree object matching a head ref fetched into
Return a pygit2.Tree object matching a SHA
attributes for pygit2 callbacks
Check the username and password/keypair info for validity .
Completely clear cache
Clear update.lk for all remotes
return a boolean to let the calling function know
Returns a dictionary mapping remote IDs to their intervals, designed to
Determine which provider to use
Check if GitPython is available and at a compatible version ( > = 0.3.0 )
Check if pygit2/libgit2 are available and at a compatible version .
Write the remote_map.txt
handle locking and checking out
Return a dict of all symlinks based on a given path in the repo
Checkout the targeted branches/tags from the git_pillar remotes
is present in the correct location and
Checkout the targeted branches/tags from the winrepo remotes
Check if the minion is exposed , based on the whitelist and blacklist
standardize the subprocess call
Bouncing Traffic Server shuts down
Restart the traffic_manager and traffic_server processes on the local node .
Display the current values of all metrics whose names match the
Display the current values of all configuration variables whose
Read Traffic Server configuration variable definitions.
Set the value of a Traffic Server configuration variable.
is identified by a path
are  all  for all current
supported node states
Compare different libcloud versions
Return a libcloud node for the named VM
Return the location object to use
Return the VM 's size object
Reboot a single VM
Return a list of the VMs that are on the provider , with all fields
provided connection object has a specific method
exists in InfluxDB .
Create a database .
Drop a database.
Get information about given user .
Create a user .
Change password of a user.
privileges to a user .
Revoke cluster administration privileges from a user.
Remove a user .
Check if retention policy with given name exists .
Drop a retention policy.
Create a retention policy .
List privileges from a user.
Grant a privilege on a database to a user .
Revoke a privilege on a database from a user .
Check if continuous query with given name exists on the database .
Get an existing continuous query .
Create a continuous query .
Drop a continuous query.
Parses a ResultSet returned from InfluxDB into a dictionary of results ,
Execute a query .
Returns the recursive diff between dict values
Deletes an attribute from all of the intersect objects
Returns a list of dictionaries with key value pairs.
describing the changes
describing the changes .
Returns the new values from the diff
Returns the old values from the diff
Returns the list of changed values .
Resolves the given symlink path to its real path , up to a maximum of the
Return the id of the primary group that owns a given file ( Windows only )
Return the user that owns a given file
Chown a file , pass the file the desired user and group without following any
Return a dict containing the stats about a given file
Return a dictionary object with the Windows
Set file attributes for a file.
Remove the named file or directory
Create a symbolic link to a file
Check if the path is a symlink
Return the path that a symlink points to
is available and permissions are set .
Set owner and permissions for each directory created.
Check owner and permissions for the passed directory.
Set permissions for the given path
Check if current number of sessions of a server for a specific haproxy backend
Given a thing type name , check to see if the given thing type exists
Given a thing type name describe its properties .
Given a valid config , create a thing type .
Given a thing type name , deprecate it when undoDeprecate is False
Given a thing type name , delete it .
Given a policy name , check to see if the given policy exists .
Given a valid config , create a policy .
Given a policy name describe its properties .
Given a policy name and version ID , check to see if the given policy version exists .
Given a policy name and version , delete it .
Given a policy name and version describe its properties .
List all policies
List the versions available for the given policy .
Given a rule name , check to see if the given rule exists .
Given a valid config , create a topic rule .
Given a rule name , delete it .
Given a topic rule name describe its properties .
given topic , if specified )
Returns a unique service status
Returns all service statuses
/etc/rc.conf so a service is started or not at boot time and
List all the images with alias by location
Return datacenter ID from provider configuration
Return a list of the loadbalancers that are on the provider
Creates a loadbalancer within the datacenter from the provider config .
Return the datacenter from the config provider datacenter ID
Creates a virtual datacenter based on supplied parameters .
List all the data centers
Return a list of VMs that are on the provider
Reserve the IP Block
Return a node for the named VM
defined in cloud profile .
Enables public Internet access for the specified public_lan.
Retrieve list of SSH public keys.
return absolute path if exists .
destroy a machine by name
reboot a machine by name
stop a machine by name
start a machine by name
VM from the cloud profile .
Construct server instance from cloud profile config
Construct VM system volume list from cloud profile config
Construct a list of optional data volumes from the cloud profile
Construct a list of optional firewall rules from the cloud profile .
is provisioned .
Uninstall the specified package.
Determines if a remote exists.
make __utils__ available on demand .
allow copy.deepcopy copy bound methods
Check if there is an upgrade available for a certain package
Upgrade all of the packages to the latest available version.
packages using the pkgutil tool .
Remove a package and all its dependencies which are not in use by other
is monitored .; given will be used for Zenoss device name and should be resolvable .
Find volume by name on minion
Generates a list of salt : // < formula > /defaults .
is used much like pillar.get except that it will read
defaults.merge
defaults.update
Copied from win_system.py ( _get_date_time_format )
's value .; return the first match .
given a key .
save the task definition .
Get a connection to CouchDB
Set a key/value pair in couchdb
Generate a key for use with salt-ssh
Return the correct shell interface for the target system
Parse out an error and return a targeted error string
Return options for the ssh command base for Salt to call
pass to ssh
Return the string to execute ssh-copy-id
Execute ssh-copy-id to plant the id file on the target
Return the cmd string to execute
execute the command string
Yield None until cmd finished
Execute a remote command
scp a file or files to a remote system
Execute a shell command via VT .; is blocking and assumes that ssh
sets up the sentry handler
Return a value for 'name ' from master config file options or defaults .
substituting username where found .
Bind with binddn and bindpw only for searching LDAP
Authenticate via an LDAP bind
Simple LDAP auth
Authenticate against an LDAP group
retrieve list of minion_ids from an OU or other search .
were passed in as NAPALM optional arguments .
Reconnect the NAPALM proxy when the connection
Execute arbitrary methods from the NAPALM library.
Return the compliance report .
generate a keypair .
Alias to `{box_type}_encrypt`
is a helper function to encrypt a file and return its contents .
Alias to `{box_type}_decrypt`
using a public key generated from ` nacl.keygen ` .
using a secret key that was encrypted using a public key with ` nacl.sealedbox_encrypt ` .
using a secret key generated from ` nacl.keygen ` .
was encrypted using ` nacl.secretbox_encrypt ` using the secret key
named grafana dashboard is absent .
Return a copy without fields that can differ .
Return a dashboard with properties from parents .
Return a row with properties from parents .
Return a panel with properties from parents .
spans to take up the available width .
rows to the top of the dashboard .
panels auto-incrementing IDs .
annotation_tags into annotations .
Get a specific dashboard .
Delete a specific dashboard .
Update a specific dashboard .
Return a dictionary of changes between dashboards .
Strip falsey entries.
Given an email address , checks the local
Get all alert definitions associated with a given deployment or if metric_name
Given an email address , creates a notification-channels
get all the alarms set up against the current deployment
create an telemetry alarms .
is a dict of alert configuration data .; update an telemetry alarms .
delete an alert specified by alert_id or if not specified blows away all the alerts
returned HTTP content depending on the specified encoding .
See the following article :; Return the location of the ca bundle file .
update the CA bundle file from a URL
Render a template
Parse the `` Set-cookie '' header , and return a list of cookies .
show up in logs
sanitize each component of the url .
Returns the following
Deletes a nameserver .
Creates a new nameserver .
installed version of dnsmasq .
installed version of dnsmasq and compile options .
Dumps all options from the config file.
parsing dnsmasq files including includes .
Read in the dict structure generated by the salt key API methods and
Get a boto connection to SQS .
sqs and fire message on event bus
Send a file from the master to the location in specified
Transfer a directory down
turning them into immutable structures .
Display alternatives settings for defined command name
Display master link for the alternative
Check if the given path is an alternative for a name .
determining default commands
determining the default commands .
set the path for < name > as
Read the link from /etc/alternatives
Returns a placement solver
Returns a list of all capability definitions.
Returns a list of policies with the specified ids.
filtered by name .
Returns the default storage policy reference assigned to a datastore .
Set a value into the Redis SDB .
Get a value from the Redis SDB .
Ensure a load balancer member is present
is absent , based on IP and Port
parses the output into a dictionary
Tests for the CPU performance of minions.
tests the performance of the processor 's scheduler
Tests the implementation of mutex
tests the memory for read and write operations .
tests for the file read and write operations
Return LVM version from lvm version
Return all version info from lvm version
Return information about the physical volume(s)
Return information about the volume group(s)
Return information about the logical volume(s)
Set a physical device to be used as an LVM physical volume
Remove a physical device being used as an LVM physical volume
Create an LVM volume group
Add physical volumes to an LVM volume group
Create a new logical volume , with option for which physical volume to be used
Remove an LVM volume group
Remove a given existing logical volume from a named existing volume group
Query an instance upon creation from the Joyent API
make the rest api call for node creation .
take action call used by start , stop , reboot
Return the joyent data center to use , in this order :
provided object has a specific method
dictionary using the key as the identifier
get details about a machine
returned data from joyent , determine public/private IPs and
keeping only a brief listing
see whether the user has SSL turned on .
Get list of available images
get list of available packages
List the keys available
Make a web call to Joyent
file to use when determining runlevel .
is assumed disabled if a manual stanza is
have a control script in
upstart via init configuration
Return the enabled services
Return the disabled services
Disable an Upstart service.
Enable an Upstart service .
named service from starting on boot
see if the named service is enabled to start on boot
see if the named service is disabled to start on boot
Grab the YubiKey Client ID & Secret Key
Authenticate against yubico server
include exc_info if the handler is enabled for a specific log level
delete an object from a bucket .
List the contents of a bucket , or return an object from a bucket .
Return the metadata for a bucket , or an object in a bucket .
Create a new bucket , or upload an object to a bucket .
Examine the keys , and populate as necessary
Generate a NT HASH
List user accounts
Get user account details
Delete user account
Create user account
Modify user account
named service is running .
named service is dead ( not running ) .
Render a Cheetah template .
Get configuration items for URL , Username and Password
Send query along to Nagios.
Check status of a particular host By default
Check status of a particular service on a host on it in Nagios.
Set a key/value pair in a keyring service
Get a value from a keyring service
tries to identify errors and report
Connect to the Sentry server
Return a list of registered VMs , with minimal information
Return a list of registered VMs
Register a VM
Unregister a VM
Create a new VM
Clone a new VM from an existing VM
are available :; Return a list of a specific type of item .
Return strings values as python unicode string
@ host [ : port ] /sid [ servicename as { sysdba|sysoper } ]
given SID found in oratab
Run SQL query and return result
databases configuration from pillar .
Server Version (select banner  from v$version)
used by Oracle Client
containing GECOS field names and their values ,
change a user 's GECOS information
Set a new home directory for an existing user
Change the groups to which a user belongs
Creates a 'singleton ' manager to communicate with a local virtualbox hypervisor .
Needed for certain operations in the SDK e.g creating sessions
Max number of slots any machine can have
is needed to make this work !
has a network address to return or quit after the timeout
has been reached , checking at regular intervals .
TODO distinguish between private and public addresses
does the hypervisor have
Creates a machine on the virtualbox hypervisor
virtualbox to create a VM by cloning from an existing one
try and start machines
start up a VM .
stop a VM .
get rid of a machine and all its files from the hypervisor
build a dict from an XPCOM object .
Make machine presentable for outside world.
see if the machine with the given name is known
fetch a machine from Virtualbox and convert it to a dict
Compare before and after results from various salt functions, returning a
decode whichever type is passed , if necessary .
use to_str=True to ensure
encode whichever type is passed , if necessary
Encode all string values to bytes
Encode all string values to Unicode
filter data structures like grains and pillar
Traverse a dict using a colon-delimited ( or otherwise delimited , using the
using a delimiter character to denote
True if data is a list of one-element dicts ( as found in many SLS
Takes a list of one-element dicts ( as found in many SLS schemas ) and
is iterable , but not a string type .
representing the `` truth '' of the value passed .
Convert MySQL-style output to a python dictionary
Convert the data list , dictionary into simple types , i.e. , int , float , string ,
Given an iterable , returns its items as a list , with any non-string items
using JMESPath language ( http : //jmespath.org ) .
determine if something is not to be
remove items from an iterable with falsey value .
Returns a generator iterating over keys and values, with the keys all
sure the defaults value is absent
Converts the dotted notation of a saltclass class to its possible file counterparts.
dotted notation .
Takes a class name possibly including ` * ` or ` ? ` wildcards ( or any other wildcards supportet by ` glob.glob ` ) and
Expand the list of ` classes ` to no longer include any globbing .
given parameter value is an instance of either
Returns the type , id and option of a configuration object .
Returns the only one key and it 's value from a dictionary .
Creates Arguments in a TypedParametervalue.
Parses a TypedParameter and fills it with values .
Parses the configuration and creates Parameter instances .
Parses the configuration and creates an Option instance .
arg is a reference to a previously defined statement .
arg is a junction statement .
Adds a reference to statement .
arg is an inline definition of a statement .
Adds an inline definition to statement .
Adds a junction to the _current_statement .
Parses a log path .
Build the configuration tree .
is intended to be used from
Sets the path , where the syslog-ng binary can be found .
is intended to be used from; Sets the configuration 's name .
Runs the command cmd with options as its CLI parameters and returns the
Sets variables.
Runs the specified command with the syslog_ng_sbin_dir in the PATH
Creates a dictionary from the parameters , which can be used to return data
Returns the version of the installed syslog-ng .
Returns the available modules .; syslog_ng_sbin_dir is specified , it
running syslog-ng instance .
Creates the state result dictionary .
Adds key and value as a command line parameter to params.
Adds key as a command line parameter to params.
is intended to be used from the state module .
is intended to be used from states .
Writes the given parameter config into the config file .
Removes the previous configuration file , then creates a new one and writes
Builds the body of a syslog-ng configuration object.
Builds the textual representation of the whole configuration object
Get the 'System Locale ' parameters from dbus
Parse localectl status into a dict.
set the LANG locale parameter , making
Get the current system locale
Sets the current system locale
Check if a locale is available .
Generate a locale .
Deploy stack with the specified properties
named stack is absent
Grain for the minion username
Grain for the minion groupname
Unpackages a payload
Pass in the required arguments for a payload, the enc type and the cmd,
Run the correct loads serialization format
Run the correct serialization to load a file
Run the correct dumps serialization format
Serialize the correct data into the named file object
create the socket .
delete socket if you have it
Takes two arguments, the encryption type and the base payload
Detect the encryption type based on the payload
Creates or updates LXD profiles
is not present , removing it if present .
Returns a list of modules in loader.conf that load on boot.
Add a module to loader.conf to make it persistent .
is true only comment line where
Return a dict containing information about currently loaded modules
Remove the specified kernel module
requested Server Density authentication value from pillar .
starts with ` variable_prefix ` .
see the ` API; create device in Server Density .
see the ` API
List devices in Server Density
device information in Server Density .
downloads Server Density installation agent , and installs sd-agent
Setup client and init datastore.
flattened list of keys .
Parse the info returned by udevadm command .
Replace list with only one element to the value of the element.
delivered by udevadm
Return all the udev database
detects the date/time format for the string passed .
Displays the current date
Set the current month , day , and year
Get the current system time .
be in 24 hour format .; Sets the current time .
Displays the current time zone
Use this list when setting a; Displays a list of available time zones .
list valid time_zone
is on or off .
Display the currently set network time server .
Designates a network time server .
Gets snapshot of the desired version of the artifact
Gets the specific version string of a snapshot of the desired version of the artifact
Get a value from the dictionary
merge them together
creates new host .
does not exists , eventually deletes host .
are assigned to the host .
configures the specified managed object in a single subtree ( for example , DN ) .
returns the session cookie .
Converts the etree to dict
Guess an archive type (tar, zip, or rar) by its file extension
does exactly what `` tempfile.mkstemp ( ) `` does but
copy the source directory to the destination ,
Copy files from a source to a destination in an atomic way, and if
fail with a WindowsError exception if a file
raising exceptions when reading a file fails
exists , wait for it to release first; Obtain a write lock .
set the umask and restore once the contextmanager exits
set CLOEXEC on the fd .
Shortcut for fopen with lock and context manager.
Shortcut for fopen with extra uid, gid, and mode options.
A clone of the python os.walk function with some checks for recursive
Platform-independent recursive delete.
check if the `` fcntl `` module is available or not .
returns a safe name to use
Input the full path and filename , splits on directory separator and calls safe_filename_leaf for
True if the file is; is a binary , returns bool .
os.remove ( path ) and suppresses the OSError if the file does n't exist
Return a list of all files found under directory ( and its subdirectories )
Return a mode value , normalized to a string and containing a leading zero
Convert human-readable units to bytes
Backup a file on the minion
Detect a file 's encoding using the following :
Provide grains about zpools
Provide grains for zfs/zpool
Send a `` Magic Packet '' to wake up a list of Minions .
Send a `` Magic Packet '' to wake up a Minion
Send a `` Magic Packet '' to wake up Minions that are matched in the grains cache
Return a conn object for accessing memcached
Returns the HTTP auth header
Returns the URL of the endpoint
Removes SaltStack params from **kwargs
returned by glassfish
Do a GET request to the API
Do a POST request to the API
Do a DELETE request to the API
Get an element 's properties
Get an element with or without properties
Create a new element
including it 's properties
Delete an element
Create a connection pool
Update a connection pool
Delete a connection pool
Create a connection resource
Update a connection resource
Create a JMS destination
Update a JMS destination
Delete a JDBC pool
Create a JDBC resource
Update a JDBC resource
Get system properties
Ensure a network exists and is up-to-date
Split a smartos docker uuid into repo and tag
Check if uuid is a valid smartos uuid
Return imgadm version
Get the image uuid from an imported docker image
Return a list of available images
Show manifest of a given image
Import an image from the repository
Remove an installed image
Remove unused images
Return a list of available sources
Delete a source
Add a new source
instantiating a Flags object
instantiating a Dacl class .
Required for working with; verifies a sid .
Converts a PySID object to a string SID.
Gets the name from the specified principal .
be a file , folder , registry key ,
Get the permissions for the passed object
Enable or disable an objects inheritance.
Get an object 's inheritance .
used by `` check_perms `` for checking and setting Grant and
Retrieve a set of public keys from GitHub for the specified list of users .
Retrieves DNS host record settings for the requested domain.
Gets a list of DNS servers associated with the requested domain .
Sets DNS host records settings for the requested domain.
domain to use custom DNS servers .
Required for free; domain to use namecheap default DNS servers .
Display the profiling data in a table format .
exists , copy it else from source
hosted zone exists with the given attributes .
described is absent
Get module optional environment.
Run the aptly command .
string value to it 's closest non-string analog , if possible .
creating or editing a repository .
Parse the output of an aptly show command .
matching string values to lists/dictionaries .
is readable .
Get the configuration data .
List all of the local package repositories.
detailed information about a local package repository .
Create a new local package repository .
Configure the settings for a local package repository.
Remove a local package repository .
Get a list of all the mirrored remote repositories .
detailed information about a mirrored remote repository .
is not removed .; Remove a mirrored remote repository .
Get a list of all the published repositories .
Get the details of a published repository .
tries to remove as many files; belonging to a published repository .
Get a list of all the existing snapshots .
detailed information about a snapshot .
is published , it can not be; Remove information about a snapshot .
regarding unreferenced packages and delete files in the package pool that
distinguished name list exists with the items provided .
be used to check to see if an alarm exists .
this presenter magic makes yaml.safe_dump
Get all alarm details .; be used to create an sls
update a cloudwatch alarm .
Convert a list of strings into actual arns .
Delete a cloudwatch alarm
Convert a boto.ec2.cloudwatch.alarm.MetricAlarm into a dict .
is present in keychain
is absent in keychain
Extract pillar from an hg repository
are using the latest revision in the hg repository
Open the connection to the network device
Return the connection status with the network device .
Calls an arbitrary netmiko method.
Given a trail name , check to see if the given trail exists .
Given a trail name describe its properties .
List all trails
Given a valid config , update a trail .
logging for a trail
Add tags to a trail
returns an iterator over all CloudFront distributions .
Get information about a CloudFront distribution ( configuration , tags ) with a given name .
Get details of all CloudFront distributions.
Create a CloudFront distribution with the given name , config , and ( optionally ) tags .
Update the config ( and optionally tags ) for the CloudFront distribution with the given name .
List, with moderate information, all CloudFront distributions in the bound account.
exists with the given Resource ID or False otherwise .
return any CloudFront distributions which happen to have a Comment sub-field
Set a CloudFront distribution to be disabled .
return any CloudFront Origin Access Identities which happen to have a Comment
exists with the given Resource ID or False
Add tags to a CloudFront resource.
Enforce a given set of tags on a CloudFront resource : adding , removing , or changing them
Read a file and return content
Write content to a file
Get the path to the directory on the minion where CIB 's are saved
Get the full path of a cached CIB-file with the name of the CIB
Get the full path of a temporary CIB-file with the name of the CIB
Get the full path of the file containing a checksum of a CIB-file with the name of the CIB
Ensure that an item is created
are authorized to the cluster
Setup Pacemaker cluster on nodes.
Add a node to the Pacemaker cluster via PCS
Ensure that a CIB-file with the content of the current live CIB is created
is pushed if it is changed since the creation of it with pcs.cib_present
is set to a given value
Ensure that a fencing resource is created
Ensure that a resource is created
Ensure that a constraint is created
Allow top_cfg to be YAML
See if jail service is actually enabled on boot
are set to be run
specified jail 's configuration
defined in specified
See if specified jail is currently running
related kernel states ( sysctl )
Execute the command passed with pecl
Uninstall one or several pecl extensions.
installed pecl extensions .
Return cloud client
Create a cloud vm with the given profile and instances , instances can be a
named vm ( s )
Execute a single action on the given map/provider/instance
Get splunk search properties from an object
Get a splunk search
Update a splunk search
Create a splunk search
Delete a splunk search
List splunk searches (names only)
be used to create; Get all splunk search details .
is added .
is removed .
Ensure the current node joined to a cluster with node user @ host
Create an image
Delete an image
List images
Search for images
Get a single image
Update properties for an image
Deletes a device from Vistara based on DNS name or partial name .
get an access_token
Return server version (``apachectl -v``)
Return server version (``apachectl -V``)
shared modules ( `` apachectl -M `` )
compiled into the server ( `` apachectl -l `` )
Return list of directives together with expected arguments
Show the settings as parsed from the config file ( currently
httpd to start , restart , or stop .
using the `` htpasswd `` command .
Get Information from the Apache server-status handler
goes through config structure and builds final Apache configuration
Create VirtualHost configuration files
named sqs queue is deleted .
Return a dict of the changes required for a user if the user is present ,
named user is present with the specified properties
Parse a salt : // URL ; return the path and a possible saltenv query .
join `path` and `saltenv` into a 'salt://' URL.
is escaped with ` | `
add escape character ` | ` to ` url `
remove escape character ` | ` from ` url `
append `saltenv` to `url` as a query parameter to a 'salt://' url
remove the saltenv query parameter from a 'salt : // ' url
Return a string with http basic auth incorporated into it
Remove HTTP user and password
Resolve a package name from a line containing the hold expression .
Determine package manager name (yum or dnf),
contain irregular line breaks if package
Ensure that the appropriate versionlock plugin is present
be used in the yum/dnf command , based on the
representing the yum config options and values .
return its value
Takes a basedir argument as a string or a list .
specified package name , if necessary .
List the packages currently installed as a dict .
versionadded : : 2015.8.1
Check the yum repos for updated packages
Run a full system upgrade ( a `` yum upgrade `` or `` dnf upgrade `` ) , or
repos in < basedir > ( default : all dirs in ` reposdir ` yum option ) .
Display a repo from < basedir > ( default basedir : all dirs in `` reposdir ``
Delete a repo from <basedir> (default basedir: all dirs in `reposdir` yum
does not exist , it will
Turn a single repo file into a dict
Return a formatted diff between current files and original in a package .
known patches in repos .
Find out localhost outside IP .
running on the same machine ,
On datagram receive.
Create datagram connection.
Run server.
Query the broadcast for defined services.
map from the network .
declared servers .
password taken from one of the sources ( only the most prioritized one ) :
exists , eventually creates new user .
does not exist , eventually delete user .
Check if PID is still alive .
Main analyzer routine.
Call an external system command .
Package scanner switcher between the platforms.
Get packages with configuration files on Dpkg systems.
Get packages with configuration files on RPM systems.
Filter out unchanged packages on the Debian or RPM systems.
Save configuration packages.
Save payload (unmanaged files)
Build a in-memory data of all managed files .
Get a list of all system files , belonging to the Debian package manager .
Get a list of all system files , belonging to the RedHat package manager .
Walk implementation.
Get the intersection between all files and managed files .
Scan the system.
setting up the database etc .
Initialize some Salt environment .
Take a snapshot of the system .
Export description for Kiwi.
using Kiwi .
Ensure thing type exists.
Ensure policy exists.
is attached to the given principal .
Ensure topic rule exists.
passed properties is absent .
contain ascii chars , so make sure we return a str type
Change the groups to which this user belongs
Return a list of all users
use only /etc/passwd
Add a range of subordinate gids to the user
Install a DISM capability
Performs an ICMP ping to a host
Return netstat information for Linux distros
Return ss information for Linux distros
Get process information for network connections using fstat
Get process information for network connections using sockstat
Return a dict of pid to ppid mappings
Return netstat information for BSD flavors
Return netstat information for SunOS flavors
ip routing information for Linux distros
routing information for FreeBSD and macOS
routing information for NetBSD
routing information for SunOS
Return a dict containing information on all of the running TCP connections ( currently linux and solaris only )
Return the arp table from the minion
returns the network address, subnet mask and broadcast address of a cidr address
Modify hostname
Return network interface buffer information using ethtool
sizes using ethtool
linux only )
configured routes from routing table
Return default route(s) from routing table
Retrieve the interface name from a specific CIDR
Retrieve the hexadecimal representation of an IP address
Ensure trail exists.
Return a list of nodes
Return a list of node sizes
Return a list of locations for this cloud
Reboot a node in the cloud
Return a list of storage volumes for this cloud
Return a list of storage volumes snapshots for this cloud
Create a storage volume
Create a storage volume snapshot
Attaches volume to node.
Detaches a volume from a node .
Destroy a volume snapshot .
Return a list of images for this cloud
Create an image from a node
Delete an image of a node
Get an image of a node
Copies an image from a source region to the current region.
List all the available key pair objects.
Get a single key pair by name
string or a file path
Delete a key pair
Get item from a list by the id field
Get the summary from module monit and try to see if service is
running jboss instance .
running jboss instance
existing datasource in running jboss instance .
Read datasource properties in the running jboss instance.
Create a simple jndi binding in the running jboss instance
binding in the running jboss instance
Remove an existing datasource from the running jboss instance .
is running .
List all deployments on the jboss instance
Undeploy the application from jboss instance
Computes the timestamp for a future event that may occur in `` time_in `` time
Return the time in human readable format for a future event that may occur
is None ,; Return a bool telling whether or `` path `` is absolute .
Get the status of all the firewall profiles
Get the firewall property from the specified profile in the specified store
specified profile in the specified store
specified profile and
Configure firewall settings.
Configure the firewall state.
Return the error line and error message output from
Render a template from a python source file
Gets information about fund in the user's account.
provided minimum value is present in the user 's account .
Return the ZeroMQ URI to connect the Minion to the Master .
are set as specified in `` opts '' .
Send a load across the wire in cleartext
Send a request , return a future which will complete when we send the message
Return the master publish port
Take the zmq messages , decrypt/decode them into a payload
Return the current zmqstream , creating one if necessary